<!DOCTYPE html>
<html lang="zh-cn" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention - 时间飘过</title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="weedge" /><meta name="description" content="论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention" />
<meta name="keywords" content="transformer" />







<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://weedge.github.io/post/paper/transformer/infini_attention/" />





<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css" integrity="sha256-&#43;ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media="screen" crossorigin="anonymous">




<link rel="stylesheet" href="/css/copy-to-clipboard.css">


<meta property="og:title" content="论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention" />
<meta property="og:description" content="论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://weedge.github.io/post/paper/transformer/infini_attention/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-04-12T10:26:12+08:00" />
<meta property="article:modified_time" content="2024-04-12T10:26:12+08:00" />

<meta itemprop="name" content="论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention">
<meta itemprop="description" content="论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"><meta itemprop="datePublished" content="2024-04-12T10:26:12+08:00" />
<meta itemprop="dateModified" content="2024-04-12T10:26:12+08:00" />
<meta itemprop="wordCount" content="13036">
<meta itemprop="keywords" content="LLM,model,transformer," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"/>
<meta name="twitter:description" content="论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->



<script>
  MathJax = {
    tex: {
      inlineMath: [["$", "$"]],
    },
    displayMath: [
      ["$$", "$$"],
      ["\[\[", "\]\]"],
    ],
    svg: {
      fontCache: "global",
    },
  };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>





</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">时间飘过</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/">主页</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/post/">归档</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/tags/">标签</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/categories/">分类</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/about/">About</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/perf-book-cn/zh/" rel="noopener" target="_blank">
              《现代CPU性能分析与优化》
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          
        
      </li>
    

    
  </ul>
</nav>


  
    






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

  

  

  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      时间飘过
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/">主页</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/post/">归档</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/tags/">标签</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/categories/">分类</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/about/">About</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/perf-book-cn/zh/" rel="noopener" target="_blank">
              《现代CPU性能分析与优化》
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</h1>
      
      <div class="post-meta">
        <time datetime="2024-04-12" class="post-time">
          2024-04-12
        </time>
        <div class="post-category">
            <a href="https://weedge.github.io/categories/%E6%8A%80%E6%9C%AF/"> 技术 </a>
            <a href="https://weedge.github.io/categories/paper/"> paper </a>
            
          </div>
        

        
        

        
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#1-引言">1 引言</a></li>
    <li><a href="#2-方法">2 方法</a>
      <ul>
        <li><a href="#21-infini-attention">2.1 Infini-attention</a></li>
        <li><a href="#22-内存和有效上下文窗口memory-and-effective-context-window">2.2 内存和有效上下文窗口(Memory and Effective Context Window)</a></li>
      </ul>
    </li>
    <li><a href="#3-实验">3 实验</a>
      <ul>
        <li><a href="#31-长上下文语言建模">3.1 长上下文语言建模</a></li>
        <li><a href="#32-llm-持续预训练">3.2 LLM 持续预训练</a></li>
      </ul>
    </li>
    <li><a href="#4-相关工作">4 相关工作</a></li>
    <li><a href="#5-结论">5 结论</a></li>
    <li><a href="#references">References</a></li>
    <li><a href="#附录">附录</a>
      <ul>
        <li><a href="#a-额外的训练细节">A 额外的训练细节</a></li>
        <li><a href="#b-口令passkey检索任务"><strong>B 口令(passkey)检索任务</strong></a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>

    
    <div class="post-content">
      <p><strong>摘要</strong>： 本文介绍了一种有效的方法，将基于Transformer的大型语言模型（LLMs）扩展到无限长的输入，同时受到内存和计算的限制。我们提出的方法的关键组成部分是一种新的注意力技术，称为Infini-attention。Infini-attention将一种压缩内存集成到了传统的注意力机制中，并在单个Transformer块中构建了掩码局部注意力和长期线性注意力机制。我们通过在长上下文语言建模基准、1M序列长度的口令(keypass)上下文块检索和500K长度的书籍摘要任务中使用1B和8B LLMs，展示了我们方法的有效性。我们的方法引入了最小的有界内存参数，并实现了LLMs的快速流式推理。</p>
<p><strong>注</strong>：为解决大模型（LLMs）在处理超长输入序列时遇到的内存限制问题，本文作者提出了一种新型架构：Infini-Transformer，它可以在有限内存条件下，让基于Transformer的大语言模型（LLMs）高效处理无限长的输入序列。实验结果表明：Infini-Transformer在长上下文语言建模任务上超越了基线模型，内存最高可节约114倍。</p>
<p>感觉有种外挂存储库(类似向量数据库)嵌入到模型结构中。比如： <a href="https://arxiv.org/abs/2203.08913">Memorizing Transformers</a> + <a href="https://github.com/lucidrains/memorizing-transformers-pytorch">code</a></p>
<p><strong>目的</strong>：</p>
<ul>
<li>通过翻译通读论文，了解Transformer相关优化模型结构论文思路，比如文中对比的 <a href="https://arxiv.org/abs/1901.02860">Transformer-XL</a> ，<a href="https://arxiv.org/abs/1911.05507">Compressive Transformer</a>，<a href="https://arxiv.org/abs/2207.06881">Recurrent Memory Transformer(RMT)</a> ， <a href="https://arxiv.org/abs/2203.08913">Memorizing Transformers</a> + <a href="https://github.com/lucidrains/memorizing-transformers-pytorch">code</a>（使用基于向量检索的 KV 存储器，主要是在该论文的基础上实验），<a href="https://arxiv.org/abs/2305.14788">Adapting Language Models to Compress Contexts(AutoCompressors)</a>；还有一篇Jeff Dean主导的模型推理工程优化论文：<a href="https://arxiv.org/pdf/2211.05102.pdf">Efficiently scaling transformer inference</a>可以通读下 。</li>
<li>Infini-Transformer论文中提到的优化技术：Compressive Memory  还可以整合到现到Sparse MoE 相关的模型结构中，比如<a href="https://arxiv.org/abs/2101.03961">Switch Transformers</a> , <a href="https://arxiv.org/abs/2306.04640">ModuleFormer</a>中提到的Sparse MoE中；诶~是不是做做实验，困惑度(perplexity)/loss 效果不错的话是不是可以发个论文嘞~；Google大法下了个优化蛋，后面跟着一批组合优化蛋。法力无边，ღ( ´･ᴗ･` )比心~</li>
<li>借鉴在小模型上的实验方法和评估方法；希望低成本实现方案去复现下。</li>
</ul>
<p><strong>论文解读：</strong></p>
<p>原论文地址： <a href="https://arxiv.org/pdf/2404.07143.pdf">https://arxiv.org/pdf/2404.07143.pdf</a></p>
<p>论文中提到 Compressive Memory 来自：</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Sparse_distributed_memory">Sparse distributed memory (SDM)</a>
<ul>
<li>SDM 与原始attention的结合在GPT2模型结构中，可以看下这个视频：<a href="https://www.youtube.com/watch?v=THIIk7LR9_8">Attention Approximates Sparse Distributed Memory</a> + <a href="https://github.com/TrentBrick/attention-approximates-sdm">code</a></li>
</ul>
</li>
<li><a href="https://papers.nips.cc/paper_files/paper/2019/file/182bd81ea25270b7d1c2fe8353d17fe6-Paper.pdf">Metalearned neural memory. Advances in Neural Information Processing Systems</a> | <a href="https://arxiv.org/abs/2011.07831">Learning associative inference using fast weight memory</a> + <a href="https://github.com/ischlag/Fast-Weight-Memory-public">code</a>；</li>
</ul>
<p>Infini-attention的优化借鉴Linear attetion机制 + 增量规则 =&gt; 更新规则（线性(Linear) + 增量(Delta)）：</p>
<p><strong>更新规则(Update Rule)</strong>: 如果在KV 绑定已经存在于内存中，则保持关联矩阵不变，同时，仍跟踪与前一个更新规则相同的归一化项（线性）以保证数值稳定性</p>
<ul>
<li><a href="https://arxiv.org/pdf/1812.01243.pdf">Efficient Attention: Attention with Linear Complexities [v10]</a> + <a href="https://github.com/cmsflash/efficient-attention">code</a> + <a href="https://www.youtube.com/watch?v=_wnjhTM04NM">video</a> 中的线性注意力(Linear attention)机制；并利用相关方法中的稳定训练技术；</li>
<li><a href="https://arxiv.org/abs/2006.16236">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a> + <a href="https://linear-transformers.com/">website</a> 中的线性注意力(Linear attention)机制；更新规则和检索机制，主要是因为其简单性和竞争性能；</li>
<li><a href="https://arxiv.org/abs/2009.14794">Rethinking Attention with Performers</a> + <a href="https://research.google/blog/rethinking-attention-with-performers/">blog</a> + <a href="https://github.com/google-research/google-research/blob/master/performer/fast_attention/README.md">code</a> + <a href="https://www.youtube.com/watch?v=xJrKIPwVwGM">video</a> 论文中未提到，但是感觉可以加入优化；</li>
<li><a href="https://en.wikipedia.org/wiki/Delta_rule">Delta Rule</a>: 增量规则尝试通过首先检索现有值条目并将它们从新值中减去，然后应用关联绑定作为新的更新，稍微改进了内存更新</li>
</ul>
<p>论文作者： <a href="https://www.tsendeemts.com/">Tsendsuren Munkhdalai</a>  , 也是 <strong>Metalearned neural memory</strong> 和 <strong>Learning associative inference using fast weight memory (FWM)</strong> 的作者，沿用了以前的优化，与原始attention结构结合；在压缩内存中存储键和值状态的绑定，并使用查询向量进行检索。</p>
<h2 id="1-引言">1 引言</h2>
<p>内存作为智能的基石，它使得针对特定上下文的高效计算成为可能。然而，Transformer（Vaswani等人，2017年）和基于Transformer的大型语言模型（LLMs）（Brown等人，2020年；Touvron等人，2023年；Anil等人，2023年；Groeneveld等人，2024年）由于注意力机制的本质，具有受限的上下文依赖内存。</p>
<p>Transformer中的注意力机制在内存占用和计算时间上表现出二次方复杂度。例如，对于一个批量大小为512，上下文长度为2048的500B模型，注意力键值（KV）状态有3TB的内存占用（Pope等人，2023年）。实际上，将LLMs扩展到更长的序列（例如1M个标记）对于标准Transformer架构来说是一个挑战，而且随着上下文模型变得越来越长，其服务成本也越来越高。</p>
<p>压缩内存(Compressive memory)系统承诺比注意力机制更可扩展和高效，特别是对于极长序列（Kanerva，1988年；Munkhdalai等人，2019年）。与随输入序列长度增长的数组不同，压缩内存主要通过改变其参数来存储和回忆信息，以实现有界的存储和计算成本。在压缩内存中，通过改变其参数来添加新信息到内存中，目标是稍后能够恢复这些信息。然而，目前的LLMs还没有看到一个有效的、实用的压缩内存技术，该技术在简单性和质量之间取得了平衡。</p>
<p>在这项工作中，我们介绍了一种新的方法，使得Transformer LLMs能够有效地处理无限长的输入，同时内存占用和计算资源有界。我们提出的方法的一个关键组成部分是一个新的注意力技术，称为Infini-attention（图1）。Infini-attention将压缩内存整合到传统的注意力机制中，并在单个Transformer块中构建了掩蔽的局部注意力(Causal SDPA)和长期的线性注意力(Linear attention)机制。</p>
<p><img src="https://arxiv.org/html/2404.07143v1/x1.png" alt=""></p>
<p><em>图 1：Infini-attention 具有一个额外的压缩内存，使用线性注意力处理无限长的上下文。${KV}_{s-1}$  和 ${KV}_s$ 分别是当前和前一个输入片段的注意力键和值，而  $Q_s$ 是注意力查询。PE 表示位置嵌入。</em></p>
<p>这种对Transformer注意力层的微妙但关键的修改，使得现有的LLMs能够通过持续的预训练和微调，自然地扩展到无限长的上下文。我们的Infini-attention重用了标准注意力计算的所有键、值和查询状态，用于长期内存的巩固和检索。我们将注意力的旧KV状态存储在压缩内存中，而不是像在标准注意力机制中那样丢弃它们。然后，我们在使用注意力查询状态处理后续序列时，从内存中检索值。为了计算最终的上下文输出，Infini-attention聚合了长期内存检索到的值和局部注意力上下文。</p>
<p>在我们的实验中，我们展示了我们的方法在长上下文语言建模基准测试中胜过了基线模型，同时在内存大小上具有114倍的理解比率。当使用100K序列长度进行训练时，模型达到了更低的困惑度(perplexity)。一个1B LLM自然扩展到1M序列长度，并在注入Infini-attention后解决了口令(passkey)检索任务。最后，我们展示了一个8B模型在持续预训练和任务微调后，使用Infini-attention在500K长度的书籍摘要任务上达到了新的SOTA结果。</p>
<p>总之，我们的工作做出了以下贡献：</p>
<ol>
<li>我们引入了一个实用而强大的注意力机制Infini-attention，它具有长期压缩内存和局部因果注意力，能够有效地建模长距离和短距离的上下文依赖关系。</li>
<li>Infini-attention对标准的缩放点积注意力进行了最小的改变，并支持即插即用的持续预训练和长上下文适应。</li>
<li>我们的方法使Transformer LLMs能够通过流式处理极长的输入，扩展到无限长的上下文，同时保持有界的内存和计算资源。</li>
</ol>
<h2 id="2-方法">2 方法</h2>
<p>图2比较了我们的模型Infini-Transformer和Transformer-XL（Dai等人，2019年）。与Transformer-XL类似，Infini-Transformer在一系列片段上操作。我们在每个片段内计算标准的因果点积注意力上下文。因此，点积注意力计算在某种意义上是局部的，它涵盖了当前片段S（N是片段长度），总共N个token。</p>
<p><img src="https://arxiv.org/html/2404.07143v1/x2.png" alt=""></p>
<p><em>图 2：Infini-Transformer（顶部）具有完整的上下文历史，而 Transformer-XL（底部）会丢弃旧的上下文，因为它只缓存了最后一个片段的 KV 状态。</em></p>
<p>然而，局部注意力（Dai等人，2019年）在处理下一个片段时会丢弃前一个片段的注意力状态。在Infini-Transformers中，我们提出不仅不丢弃旧的KV注意力状态，而是重用它们来通过压缩内存维持整个上下文历史。因此，Infini-Transformers的每个注意力层都具有全局压缩和局部细粒度状态。我们称这种高效的注意力机制为Infini-attention，如图1所示，并在以下各节中正式描述。</p>
<h3 id="21-infini-attention">2.1 Infini-attention</h3>
<p>如图1所示，我们的Infini-attention计算局部和全局上下文状态，并结合它们输出。与多头注意力（MHA）类似，它在每个注意力层中维护H个并行的压缩内存（H是注意力头的数量），除了点积注意力外。</p>
<h4 id="211-缩放点积注意力">2.1.1 缩放点积注意力</h4>
<p>多头缩放点积注意力（Vaswani等人，2017年），特别是其自注意力变体（Munkhdalai等人，2016年；Cheng等人，2016年），已成为LLMs的主要构建块。MHA的强大能力可以模拟上下文依赖的动态计算，并且其时间掩蔽的便利性在自回归生成模型中得到了广泛利用。</p>
<p>在普通 MHA 中，一个单独的注意力头（head）从输入序列段（input segments）$X \in {\rm I!R}^{N \times d_{model}}$ 计算其注意力上下文 $A_{dot} \in {\rm I!R}^{N \times d_{value}}$，具体过程如下。首先，它计算注意力的查询（query）、键（key）和值（value）状态：
$$
K = XW_K, \text{ } V = XW_V \text{ 且 } Q = XW_Q.
$$
这里，$W_K \in {\rm I!R}^{d_{model} \times d_{key}}$、$W_V \in {\rm I!R}^{d_{model} \times d_{value}}$ 和 $W_Q \in {\rm I!R}^{d_{model} \times d_{key}}$ 是可训练的投影矩阵。然后，注意力上下文被计算为所有其他值的加权平均值：</p>
<p>$$
A_{dot} = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_{model}}}\right) V.
$$
对于 MHA，我们并行地为每个序列元素计算 $H$ 个注意力上下文向量，沿着第二个维度将它们连接起来，最后将连接的向量投影到模型空间以获得注意力输出。</p>
<h4 id="212-压缩式内存compressive-memory">2.1.2 压缩式内存（Compressive Memory）</h4>
<p>在无限注意力（Infini-attention）中，与为压缩式内存计算新的内存条目不同，我们重复使用点积注意力计算中的查询（query）、键（key）和值（value）状态（$Q$、$K$ 和 $V$）。点积注意力和压缩内存之间的状态共享和重用不仅实现了高效的即插即用长上下文适应，还加快了训练和推理速度。与之前的工作（Munkhdalai等人，2019年）类似，我们的目标是在压缩内存中存储键和值状态的绑定，并使用查询向量进行检索。</p>
<p>尽管文献中提出了不同形式的压缩内存（Hopfield，1982年；Kanerva，1988年；Schlag等人，2019年；Munkhdalai等人，2019年），为了简单和计算效率，我们在这项工作中用关联矩阵（Schlag等人，2020年）参数化内存。这种方法进一步允许我们将内存更新和检索过程视为线性注意力机制（Shen等人，2018年），并利用相关方法中的稳定训练技术。特别是，我们采用了Katharopoulos等人（2020年）的更新规则和检索机制，主要是因为其简单性和竞争性能。</p>
<p><strong>内存检索(Memory retrieval)。</strong> 在无限注意力中，我们通过使用查询 $Q \in {\rm I!R}^{N \times d_{key}}$ 从内存 $M_{s-1} \in {\rm I!R}^{d_{key} \times d_{value}}$ 中检索新的内容 $A_{mem} \in {\rm I!R}^{N \times d_{value}}$，计算如下：
$$
A_{mem} =  \frac{\sigma({Q}) M_{s-1}}
{{\sigma(Q)} z_{s-1}}.
$$
这里，$\sigma$ 和 $z_{s-1} \in {\rm I!R}^{d_{key}}$ 分别是非线性激活函数和归一化项。由于非线性和规范化方法的选择对训练稳定性至关重要，因此我们遵循Katharopoulos等人（2020年）记录所有键的总和作为归一化项$z_{s-1}$，并使用逐元素ELU + 1作为激活函数（Clevert等人，2015年）。</p>
<p><strong>内存更新(Memory update)。</strong> 一旦检索完成，我们使用新的 KV 条目更新内存和归一化项，并获得下一个状态，如下所示：
$$
M_{s} \leftarrow M_{s-1} + \sigma(K)^T V \text{ 和 } z_{s} \leftarrow z_{s-1} + \sum_{t=1}^N \sigma(K_t).
$$
新的内存状态 $M_{s}$ 和 $z_{s}$ 然后传递给下一个段 $S+1$，在每个注意力层中形成了一个递归。式中的右侧项 $\sigma(K)^T V $ 被称为关联绑定算子。</p>
<p>受到增量规则(Delta Rule)的成功启发，我们还将其纳入了我们的无限注意力中。增量规则尝试通过首先检索现有值条目并将它们从新值中减去，然后应用关联绑定作为新的更新，稍微改进了内存更新。
$$
M_{s} \leftarrow M_{s-1} + \sigma(K)^T (V - \frac{\sigma({K}) M_{s-1}}
{{\sigma(K)} z_{s-1}}).
$$
更新规则（线性(Linear) + 增量(Delta)）：如果在 KV 绑定已经存在于内存中，则保持关联矩阵不变，同时，仍跟踪与前一个更新规则相同的归一化项（线性）以保证数值稳定性。</p>
<p><strong>长期上下文注入(Long-term context injection)。</strong> 我们通过一个学习得到的门控标量 $\beta$ 聚合了局部注意力状态 $A_{dot}$ 和内存检索的内容 $A_{mem}$：
$$
A = \textit{sigmoid} (\beta) \odot A_{mem} + (1 - \textit{sigmoid}(\beta)) \odot A_{dot}.
$$
这仅增加了每个头部的单个标量值作为训练参数，同时允许在模型中学习长期和局部信息流之间的可学习权衡。与标准 MHA 类似，对于多头的无限注意力，我们并行计算 $H$ 个上下文状态，并将它们连接和投影以获得最终的注意力输出 $O \in  {\rm I!R}^{N \times d_{model}}$：
$$
O = [A^1; \dotso A^H] W_O
$$</p>
<p>其中 $W_O \in {\rm I!R}^{H \times d_{value} \times d_{model}}$ 是可训练权重。</p>
<h3 id="22-内存和有效上下文窗口memory-and-effective-context-window">2.2 内存和有效上下文窗口(Memory and Effective Context Window)</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th style="text-align:left">Memory (cache) footprint</th>
<th style="text-align:left">Context length</th>
<th style="text-align:left">Memory update</th>
<th style="text-align:left">Memory retrieval</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Transformer-XL</td>
<td style="text-align:left">$(d_{key} + d_{value}) \times H \times N \times l$</td>
<td style="text-align:left">$N \times l$</td>
<td style="text-align:left">Discarded(丢弃)</td>
<td style="text-align:left">Dot-product attention</td>
</tr>
<tr>
<td style="text-align:left">Compressive Transformer</td>
<td style="text-align:left">$d_{model} \times (c + N) \times l$</td>
<td style="text-align:left">$(c \times r + N) \times l$</td>
<td style="text-align:left">Discarded(丢弃)</td>
<td style="text-align:left">Dot-product attention</td>
</tr>
<tr>
<td style="text-align:left"><strong>Memorizing Transformers</strong></td>
<td style="text-align:left">$(d_{key} + d_{value}) \times H \times N \times S$</td>
<td style="text-align:left">$N \times S$</td>
<td style="text-align:left">None</td>
<td style="text-align:left">kNN + dot-product attention</td>
</tr>
<tr>
<td style="text-align:left">RMT</td>
<td style="text-align:left">$d_{model} \times p \times l \times 2$</td>
<td style="text-align:left">$N \times S$</td>
<td style="text-align:left">Discarded(丢弃)</td>
<td style="text-align:left">Soft-prompt input</td>
</tr>
<tr>
<td style="text-align:left">AutoCompressors</td>
<td style="text-align:left">$d_{model} \times p \times (m + 1) \times l$</td>
<td style="text-align:left">$N \times S$</td>
<td style="text-align:left">Discarded(丢弃)</td>
<td style="text-align:left">Soft-prompt input</td>
</tr>
<tr>
<td style="text-align:left"><strong>Infini-Transformers</strong></td>
<td style="text-align:left">$d_{key} \times (d_{value} + 1) \times H \times l$</td>
<td style="text-align:left">$N \times S$</td>
<td style="text-align:left">Incremental(增量)</td>
<td style="text-align:left">Linear attention</td>
</tr>
</tbody>
</table>
<p><em>表1:  列出了以前具有段级内存的 Transformer 模型及其上下文-内存占用(footprint)和有效上下文长度，这些长度是根据模型参数和输入段长度定义的。对于每个模型，内存大小和有效上下文长度的定义如下：$N$：输入段长度，$S$：段的数量，$l$：层数，$H$：注意力头的数量，$c$：压缩式 Transformer 内存大小，$r$：压缩比率，$p$：软提示摘要向量(soft-prompt summary vectors)的数量，$m$：摘要向量积累步骤数。</em></p>
<p>我们的 Infini-Transformer 能够以有限的内存占用实现无界上下文窗口。为了说明这一点，表1 列出了先前具有段级内存的模型及其上下文-内存占用和有效上下文长度，这些长度是根据模型参数和输入段长度定义的。Infini-Transformer 对于每个头部在单个层中存储压缩的上下文的内存复杂度为 $d_{key} \times d_{value} + d_{key}$，而对于其他模型，复杂度随着序列维度增长而增加 - 内存复杂度要么依赖于缓存大小（如 Transformer-XL~(Dai等，2019年)、Compressive Transformer~(Rae等，2019年)和Memorizing Transformers~(Wu等，2022年)），要么依赖于软提示大小（如 RTM~(Bulatov等，2022年) 和 AutoCompressors~(Ge等，2023年)）。</p>
<p>Transformer-XL 在计算注意力时，除了当前状态外，还会对上一段缓存的 KV 状态进行注意力计算。由于这是在每一层都进行的操作，因此 Transformer-XL 将上下文窗口从 $N$ 扩展到 $N \times l$ 个标记，而额外的内存占用为 $(d_{key} + d_{value}) \times H \times N \times l$。Compressive Transformer 在 Transformer-XL 的基础上增加了第二个缓存，并存储过去段激活的压缩表示。因此，它将 Transformer-XL 的上下文窗口扩展了 $c \times r \times l$，但仍具有较大的上下文-内存复杂度。Memorizing Transformers 选择将整个 KV 状态作为输入序列的上下文存储。由于在这种情况下存储变得成本过高，因此它们将上下文计算限制为仅在单个层中进行。<strong>通过利用快速 kNN 检索器，Memorizing Transformers 构建一个覆盖整个序列历史的上下文窗口，长度为 $N \times S$，但存储成本增加了。我们的实验表明，在 Memorizing Transformers 的基础上，Infini-Transformer LM 可以实现超过 100 倍的压缩率，同时进一步改善了困惑度(perplexity)得分</strong>。</p>
<p><strong>RMT 和 AutoCompressors 允许潜在的无限上下文长度，因为它们将输入压缩成摘要向量，然后将它们作为额外的软提示输入传递给后续的段落。然而，在实践中，这些技术的成功程度高度依赖于软提示向量的大小</strong>。换句话说，为了使 AutoCompressors (Chevalier 等，2023年) 实现更好的性能，需要增加软提示（摘要）向量的数量，而这样做会导致内存和计算复杂度迅速增长，从而降低效率。也观察到在 AutoCompressors (Chevalier 等，2023年) 中需要一个高效的压缩目标来训练这种提示压缩技术 (Ge 等，2023年)。</p>
<h2 id="3-实验">3 实验</h2>
<p>我们在涉及极长输入序列的基准数据集上评估了我们的 Infini-Transformer 模型：</p>
<blockquote>
<p>长上下文语言建模、100万长度口令(passkey)块检索和50万长度书籍摘要任务。</p>
<p>对于长上下文语言建模基准，我们从头开始训练我们的模型；</p>
<p>而对于口令(passkey)块和书籍摘要任务，我们持续预训练现有的 LLMs，以突出我们方法的插拔式长上下文适应能力。</p>
</blockquote>
<h3 id="31-长上下文语言建模">3.1 长上下文语言建模</h3>
<p>我们在 PG19（Rae 等，2019年）和 Arxiv-math（Wu 等，2022年）基准数据集上训练和评估了小型的 Infini-Transformer 模型。我们的设置与 Memorizing Transformers（Wu 等，2022年）非常相似。具体来说，我们所有的模型都有 12 层，每层有 8 个注意力头，每个头的维度为 128，以及隐藏层为 4096 的 FFNs。</p>
<p>我们将 Infini-attention 的段长度 $N$ 设置为 2048，对于所有的注意力层输入序列长度设置为 32768 进行训练。这样可以让 Infini-attention 在关于其压缩内存状态的 16 个步骤上展开。对于 RMT 基线，我们进行了几次运行，摘要提示长度为 50、100 和 150，序列长度分别为 4096、8196 和 32768。当在长度为 8196 的序列上训练时，RMT 使用 100 个摘要向量时达到了最佳结果。</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Memory size (comp.)</th>
<th>XL cache</th>
<th>Segment length</th>
<th>PG19</th>
<th>Arxiv-math</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformer-XL</td>
<td>50M (3.7x)</td>
<td>2048</td>
<td>2048</td>
<td>11.88</td>
<td>2.42</td>
</tr>
<tr>
<td>Memorizing Transformers</td>
<td>183M (1x)</td>
<td>2048</td>
<td>2048</td>
<td>11.37</td>
<td>2.26</td>
</tr>
<tr>
<td>RMT</td>
<td>2.5M (73x)</td>
<td>None</td>
<td>2048</td>
<td>13.27</td>
<td>2.55</td>
</tr>
<tr>
<td>Infini-Transformer (Linear)</td>
<td>1.6M (114x)</td>
<td>None</td>
<td>2048</td>
<td><strong>9.65</strong></td>
<td>2.24</td>
</tr>
<tr>
<td>Infini-Transformer (Linear + Delta)</td>
<td>1.6M (114x)</td>
<td>None</td>
<td>2048</td>
<td>9.67</td>
<td><strong>2.23</strong></td>
</tr>
</tbody>
</table>
<p><em>表2：长上下文语言建模结果以平均单词级困惑度进行比较。Comp. 表示压缩比。Infini-Transformer 模型优于内存变压器（Memorizing Transformers），其存储器长度为 65K，并实现了114倍的压缩比。</em></p>
<p>语言建模实验的主要结果总结在表2中。我们的 Infini-Transformer 在保持比 Memorizing Transformers（Wu 等，2022年）模型参数少 114 倍的情况下，优于 Transformer-XL（Dai 等，2019年）和 Memorizing Transformers 的基准模型，后者使用基于向量检索的 KV 存储器，在其第9层具有长度为 65K 的存储器。</p>
<p><strong>100K长度训练。</strong> 我们进一步将训练序列长度从32K增加到100K，并在 Arxiv-math 数据集上对模型进行了训练。100K的训练进一步将困惑度分数降低到 <strong>2.21</strong> 和 <strong>2.20</strong> 分别对于 $Linear$ 和 $Linear + Delta$ 模型。</p>
<p><strong>门控分数可视化(Gating score visualization)。</strong> 图3 展示了每一层中所有注意力头的压缩内存的门控分数 $\textit{sigmoid}(\beta)$。在训练后，Infini-attention 中出现了两种类型的头：门控分数接近 0 或 1 的专用头和分数接近 0.5 的混合头。专用头通过局部注意力计算处理上下文信息或从压缩内存中检索，而混合头将当前上下文信息和长期内存内容合并到一个输出中。有趣的是，每一层至少有一个短程头，允许输入信号向前传播直至输出层。我们还观察到在正向计算过程中长期和短期内容的检索交错进行。</p>
<p><img src="https://arxiv.org/html/2404.07143v1/x3.png" alt=""></p>
<p><em>图3：图中显示了训练后，在Infini-attention中出现了两种类型的注意力头：具有接近0或1的门控分数(gating score)的专用头和接近0.5的混合头。专用头通过局部注意力机制处理上下文信息或从压缩内存中检索，而混合头将当前上下文信息和长期(long-term)内存内容聚合到单个输出中。</em></p>
<h3 id="32-llm-持续预训练">3.2 LLM 持续预训练</h3>
<p>我们对现有 LLM 进行了轻量级的持续预训练，以适应长上下文。预训练数据包括 PG19 和 Arxiv-math 语料库以及长度超过 4K 令牌的 C4 文本（Raffel等人，2020）。我们的实验中，段长度 $N$ 设定为 2K。</p>
<p><strong>1M 口令(passkey)检索基准测试</strong>。我们将一个 10 亿参数的 LLM 中的普通 MHA 替换为 Infini-attention，并继续对长度为 4K 的输入进行预训练。模型在进行 1M 口令(passkey)检索任务 fine-tune 之前，以批量大小为 64 进行了 30K 步的训练。</p>
<p>口令(passkey)任务将一个随机数隐藏在一个长文本中，并在模型输出中询问它。分心文本的长度通过多次重复文本块来变化。先前的工作（Chen等人，2023）表明，一个 80 亿参数的 LLaMA 模型可以在使用位置插值进行相同长度的 32K 输入 fine-tune 后，解决长达 32K 的任务。我们进一步挑战，并仅在长度为 5K 的输入上进行 fine-tune，以在 1M 长度范围内进行测试。</p>
<p>表3 报告了从 32K 到 1M 不同长度的测试子集的令牌级准确率。对于每个测试子集，我们控制了口令(passkey)的位置，使其位于输入序列的开始、中间或结尾附近。我们报告了零点命中率和 fine-tuning 准确率。Infini-Transformers 在使用长度为 5K 的输入进行 400 步 fine-tune 后，可以解决长达 1M 的任务。</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th>Zero-shot</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
<td>32K</td>
<td>128K</td>
<td>256K</td>
<td>512K</td>
<td>1M</td>
</tr>
<tr>
<td style="text-align:left">Infini-Transformer (Linear)</td>
<td>14/13/98</td>
<td>11/14/100</td>
<td>6/3/100</td>
<td>6/7/99</td>
<td>8/6/98</td>
</tr>
<tr>
<td style="text-align:left">Infini-Transformer (Linear + Delta)</td>
<td>13/11/99</td>
<td>6/9/99</td>
<td>7/5/99</td>
<td>6/8/97</td>
<td>7/6/97</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td>FT (400 steps)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Infini-Transformer (Linear)</td>
<td>100/100/100</td>
<td>100/100/100</td>
<td>100/100/100</td>
<td>97/99/100</td>
<td>96/94/100</td>
</tr>
<tr>
<td style="text-align:left">Infini-Transformer (Linear + Delta)</td>
<td>100/100/100</td>
<td>100/100/99</td>
<td>100/100/99</td>
<td>100/100/100</td>
<td>100/100/100</td>
</tr>
</tbody>
</table>
<p><em>表3：当在长度为5K的输入上进行微调时，Infini-Transformers解决了带有长达1M的上下文长度的口令(passkey)任务。我们报告了在长度为32K到1M的长输入中隐藏的口令在token级别上的检索准确率，这些口令位于不同部分（<em>开始/中间/结束</em>）。</em></p>
<p><strong>500K 长度的书籍摘要（BookSum）</strong>。我们进一步扩展了我们的方法，通过对 8K 输入长度进行 30K 步持续预训练，连续预训练了一个 80 亿参数的 LLM 模型。然后我们在书籍摘要任务 BookSum（Kryscinski等人，2021）上进行了微调，该任务的目标是生成整本书的摘要。</p>
<p>我们将输入长度设置为 32K 进行微调，并将其增加到 500K 以进行评估。我们使用生成温度为 0.5 和 $top_{p} = 0.95$，并将解码步骤数量设置为 1024，以生成每本书的摘要。</p>
<p>表4 将我们的模型与专门用于摘要任务的编码器-解码器模型（Lewis等人，2019；Xiao等人，2021）及其基于检索的长上下文扩展（Bertsch等人，2024）进行了比较。我们的模型在 BookSum 上超越了先前的最佳结果，并通过处理整本书的文本实现了 BookSum 上的新 SOTA。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th>Rouge-1</th>
<th>Rouge-2</th>
<th>Rouge-L</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">BART</td>
<td>36.4</td>
<td>7.6</td>
<td>15.3</td>
<td>16.2</td>
</tr>
<tr>
<td style="text-align:left">BART + Unlimiformer</td>
<td>36.8</td>
<td>8.3</td>
<td>15.7</td>
<td>16.9</td>
</tr>
<tr>
<td style="text-align:left">PRIMERA</td>
<td>38.6</td>
<td>7.2</td>
<td>15.6</td>
<td>16.3</td>
</tr>
<tr>
<td style="text-align:left">PRIMERA + Unlimiformer</td>
<td>37.9</td>
<td>8.2</td>
<td>16.3</td>
<td>17.2</td>
</tr>
<tr>
<td style="text-align:left">Infini-Transformers (Linear)</td>
<td>37.9</td>
<td>8.7</td>
<td>17.6</td>
<td>18.0</td>
</tr>
<tr>
<td style="text-align:left">Infini-Transformers (Linear + Delta)</td>
<td><strong>40.0</strong></td>
<td><strong>8.8</strong></td>
<td><strong>17.9</strong></td>
<td><strong>18.5</strong></td>
</tr>
</tbody>
</table>
<p><em>表4：500K长度的书籍摘要（BookSum）结果。BART、PRIMERA 和 Unlimiformer 的结果来自 (Bertsch等 2024)。</em></p>
<p>我们还在图4 中绘制了 BookSum 数据验证集上的整体 Rouge 分数。有一个清晰的趋势显示，随着提供给书籍的输入文本越来越多，我们的 Infini-Transformers 在摘要性能指标上有所提升。</p>
<p><img src="https://arxiv.org/html/2404.07143v1/x4.png" alt=""></p>
<p><em>图 4：Infini-Transformer 使用更多的书籍文本作为输入获得更好的 Rouge 总体分数。</em></p>
<h2 id="4-相关工作">4 相关工作</h2>
<p><strong>压缩内存(Compressive memory)。</strong> 受生物神经元可塑性的启发（Munkhdalai等人，2017；Miconi等人，2018），压缩内存方法将参数化函数视为内存来存储和检索信息（Hinton等人，1987；Schmidhuber，1992；Ba和Hinton，2016；Munkhdalai和Yu，2019）。与Transformer KV 内存数组（Vaswani等人，2017；Wu等人，2022）不同，后者随着输入序列长度增长，压缩内存系统保持恒定数量的内存参数以提高计算效率。这些参数通过更新规则进行修改以存储信息，然后通过内存读取机制检索信息（Graves等人，2014；Sukhbaatar等人，2015；Munkhdalai等人，2017）。</p>
<p>压缩的输入表示可以视为过去序列片段的摘要（Rae等人，2019；Chevalier等人，2023）。沿着这个方向，更近期的工作已经利用Transformer LLM 本身来压缩输入序列，以实现高效的长上下文建模（Bulatov等人，2022；Chevalier等人，2023；Ge等人，2023；Mu等人，2024）。然而，先前的段级压缩方法，包括压缩Transformer（Rae等人，2019），仍会丢弃旧段的内存条目，以释放空间给新段，从而将上下文窗口限制为最近的段。这与我们的Infini-attention相反，后者以递归方式对固定数量的内存参数进行增量内存更新。</p>
<p><strong>长上下文持续预训练(Long-context continual pre-training)。</strong> 有一系列工作在扩展点乘注意力层的基础上继续对LLM 进行训练，以处理长上下文（Xiong等人，2023；Fu等人，2024）。这些注意力扩展包括将稀疏性纳入注意力层（Chen等人，2023；Ratner等人，2022；Mohtashami等人，2024），以及操纵位置编码（Chen等人，2023；Peng等人，2023）。虽然基于位置编码的方法，如位置插值技术（Chen等人，2023），可以是数据高效的，因为它们只调整了注意力层中的位置偏差，但它们在推理方面仍然昂贵。</p>
<p>注意机制也容易出现注意力聚焦问题（Xiao等人，2023）和迷失在中间问题（Liu等人，2024）。因此，它们在上下文长度超出训练中观察到的长度时往往会遇到困难。提出的Infini-attention通过启用固定的本地注意力窗口，以段级流式计算长序列，解决了这些问题。我们的Infini-Transformers在训练32K甚至5K长度序列时，成功地推广到了100万长度的输入范围。</p>
<p><strong>高效的注意力(Efficient attention)。</strong> 高效的注意力技术试图通过近似或系统级优化来改善点乘注意力的效率。不同形式的高效注意力近似已经探索了多个方向，包括基于稀疏性的（Child等人，2019；Beltagy等人，2020；Sukhbaatar等人，2021；Ding等人，2023）和线性注意力近似（Shen和Tao，2018；Katharopoulos和Fleuret，2020；Schlag等人，2021）。在这些方法中，线性注意力变种与关联内存矩阵（Schlag等人，2020；Schlag等人，2021）和元学习神经内存（Munkhdalai和Yu，2019）密切相关，其中KV 绑定（Smolensky，1990）存储在快速权重（Hinton等人，1987；Schmidhuber，1992；Ba和Hinton，2016）中，这些权重根据新的上下文信息进行修改。最近，通过利用特定硬件架构来使精确的注意力计算更加高效，提出了系统级优化技术（Dao等人，2022；Liu等人，2023）。</p>
<h2 id="5-结论">5 结论</h2>
<p>有效的内存系统不仅对于理解LLM中的长上下文至关重要，而且对于推理、规划、持续适应新知识甚至学习如何学习也是至关重要的。本文介绍了将压缩内存模块紧密集成到普通点乘注意力层中。这对注意力层的微妙但关键修改使LLM能够在有界的内存和计算资源下处理无限长的上下文。我们展示了我们的方法可以自然地扩展到百万长度的输入序列范围，同时在长上下文语言建模基准测试和书籍摘要任务上胜过了基线。我们还展示了我们方法的有希望的长度泛化能力。fine-tune在长度为5K的序列上的1B模型解决了1M长度的问题。</p>
<h2 id="references">References</h2>
<ol>
<li>
<p>R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
</li>
<li>
<p>J. Ba, G. E. Hinton, V. Mnih, J. Z. Leibo, and C. Ionescu. Using fast weights to attend to the recent past. Advances in neural information processing systems, 29, 2016.</p>
</li>
<li>
<p>D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
</li>
<li>
<p>I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.</p>
</li>
<li>
<p>A. Bertsch, U. Alon, G. Neubig, and M. Gormley. Unlimiformer: Longrange transformers with unlimited length input. Advances in Neural Information Processing Systems, 36, 2024.</p>
</li>
<li>
<p>T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.</p>
</li>
<li>
<p>A. Bulatov, Y. Kuratov, and M. Burtsev. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11079–11091, 2022.</p>
</li>
<li>
<p>S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a.</p>
</li>
<li>
<p>Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, and J. Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023b.</p>
</li>
<li>
<p>J. Cheng, L. Dong, and M. Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.</p>
</li>
<li>
<p>A. Chevalier, A. Wettig, A. Ajith, and D. Chen. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788, 2023.</p>
</li>
<li>
<p>R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.</p>
</li>
<li>
<p>D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.</p>
</li>
<li>
<p>Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.</p>
</li>
<li>
<p>T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R´e. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.</p>
</li>
<li>
<p>J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, N. Zheng, and F. Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023.</p>
</li>
<li>
<p>Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data engineering for scaling language models to 128k context. arXiv preprint arXiv:2402.10171, 2024.</p>
</li>
<li>
<p>T. Ge, J. Hu, X. Wang, S.-Q. Chen, and F. Wei. In-context autoencoder for context compression in a large language model. arXiv preprint arXiv:2307.06945, 2023.</p>
</li>
<li>
<p>A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.</p>
</li>
<li>
<p>D. Groeneveld, I. Beltagy, P. Walsh, A. Bhagia, R. Kinney, O. Tafjord, A. H. Jha, H. Ivison, I. Magnusson, Y. Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024.</p>
</li>
<li>
<p>D. O. Hebb. The organization of behavior: A neuropsychological theory. Psychology press, 2005.</p>
</li>
<li>
<p>G. E. Hinton and D. C. Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pp. 177–186, 1987.</p>
</li>
<li>
<p>J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554–2558, 1982.</p>
</li>
<li>
<p>P. Kanerva. Sparse distributed memory. MIT press, 1988.</p>
</li>
<li>
<p>A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 5156–5165. PMLR, 2020.</p>
</li>
<li>
<p>A. Kazemnejad, I. Padhi, K. Natesan Ramamurthy, P. Das, and S. Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024.</p>
</li>
<li>
<p>W. Kry´sci´nski, N. Rajani, D. Agarwal, C. Xiong, and D. Radev. Booksum: A collection of datasets for long-form narrative summarization. arXiv preprint arXiv:2105.08209, 2021.</p>
</li>
<li>
<p>M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.</p>
</li>
<li>
<p>H. Liu, M. Zaharia, and P. Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023.</p>
</li>
<li>
<p>N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157–173, 2024.</p>
</li>
<li>
<p>T. Miconi, K. Stanley, and J. Clune. Differentiable plasticity: training plastic neural networks with backpropagation. In International Conference on Machine Learning, pp. 3559–3568. PMLR, 2018.</p>
</li>
<li>
<p>A. Mohtashami and M. Jaggi. Random-access infinite context length for transformers. Advances in Neural Information Processing Systems, 36, 2024.</p>
</li>
<li>
<p>J. Mu, X. Li, and N. Goodman. Learning to compress prompts with gist tokens. Advances in Neural Information Processing Systems, 36, 2024.</p>
</li>
<li>
<p>T. Munkhdalai and H. Yu. Meta networks. In International conference on machine learning, pp. 2554–2563. PMLR, 2017a.</p>
</li>
<li>
<p>T. Munkhdalai and H. Yu. Neural semantic encoders. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 1, pp. 397. NIH Public Access, 2017b.</p>
</li>
<li>
<p>T. Munkhdalai, J. P. Lalor, and H. Yu. Citation analysis with neural attention models. In Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis, pp. 69–77, 2016.</p>
</li>
<li>
<p>T. Munkhdalai, A. Sordoni, T. Wang, and A. Trischler. Metalearned neural memory. Advances in Neural Information Processing Systems, 32, 2019.</p>
</li>
<li>
<p>B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.</p>
</li>
<li>
<p>R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.</p>
</li>
<li>
<p>O. Press, N. A Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.</p>
</li>
<li>
<p>J. W Rae, A. Potapenko, S. M Jayakumar, and T. P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.</p>
</li>
<li>
<p>C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.</p>
</li>
<li>
<p>N. Ratner, Y. Levine, Y. Belinkov, O. Ram, O. Abend, E. Karpas, A. Shashua, K. Leyton-Brown, and Y. Shoham. Parallel context windows improve in-context learning of large language models. arXiv preprint arXiv:2212.10947, 2022.</p>
</li>
<li>
<p>I. Schlag, P. Smolensky, R. Fernandez, N. Jojic, J. Schmidhuber, and J. Gao. Enhancing the transformer with explicit relational encoding for math problem solving. arXiv preprint arXiv:1910.06611, 2019.</p>
</li>
<li>
<p>I. Schlag, T. Munkhdalai, and J. Schmidhuber. Learning associative inference using fast weight memory. arXiv preprint arXiv:2011.07831, 2020.</p>
</li>
<li>
<p>I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 9355–9366. PMLR, 2021.</p>
</li>
<li>
<p>J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131–139, 1992.</p>
</li>
<li>
<p>N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 4596–4604. PMLR, 2018.</p>
</li>
<li>
<p>Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li. Efficient attention: Attention with linear complexities. arXiv preprint arXiv:1812.01243, 2018.</p>
</li>
<li>
<p>P. Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159–216, 1990.</p>
</li>
<li>
<p>S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end memory networks. Advances in neural information processing systems, 28, 2015.</p>
</li>
<li>
<p>S. Sukhbaatar, D. Ju, S. Poff, S. Roller, A. Szlam, J. Weston, and A. Fan. Not all memories are created equal: Learning to forget by expiring. In International Conference on Machine Learning, pp. 9902–9912. PMLR, 2021.</p>
</li>
<li>
<p>H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
</li>
<li>
<p>A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
</li>
<li>
<p>Y. Wu, M. N Rabe, D. Hutchins, and C. Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022.</p>
</li>
<li>
<p>G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.</p>
</li>
<li>
<p>W. Xiao, I. Beltagy, G. Carenini, and A. Cohan. Primera: Pyramidbased masked sentence pre-training for multi-document summarization. arXiv preprint arXiv:2110.08499, 2021.</p>
</li>
<li>
<p>W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. Sankararaman, B. Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.</p>
</li>
</ol>
<h2 id="附录">附录</h2>
<h3 id="a-额外的训练细节">A 额外的训练细节</h3>
<p>对于长上下文语言建模任务，我们将学习率设置为0.01，通过对0.003、0.005、0.01和0.03的值进行小范围搜索。我们使用Adafactor优化器（Shazeer和Stern，2018），其中包括线性预热1000步，然后是余弦衰减。我们在每个段之后应用梯度检查点以节省内存。批量大小设置为64。对于LLM实验，我们在持续预训练和任务微调期间将学习率设置为0.0001。</p>
<h3 id="b-口令passkey检索任务"><strong>B 口令(passkey)检索任务</strong></h3>
<p>下面是口令(passkey)任务的输入格式。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I
will quiz you about the important information there. The grass is green. The sky is blue. The sun
is yellow. Here we go. There and back again. (repeat x times) The pass key is 9054. Remember
it. 9054 is the pass key. The grass is green. The sky is blue. The sun is yellow. Here we go.
There and ack again. (repeat y times) What is the pass key? The pass key is
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">在许多无关文本中隐藏了重要信息。找到它并记住它们。我将在那里询问你关于重要信息。草是绿色的。天空是蓝色的。太阳是黄色的。我们开始吧。再来一次。 (重复 x 次) 口令(passkey)是 9054。记住它。9054 就是口令(passkey)。草是绿色的。天空是蓝色的。太阳是黄色的。我们开始吧。再来一次。 (重复 y 次) 口令(passkey)是什么？口令(passkey)是
</code></pre></div>
    </div>

    
    


    
    

    <footer class="post-footer">
      <div class="post-tags">
          <a href="https://weedge.github.io/tags/llm/">LLM</a>
          <a href="https://weedge.github.io/tags/model/">model</a>
          <a href="https://weedge.github.io/tags/transformer/">transformer</a>
          
        </div>

      
      <nav class="post-nav">
        
        
          <a class="next" href="/post/nlp/translate_model_inference_and_finetune/">
            <span class="next-text nav-default">翻译模型 inference 和 微调</span>
            <span class="prev-text nav-mobile">下一篇</span>
            
            <i class="iconfont">
              <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

            </i>
          </a>
      </nav>
    </footer>
  </article>

  
  

  
  

  

  
  

  

  

  <div class="disqus-comment">
  <div class="disqus-button" id="load_disqus" onclick="load_disqus()">
    显示 Disqus 评论
  </div>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = "https://weedge.github.io/post/paper/transformer/infini_attention/";
    };
    function load_disqus() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'weedge';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

      $('#load_disqus').remove();
    };
  </script>
  <noscript>Please enable JavaScript to view the
    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  </div>

    

  

        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="icon-links">
  
  
    <a href="mailto:weege007@gmail.com" rel="me noopener" class="iconfont"
      title="email" >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://github.com/weedge" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://weibo.com/weedge" rel="me noopener" class="iconfont"
      title="weibo"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M385.714286 733.714286q12-19.428571 6.285714-39.428571t-25.714286-28.571429q-19.428571-8-41.714286-0.571429t-34.285714 26.285714q-12.571429 19.428571-7.428571 39.142857t24.571429 28.857143 42.571429 1.428571 35.714286-27.142857zm53.714286-69.142857q4.571429-7.428571 2-15.142857t-10-10.571429q-8-2.857143-16.285714 2.857143t-12.285714 10.571429q-9.714286 17.714286 7.428571 25.714286 8 2.857143 16.571429 2.857143t12.571429-10.571429zm99.428571 61.142857q-25.714286 58.285714-90.285714 85.714286t-128 6.857143q-61.142857-19.428571-84.285714-72.285714t3.714286-107.142857q26.857143-53.142857 86.571429-79.428571t120.285714-10.857143q63.428571 16.571429 90.571429 68.285714t1.428571 108.857143zm178.285714-91.428571q-5.142857-54.857143-50.857143-97.142857t-119.142857-62.285714-156.857143-12q-127.428571 13.142857-211.142857 80.857143t-75.714286 151.142857q5.142857 54.857143 50.857143 97.142857t119.142857 62.285714 156.857143 12q127.428571-13.142857 211.142857-80.857143t75.714286-151.142857zm176 2.285714q0 38.857143-21.142857 79.714286t-62.285714 78.285714-96.285714 67.142857-129.142857 47.428571-154.571429 17.714286-157.142857-19.142857-137.428571-53.142857-98-86.285714-37.142857-114q0-65.714286 39.714286-140t112.857143-147.428571q96.571429-96.571429 195.142857-134.857143t140.857143 4q37.142857 36.571429 11.428571 119.428571-2.285714 8-0.571429 11.428571t5.714286 4 8.285714 2.857143 7.714286-2l3.428571-1.142857q79.428571-33.714286 140.571429-33.714286t87.428571 34.857143q25.714286 36 0 101.714286-1.142857 7.428571-2.571429 11.428571t2.571429 7.142857 6.857143 4.285714 9.714286 3.428571q32.571429 10.285714 58.857143 26.857143t45.714286 46.571429 19.428571 66.571429zm-42.285714-356.571429q24 26.857143 31.142857 62t-3.714286 67.142857q-4.571429 13.142857-16.857143 19.428571t-25.428571 2.285714q-13.142857-4.571429-19.428571-16.857143t-2.285714-25.428571q11.428571-36-13.714286-63.428571t-61.142857-20q-13.714286 2.857143-25.714286-4.571429t-14.285714-21.142857q-2.857143-13.714286 4.571429-25.428571t21.142857-14.571429q34.285714-7.428571 68 3.142857t57.714286 37.428571zm103.428571-93.142857q49.714286 54.857143 64.285714 127.142857t-7.714286 138q-5.142857 15.428571-19.428571 22.857143t-29.714286 2.285714-22.857143-19.428571-2.857143-29.714286q16-46.857143 5.714286-98.285714t-45.714286-90.285714q-35.428571-39.428571-84.571429-54.571429t-98.857143-4.857143q-16 3.428571-29.714286-5.428571t-17.142857-24.857143 5.428571-29.428571 24.857143-16.857143q70.285714-14.857143 139.428571 6.571429t118.857143 76.857143z"></path>
</svg>

    </a>


<a href="https://weedge.github.io/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
   
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2013 -
    2024
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        weedge
        
      </span></span>

  
  

  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont">
        
        <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

      </i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js" integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin="anonymous"></script>












  
    <script type="text/javascript" src="/js/load-photoswipe.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  









  <script id="dsq-count-scr" src="//weedge.disqus.com/count.js" async></script>






  <script src="/js/copy-to-clipboard.js"></script>


</body>
</html>
