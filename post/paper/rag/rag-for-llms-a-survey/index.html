<!DOCTYPE html>
<html lang="zh-cn" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>论文：Retrieval-Augmented Generation for Large Language Models: A Survey [v4] - 时间飘过</title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="weedge" />
  <meta name="description" content="大型语言模型（LLMs）展示了显著的能力，但面临着幻觉、过时知识和不透明、不可追踪的推理过程等挑战。检索增强生成（RAG）已经成为一个有前途的解决方案，通过整合外部数据库的知识。这增强了模型的准确性和可信度，特别适用于知识密集型任务，并允许持续的知识更新和领域特定信息的整合。RAG通过将LLMs的内在知识与庞大、动态的外部数据库资源相结合，产生了协同效应。这篇综述论文详细考察了RAG范式的发展，包括朴素RAG、高级RAG和模块化RAG。它对RAG框架的三方基础进行了细致的了解，其中包括检索、生成和增强技术。该论文强调嵌入(embedding)在每个关键组成部分的最先进技术，并提对RAG系统进展的深入研究了解。此外，该论文介绍了评估RAG模型的指标和基准，以及最新的评估框架。最后，该论文讲了一些研究前景，包括未来挑战、多模态的扩展以及RAG基础设施及其生态系统的进展1。
论文地址: Retrieval-Augmented Generation for Large Language Models: A Survey | PPT
注： 主要是了解RAG的发展过程(召回率)，以及对相关子模块领域的现阶段了解，如果感兴趣，通过索引到论文引用处进一步了解。(提高看相应论文的准确率)
" />

  <meta name="keywords" content="工作, 技术, 生活" />






<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://weedge.github.io/post/paper/rag/rag-for-llms-a-survey/" />





<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css" integrity="sha256-&#43;ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media="screen" crossorigin="anonymous">




<link rel="stylesheet" href="/css/copy-to-clipboard.css">


<meta property="og:title" content="论文：Retrieval-Augmented Generation for Large Language Models: A Survey [v4]" />
<meta property="og:description" content="大型语言模型（LLMs）展示了显著的能力，但面临着幻觉、过时知识和不透明、不可追踪的推理过程等挑战。检索增强生成（RAG）已经成为一个有前途的解决方案，通过整合外部数据库的知识。这增强了模型的准确性和可信度，特别适用于知识密集型任务，并允许持续的知识更新和领域特定信息的整合。RAG通过将LLMs的内在知识与庞大、动态的外部数据库资源相结合，产生了协同效应。这篇综述论文详细考察了RAG范式的发展，包括朴素RAG、高级RAG和模块化RAG。它对RAG框架的三方基础进行了细致的了解，其中包括检索、生成和增强技术。该论文强调嵌入(embedding)在每个关键组成部分的最先进技术，并提对RAG系统进展的深入研究了解。此外，该论文介绍了评估RAG模型的指标和基准，以及最新的评估框架。最后，该论文讲了一些研究前景，包括未来挑战、多模态的扩展以及RAG基础设施及其生态系统的进展1。
论文地址:  Retrieval-Augmented Generation for Large Language Models: A Survey |  PPT
注： 主要是了解RAG的发展过程(召回率)，以及对相关子模块领域的现阶段了解，如果感兴趣，通过索引到论文引用处进一步了解。(提高看相应论文的准确率)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://weedge.github.io/post/paper/rag/rag-for-llms-a-survey/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-03-08T10:26:23+08:00" />
<meta property="article:modified_time" content="2024-03-08T10:26:23+08:00" />

<meta itemprop="name" content="论文：Retrieval-Augmented Generation for Large Language Models: A Survey [v4]">
<meta itemprop="description" content="大型语言模型（LLMs）展示了显著的能力，但面临着幻觉、过时知识和不透明、不可追踪的推理过程等挑战。检索增强生成（RAG）已经成为一个有前途的解决方案，通过整合外部数据库的知识。这增强了模型的准确性和可信度，特别适用于知识密集型任务，并允许持续的知识更新和领域特定信息的整合。RAG通过将LLMs的内在知识与庞大、动态的外部数据库资源相结合，产生了协同效应。这篇综述论文详细考察了RAG范式的发展，包括朴素RAG、高级RAG和模块化RAG。它对RAG框架的三方基础进行了细致的了解，其中包括检索、生成和增强技术。该论文强调嵌入(embedding)在每个关键组成部分的最先进技术，并提对RAG系统进展的深入研究了解。此外，该论文介绍了评估RAG模型的指标和基准，以及最新的评估框架。最后，该论文讲了一些研究前景，包括未来挑战、多模态的扩展以及RAG基础设施及其生态系统的进展1。
论文地址:  Retrieval-Augmented Generation for Large Language Models: A Survey |  PPT
注： 主要是了解RAG的发展过程(召回率)，以及对相关子模块领域的现阶段了解，如果感兴趣，通过索引到论文引用处进一步了解。(提高看相应论文的准确率)"><meta itemprop="datePublished" content="2024-03-08T10:26:23+08:00" />
<meta itemprop="dateModified" content="2024-03-08T10:26:23+08:00" />
<meta itemprop="wordCount" content="32853">
<meta itemprop="keywords" content="RAG,LLM," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="论文：Retrieval-Augmented Generation for Large Language Models: A Survey [v4]"/>
<meta name="twitter:description" content="大型语言模型（LLMs）展示了显著的能力，但面临着幻觉、过时知识和不透明、不可追踪的推理过程等挑战。检索增强生成（RAG）已经成为一个有前途的解决方案，通过整合外部数据库的知识。这增强了模型的准确性和可信度，特别适用于知识密集型任务，并允许持续的知识更新和领域特定信息的整合。RAG通过将LLMs的内在知识与庞大、动态的外部数据库资源相结合，产生了协同效应。这篇综述论文详细考察了RAG范式的发展，包括朴素RAG、高级RAG和模块化RAG。它对RAG框架的三方基础进行了细致的了解，其中包括检索、生成和增强技术。该论文强调嵌入(embedding)在每个关键组成部分的最先进技术，并提对RAG系统进展的深入研究了解。此外，该论文介绍了评估RAG模型的指标和基准，以及最新的评估框架。最后，该论文讲了一些研究前景，包括未来挑战、多模态的扩展以及RAG基础设施及其生态系统的进展1。
论文地址:  Retrieval-Augmented Generation for Large Language Models: A Survey |  PPT
注： 主要是了解RAG的发展过程(召回率)，以及对相关子模块领域的现阶段了解，如果感兴趣，通过索引到论文引用处进一步了解。(提高看相应论文的准确率)"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->



<script>
  MathJax = {
    tex: {
      inlineMath: [["$", "$"]],
    },
    displayMath: [
      ["$$", "$$"],
      ["\[\[", "\]\]"],
    ],
    svg: {
      fontCache: "global",
    },
  };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>





</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">时间飘过</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/">主页</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/post/">归档</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/tags/">标签</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/categories/">分类</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/about/">About</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/perf-book-cn/zh/" rel="noopener" target="_blank">
              《现代CPU性能分析与优化》
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          
        
      </li>
    

    
  </ul>
</nav>


  
    






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

  

  

  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      时间飘过
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/">主页</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/post/">归档</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/tags/">标签</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/categories/">分类</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/about/">About</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/perf-book-cn/zh/" rel="noopener" target="_blank">
              《现代CPU性能分析与优化》
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">论文：Retrieval-Augmented Generation for Large Language Models: A Survey [v4]</h1>
      
      <div class="post-meta">
        <time datetime="2024-03-08" class="post-time">
          2024-03-08
        </time>
        <div class="post-category">
            <a href="https://weedge.github.io/categories/rag/"> RAG </a>
            <a href="https://weedge.github.io/categories/llm/"> LLM </a>
            <a href="https://weedge.github.io/categories/%E6%8A%80%E6%9C%AF/"> 技术 </a>
            <a href="https://weedge.github.io/categories/paper/"> paper </a>
            
          </div>
        

        
        

        
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#第1章引言">第1章：引言</a></li>
    <li><a href="#第2章定义">第2章：定义</a></li>
    <li><a href="#第3章rag框架">第3章：RAG框架</a>
      <ul>
        <li><a href="#31-朴素ragnaive-rag">3.1 朴素RAG(<strong>Naive RAG</strong>)</a></li>
        <li><a href="#32-高级rag-advanced-rag">3.2 高级RAG( <strong>Advanced RAG</strong>)</a></li>
        <li><a href="#33-模块化ragmodular-rag">3.3 模块化RAG(<strong>Modular RAG</strong>)</a></li>
      </ul>
    </li>
    <li><a href="#第4章检索">第4章：检索</a>
      <ul>
        <li><a href="#41-增强语义表示enhancing-semantic-representations">4.1 增强语义表示(<strong>Enhancing Semantic Representations</strong>)</a></li>
        <li><a href="#42-对齐查询和文档aligning-queries-and-documents">4.2 对齐查询和文档(<strong>Aligning Queries and Documents</strong>)</a></li>
        <li><a href="#43-对齐检索器和llmaligning-retriever-and-llm">4.3 对齐检索器和LLM(<strong>Aligning Retriever and LLM</strong>)</a></li>
      </ul>
    </li>
    <li><a href="#第5章生成">第5章：生成</a>
      <ul>
        <li><a href="#51-后检索处理与冻结的llmpost-retrieval-with-frozen-llm">5.1 后检索处理与冻结的LLM(<strong>Post-retrieval with Frozen LLM</strong>)</a></li>
        <li><a href="#52-为rag微调llm-fine-tuning-llm-for-rag">5.2 为RAG微调LLM( <strong>Fine-tuning LLM for RAG</strong>)</a></li>
      </ul>
    </li>
    <li><a href="#第6章rag中的增强">第6章：RAG中的增强</a>
      <ul>
        <li><a href="#61-rag-增强阶段-rag-in-augmentation-stages">6.1 RAG 增强阶段( <strong>RAG in Augmentation Stages</strong>)</a></li>
        <li><a href="#62-增强来源augmentation-source">6.2 增强来源(<strong>Augmentation Source</strong>)</a></li>
        <li><a href="#63-增强过程augmentation-process">6.3 增强过程(<strong>Augmentation Process</strong>)</a></li>
        <li><a href="#64-rag与微调rag-vs-fine-tuning">6.4 RAG与微调(<strong>RAG vs Fine-Tuning</strong>)</a></li>
      </ul>
    </li>
    <li><a href="#第7章rag评估">第7章：RAG评估</a>
      <ul>
        <li><a href="#71-评估目标evaluation-targets">7.1 评估目标(<strong>Evaluation Targets</strong>)</a></li>
        <li><a href="#72-评估方面evaluation-aspects">7.2 评估方面(<strong>Evaluation Aspects</strong>)</a></li>
        <li><a href="#73-评估基准和工具evaluation-benchmarks-and-tools">7.3 评估基准和工具(<strong>Evaluation Benchmarks and Tools</strong>)</a></li>
      </ul>
    </li>
    <li><a href="#第8章未来前景">第8章：未来前景</a>
      <ul>
        <li><a href="#81-rag的未来挑战future-challenges-of-rag">8.1 RAG的未来挑战(<strong>Future Challenges of RAG</strong>)</a></li>
        <li><a href="#82-rag生态系统-ecosystem-of-rag">8.2 RAG生态系统( <strong>Ecosystem of RAG</strong>)</a></li>
      </ul>
    </li>
    <li><a href="#第9章结论">第9章：结论</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
  </div>
</div>

    
    <div class="post-content">
      <p>大型语言模型（LLMs）展示了显著的能力，但面临着幻觉、过时知识和不透明、不可追踪的推理过程等挑战。检索增强生成（RAG）已经成为一个有前途的解决方案，通过整合外部数据库的知识。这增强了模型的准确性和可信度，特别适用于知识密集型任务，并允许持续的知识更新和领域特定信息的整合。RAG通过将LLMs的内在知识与庞大、动态的外部数据库资源相结合，产生了协同效应。这篇综述论文详细考察了RAG范式的发展，包括朴素RAG、高级RAG和模块化RAG。它对RAG框架的三方基础进行了细致的了解，其中包括检索、生成和增强技术。该论文强调嵌入(embedding)在每个关键组成部分的最先进技术，并提对RAG系统进展的深入研究了解。此外，该论文介绍了评估RAG模型的指标和基准，以及最新的评估框架。最后，该论文讲了一些研究前景，包括未来挑战、多模态的扩展以及RAG基础设施及其生态系统的进展<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>论文地址:  <a href="https://arxiv.org/pdf/2312.10997.pdf">Retrieval-Augmented Generation for Large Language Models: A Survey</a> |  <a href="https://github.com/Tongji-KGLLM/RAG-Survey/blob/main/assets/RAG_Slide_ENG.pdf">PPT</a></p>
<p>注： 主要是了解RAG的发展过程(召回率)，以及对相关子模块领域的现阶段了解，如果感兴趣，通过索引到论文引用处进一步了解。(提高看相应论文的准确率)</p>
<h2 id="第1章引言">第1章：引言</h2>
<p>大型语言模型（LLMs）如GPT系列和LLama系列在自然语言处理方面取得了显著的成功，展示了在各种基准测试中的卓越性能，包括SuperGLUE、MMLU和BIG-bench。尽管取得了这些进步，LLMs在处理特定领域或高度专业化的查询时表现出明显的局限性。一个常见问题是生成错误信息，或称为“幻觉”，尤其是当查询超出模型的训练数据或需要最新信息时。这些缺点强调了在没有额外保障措施的情况下，将LLMs作为黑盒解决方案部署在现实世界生产环境中的不切实际性。缓解这些局限性的一个有前景的方法是检索增强生成（Retrieval-Augmented Generation, RAG），它通过将外部数据检索整合到生成过程中，从而增强了模型提供准确和相关响应的能力。</p>
<p>RAG由Lewis等人在2020年中期引入，作为LLMs领域内的一种范式，增强了生成任务。具体来说，RAG涉及一个初始检索步骤，其中LLMs查询外部数据源以获取相关信息，然后再进行回答问题或生成文本。这个过程不仅为后续的生成阶段提供信息，还确保响应基于检索到的证据，从而显著提高了输出的准确性和相关性。在推理阶段从知识库动态检索信息允许RAG解决生成事实上不正确内容的问题，通常称为“幻觉”。将RAG整合到LLMs中已经迅速被采用，并成为完善聊天机器人能力和使LLMs更适用于实际应用的关键技术。</p>
<p>RAG的发展轨迹经历了四个不同的阶段，如图1所示。在其2017年的起源时，与Transformer架构的出现相一致，主要重点是通过预训练模型（PTM）吸收额外的知识来增强语言模型。这个时期见证了RAG的基础工作主要集中在优化预训练方法上。在chatGPT出现之前，有一个相对休眠的时期，RAG相关研究的进展很少。</p>
<p><img src="https://github.com/weedge/mypic/raw/master/rag/RAG_for_LLMs_A_Survey/1.png" alt=""></p>
<p>随后chatGPT的到来标志着RAG发展轨迹中的一个关键时刻，将LLMs推向了前沿。社区的焦点转向利用LLMs的能力以实现更高的可控性并满足不断演变的需求。因此，大部分RAG工作集中在推理上，少数致力于微调过程。随着LLMs能力的持续进步，特别是随着GPT-4的引入，RAG技术领域经历了重大变革。重点发展成为一种混合方法，结合了RAG和微调的优势，同时还有一小部分继续专注于优化预训练方法。</p>
<p>尽管RAG研究迅速增长，但该领域缺乏系统的整合和抽象，这在理解RAG进步的全面景观方面带来了挑战。本综述旨在概述整个RAG过程，并包含RAG研究的当前和未来方向，通过全面检查LLMs中的检索增强。</p>
<p>因此，本文旨在全面总结和组织技术原理、发展历史、内容，特别是RAG的相关方法和应用，以及评估方法和应用场景。它旨在为读者和实践者提供一个全面和系统的理解和分析大型模型和RAG，阐明检索增强的进展和关键技术，明确各种技术的优缺点及其适用背景，并预测潜在的未来发展。我们的贡献如下：</p>
<ul>
<li>我们对最先进的RAG进行了全面和系统的回顾，描述了其通过包括朴素RAG、高级RAG和模块化RAG在内的范式的发展。这个回顾将RAG研究的更广泛范围置于LLMs的景观中。</li>
<li>我们确定并讨论了RAG过程中不可或缺的核心技术，特别关注“检索”、“生成器”和“增强”方面，并深入探讨它们的协同作用，阐明这些组件如何复杂地协作形成一个连贯有效的RAG框架。</li>
<li>我们构建了一个全面的RAG评估框架，概述了评估目标和指标。我们的比较分析从各种角度澄清了RAG与微调相比的优势和劣势。此外，我们预测了RAG的未来方向，强调了解决当前挑战、扩展到多模态设置和其生态系统发展的潜在增强。</li>
</ul>
<p>本文的结构如下：第2和3节定义了RAG并详细描述了其发展过程。第4至6节探讨了核心组件——检索、“生成”和“增强”——突出了嵌入的各种技术。第7节专注于RAG的评估系统。第8节将RAG与其他LLM优化方法进行比较，并提出其发展的潜在方向。本文在第9节结束。</p>
<h2 id="第2章定义">第2章：定义</h2>
<p>RAG的定义可以从其工作流程中总结出来。图2展示了一个典型的RAG应用工作流程。在这种情况下，用户询问ChatGPT关于一个最近引起公众广泛讨论的高调事件（即OpenAI首席执行官的突然解雇和重新任命）。作为最著名和广泛使用的LLM，ChatGPT受限于其预训练数据，缺乏对最近事件的了解。RAG通过从外部知识库检索最新的文档摘要来解决这一差距。在这种情况下，它获取了与查询相关的新闻文章选择。这些文章以及初始问题随后被合并到一个丰富的提示中，使ChatGPT能够合成一个知情的回应。这个例子说明了RAG过程，展示了它通过实时信息检索增强模型响应的能力。</p>
<p><img src="https://github.com/weedge/mypic/raw/master/rag/RAG_for_LLMs_A_Survey/2.png" alt=""></p>
<p>技术上，RAG通过各种创新方法得到丰富，解决了关键问题，如“检索什么”、“何时检索”和“如何使用检索到的信息”。对于“检索什么”，研究已经从简单的token[Khandelwal等人，2019]和实体检索[Nishikawa等人，2022]发展到更复杂的结构，如块[Ram等人，2023]和知识图谱[Kang等人，2023]，研究集中在检索粒度和数据结构水平上。粗粒度带来更多信息，但精度较低。检索结构化文本提供更多信息，但牺牲效率。关于“何时检索”的问题导致了从单一[Wang等人，2023e, Shi等人，2023]到自适应[Jiang等人，2023b, Huang等人，2023]和多次检索[Izacard等人，2022]策略。高频率的检索带来更多信息和更低的效率。至于“如何使用”检索到的数据，集成技术已经开发到模型架构的各个层次，包括输入[Khattab等人，2022]、中间[Borgeaud等人，2022]和输出层[Liang等人，2023]。尽管“中间”和“输出层”更有效，但存在训练需求和低效率的问题。</p>
<p>RAG是一种通过整合外部知识库来增强LLMs的范式。它采用协同方法，结合信息检索机制和上下文学习（In-Context Learning, ICL）来增强LLM的性能。在这个框架中，用户发起的查询通过搜索算法检索相关信息。然后将这些信息编织到LLM的提示中，为生成过程提供额外的上下文。RAG的关键优势在于它消除了为特定任务重新训练LLMs的需要。开发者可以添加一个外部知识库，丰富输入，从而提高模型输出的精确度。RAG已经成为LLMs系统中最受欢迎的架构之一，由于其高实用性和低门槛，许多对话产品几乎完全建立在RAG之上。</p>
<p>RAG工作流程包括三个关键步骤。首先，将语料库分割成离散的块，然后使用编码模型构建向量索引。其次，RAG根据查询和索引块的向量相似性识别和检索块。最后，模型根据从检索到的块中获得的上下文信息合成响应。这些步骤构成了RAG过程的基础框架，支撑其信息检索和上下文感知生成能力。接下来，我们将介绍RAG研究框架。</p>
<h2 id="第3章rag框架">第3章：RAG框架</h2>
<p>RAG研究范式不断发展，本节主要描述其进展。我们将其分类为三种类型：Naive RAG、Advanced RAG和Modular RAG。虽然RAG具有成本效益并超越了原生LLM的性能，但它们也表现出几个局限性。高级RAG和模块化RAG的发展是为了应对朴素RAG中的具体缺点。</p>
<h3 id="31-朴素ragnaive-rag">3.1 朴素RAG(<strong>Naive RAG</strong>)</h3>
<p>朴素RAG研究范式代表了最早的方法，它在ChatGPT广泛采用后不久获得了显著地位。朴素RAG遵循一个传统的流程，包括索引、检索和生成。它也被称为“检索-阅读”框架[Ma等人，2023a]。</p>
<h4 id="索引indexing">索引(<strong>Indexing</strong>)</h4>
<p>索引过程是数据准备中的关键初始步骤，它在离线时进行，并涉及几个阶段。它从数据索引开始，原始数据被清理和提取，各种文件格式（如PDF、HTML、Word和Markdown）被转换为标准化的纯文本。为了适应语言模型的上下文限制，这些文本随后被分割成更小、更易管理的块，这个过程被称为分块。这些块随后通过嵌入模型转换为向量表示，该模型在推理效率和模型大小之间取得了平衡。这便于在检索阶段进行相似性比较。最后，创建一个索引来存储这些文本块及其向量嵌入作为键值对，允许高效和可扩展的搜索能力。</p>
<h4 id="检索retrieval">检索(<strong>Retrieval</strong>)</h4>
<p>在收到用户查询后，系统使用与索引阶段相同的编码模型将输入转换为向量表示。然后，它计算查询向量与索引语料库中向量化块之间的相似度分数。系统优先检索与查询最相似的前K个块。这些块随后被用作解决用户请求的扩展上下文基础。</p>
<h4 id="生成generation">生成(<strong>Generation</strong>)</h4>
<p>提出的查询和选定的文档被合成为一个连贯的提示，大型语言模型被要求制定一个回应。模型回答问题的方法可能因任务特定标准而有所不同，允许它要么利用其固有的参数知识，要么将其响应限制在提供的文档所包含的信息之内。在持续对话的情况下，任何现有的对话历史都可以集成到提示中，使模型能够有效地进行多轮对话互动。</p>
<h4 id="朴素rag的缺陷">朴素RAG的缺陷</h4>
<p>朴素RAG在三个关键领域面临重大挑战：“检索”、“生成”和“增强”。检索质量提出了多样化的挑战，包括低精度，导致检索到的块与查询不一致，可能产生幻觉或中途消失的问题。低召回率也会出现，导致未能检索到所有相关块，从而阻碍LLMs制定全面响应。过时的信息进一步加剧了问题，可能导致不准确的检索结果。响应生成质量提出了幻觉挑战，模型生成的答案没有基于提供的上下文，以及模型输出中的不相关上下文和潜在的有害或偏见问题。增强过程在将检索到的段落上下文与当前生成任务有效整合方面也面临挑战，可能导致不连贯或不一致的输出。冗余和重复也是问题，特别是当多个检索到的段落包含相似信息时，会导致生成响应中的重复内容。辨别多个检索到的段落对生成任务的重要性和相关性是另一个挑战，需要适当平衡每个段落的价值。此外，调和不同写作风格和语调的差异以确保输出的一致性至关重要。最后，生成模型过度依赖增强信息的风险，可能导致输出仅重复检索到的内容，而没有提供新的价值或综合信息。</p>
<h3 id="32-高级rag-advanced-rag">3.2 高级RAG( <strong>Advanced RAG</strong>)</h3>
<p>高级RAG是为了针对性地增强朴素RAG的不足而开发的。在检索质量方面，高级RAG实施了预检索和后检索策略。为了应对朴素RAG在索引方面的挑战，高级RAG通过使用滑动窗口、细粒度分割和元数据等技术改进了其索引方法。它还引入了各种方法来优化检索过程[ILIN, 2023]。</p>
<h4 id="预检索过程pre-retrieval-process">预检索过程(<strong>Pre-Retrieval Process</strong>)</h4>
<p><strong>优化数据索引</strong>(<em>Optimizing Data Indexing</em>)。优化数据索引的目标是提高被索引内容的质量。这涉及五个主要策略：增强数据粒度(enhancing data granularity)、优化索引结构(optimizing index structures)、添加元数据(adding metadata)、对齐优化(alignment optimization)和混合检索(mixed retrieval)。</p>
<ul>
<li>
<p>增强数据粒度旨在提高文本标准化、一致性、事实准确性和丰富上下文，以提高RAG系统的性能。这包括去除无关信息、消除实体和术语的歧义、确认事实准确性、保持上下文和更新过时文档。</p>
</li>
<li>
<p>优化索引结构涉及调整块的大小以捕获相关上下文，跨多个索引路径查询，并利用图数据索引中的节点关系来捕获相关上下文。</p>
</li>
<li>
<p>添加元数据信息涉及将引用的元数据（如日期和目的）集成到块中以进行过滤，并结合元数据（如参考章节和小节）以提高检索效率。</p>
</li>
<li>
<p>对齐优化通过在文档中引入“假设性问题”[Li等人，2023d]来解决文档之间的对齐问题和差异。</p>
</li>
</ul>
<h4 id="检索retrieval-1">检索(<strong>Retrieval</strong>)</h4>
<p>在检索阶段，主要关注的是计算查询和块之间的相似度，以识别适当的上下文。嵌入模型在这一过程中起着核心作用。在高级RAG中，嵌入模型的优化是可能的。</p>
<p><strong>微调嵌入</strong>(<em>Fine-tuning Embedding</em>)。微调嵌入模型显著影响RAG系统中检索内容的相关性。这个过程涉及定制嵌入模型以增强特定领域背景下的检索相关性，特别是对于涉及不断发展或罕见术语的专业领域。BGE嵌入模型[BAAI, 2023]，如BAAI开发的BGE-large-EN<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，是一个高性能嵌入模型，可以微调以优化检索相关性。微调的训练数据可以使用像GPT-3.5-turbo这样的语言模型生成，以制定基于文档块的问题，然后用作微调对。</p>
<p><strong>动态嵌入</strong>(<em>Dynamic Embedding</em>)适应单词使用的上下文，与静态嵌入不同，静态嵌入为每个单词使用单一向量[Karpukhin等人，2020]。例如，在像BERT这样的变换器模型中，同一个单词可以根据周围的单词有不同的嵌入。OpenAI的embeddings-ada-02模型<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>，基于像GPT这样的LLMs的原则，是一个复杂的动态嵌入模型，能够捕捉上下文理解。然而，它可能没有最新的全尺寸语言模型（如GPT-4）对上下文的敏感性。</p>
<h4 id="后检索过程post-retrieval-process">后检索过程(<strong>Post-Retrieval Process</strong>)</h4>
<p>在从数据库检索到有价值的上下文后，将其与查询合并为LLMs的输入，同时解决上下文窗口限制带来的挑战，这是至关重要的。简单地一次性向LLMs呈现所有相关文档可能会超过上下文窗口限制，引入噪声，并阻碍对关键信息的关注。对检索到的内容进行额外处理是必要的，以解决这些问题。</p>
<p><strong>重新排名</strong>(<em>Re-Ranking</em>)。将检索到的信息重新排名，将最相关的内容重新定位到提示的边缘，是一个关键策略。这个概念已经在LlamaIndex<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>、LangChain<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>和HayStack[Blagojevi, 2023]等框架中实施。例如，Diversity Ranker<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>根据文档多样性优先重新排序，而LostInTheMiddleRanker在上下文窗口的开头和结尾交替放置最佳文档。此外，像cohereAI rerank[Cohere, 2023]、bge-rerank<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>和LongLLMLingua[Jiang等人，2023a]这样的方法重新计算相关文本和查询之间的语义相似度，解决了解释基于向量的模拟搜索的语义相似度的挑战。</p>
<p><strong>提示压缩</strong>(<em>Prompt Compression</em>)。研究表明，检索文档中的噪声对RAG性能产生不利影响。在后处理中，重点是压缩不相关的上下文，突出关键段落，并减少整体上下文长度。像Selective Context和LLMLingua[Litman等人，2020, Anderson等人，2022]这样的方法利用小型语言模型计算提示互信息或困惑度，估计元素重要性。Recomp[Xu等人，2023a]通过在不同粒度上训练压缩器来解决这个问题，而Long Context[Xu等人，2023b]和“Walking in the Memory Maze”[Chen等人，2023a]设计了总结技术，以增强LLM的关键信息感知，特别是在处理广泛上下文时。</p>
<h3 id="33-模块化ragmodular-rag">3.3 模块化RAG(<strong>Modular RAG</strong>)</h3>
<p>模块化RAG结构与传统的朴素RAG框架不同，提供了更大的灵活性和灵活性。它整合了各种方法来增强功能模块，例如结合搜索模块进行相似性检索，并在检索器中应用微调方法[Lin等人，2023]。重组的RAG模块[Yu等人，2022]和迭代方法[Shao等人，2023]已经被开发出来以解决特定问题。模块化RAG范式在RAG领域越来越成为常态，允许在多个模块之间进行串行管道或端到端训练。三种RAG范式的比较如图3所示。然而，模块化RAG并非独立存在。高级RAG是模块化RAG的一种专门形式，而进一步地，朴素RAG本身是高级RAG的一个特殊情况。这三种范式之间的关系是继承和发展的关系。</p>
<p><img src="https://github.com/weedge/mypic/raw/master/rag/RAG_for_LLMs_A_Survey/3.png" alt=""></p>
<h4 id="新模块new-modules">新模块(<strong>New Modules</strong>)</h4>
<p><strong>搜索模块</strong>(<em>Search Module</em>)。与朴素/高级RAG中的相似性检索不同，搜索模块针对特定场景进行定制，并结合直接在额外的语料库上进行搜索。这种整合是通过LLM生成的代码、SQL或Cypher等查询语言以及其他自定义工具实现的。这些搜索的数据源可以包括搜索引擎、文本数据、表格数据和知识图谱[Wang等人，2023d]。</p>
<p><strong>记忆模块</strong>(<em>Memory Module</em>)。这个模块利用LLM的记忆能力来指导检索。该方法涉及识别与当前输入最相似的记忆。Selfmem[Cheng等人，2023b]利用检索增强的生成器迭代地创建一个无界记忆池，结合“原始问题”和“双重问题”。通过使用检索增强的生成模型，该模型使用自己的输出来改进自己，文本在推理过程中与数据分布更加一致。因此，模型的输出被用来代替训练数据[Wang等人，2022a]。</p>
<p><strong>融合</strong>(<em>Fusion</em>)。RAG-Fusion[Raudaschl，2023]通过多查询方法增强了传统搜索系统，该方法将用户查询扩展到多个不同的视角，使用LLM。这种方法不仅捕获用户寻求的显式信息，还揭示了更深层次的、变革性的知识。融合过程涉及原始查询和扩展查询的并行向量搜索，智能重新排名以优化结果，并将最佳结果与新查询配对。这种复杂方法确保搜索结果与用户的显式和隐式意图紧密对齐，从而发现更有洞察力和相关的信息。</p>
<p><strong>路由</strong>(<em>Routing</em>)。RAG系统的检索过程利用了不同领域、语言和格式的多样化来源，这些来源可以根据情况交替或合并[Li等人，2023b]。查询路由器决定用户查询的后续操作，选项包括摘要、搜索特定数据库或将不同路径合并为单个响应。查询路由器还选择适当的数据存储用于查询，可能包括各种来源，如向量存储、图数据库或关系数据库，或索引的层次结构——例如，用于多文档存储的摘要索引和文档块向量索引。查询路由器的决策是预先定义的，并通过LLMs调用执行，将查询定向到所选索引。</p>
<p><strong>预测</strong>(<em>Predict</em>)。它解决了检索内容中的冗余和噪声问题。与直接从数据源检索不同，这个模块使用LLM生成必要的上下文[Yu等人，2022]。LLM生成的内容更有可能包含相关信息，而不是通过直接检索获得的内容。</p>
<p><strong>任务适配器</strong>(<em>Task Adapter</em>)。这个模块专注于将RAG适应于各种下游任务。UPRISE自动化了从预构建的数据池中检索零次任务输入的提示，从而提高了跨任务和模型的通用性[Cheng等人，2023a]。同时，PROMPTAGATOR[Dai等人，2022]利用LLM作为少次查询生成器，并根据生成的数据创建特定任务的检索器。通过利用LLM的泛化能力，它使得开发具有最小示例的特定任务端到端检索器成为可能。</p>
<h4 id="新模式new-patterns">新模式(New Patterns)</h4>
<p>模块化RAG的组织结构高度适应性，允许在RAG过程中替换或重新排列模块以适应特定的问题背景。</p>
<p>朴素RAG和高级RAG都可以被视为由一些固定模块组成。如图3所示，朴素RAG主要由“检索”和“阅读”模块组成。高级RAG的典型模式在朴素RAG的基础上增加了“重写”和“重新排名”模块。然而，总的来说，模块化RAG享有更大的多样性和灵活性。</p>
<p>当前的研究主要探索两种组织范式。第一种涉及添加或替换模块，而第二种专注于调整模块之间的组织流程。这种灵活性使得RAG过程能够有效地解决广泛的任务。</p>
<p><strong>添加或替换模块</strong>(<em>Adding or Replacing Modules</em>)。引入或替换模块的策略涉及在保持检索-阅读过程的核心结构的同时，整合额外的模块以增强特定功能。RRR模型[Ma等人，2023a]引入了重写-检索-阅读过程，利用LLM性能作为重写模块的强化学习激励。这使得重写器能够微调检索查询，从而提高下游任务的读者性能。</p>
<p>类似地，模块可以在像Generate-Read[Yu等人，2022]这样的方法中选择性地交换，其中LLM的生成模块取代了检索模块。Recite-Read方法[Sun等人，2022]将外部检索转化为模型权重的检索，要求LLM首先记住特定任务的信息，然后产生能够处理知识密集型自然语言处理任务的输出。</p>
<p><strong>调整模块之间的流程</strong>(<em>Adjusting the Flow between Modules</em>)。在模块流程调整领域，重点是增强语言模型和检索模型之间的互动。DSP[Khattab等人，2022]引入了DemonstrateSearch-Predict框架，将上下文学习系统视为一个明确的程序，而不是最终任务提示，从而更有效地处理知识密集型任务。ITER-RETGEN[Shao等人，2023]方法利用生成的内容指导检索，在检索-阅读-检索-阅读流程中迭代实施“检索增强生成”和“生成增强检索”。这种方法展示了一种创新的使用一个模块的输出来改进另一个模块功能的方式。</p>
<h4 id="优化rag流程optimizing-the-rag-pipeline">优化RAG流程(<strong>Optimizing the RAG Pipeline</strong>)</h4>
<p>检索过程的优化旨在提高RAG系统中信息的效率和质量。当前的研究集中在整合多样化的搜索技术、完善检索步骤、纳入认知回溯、实施多功能查询策略和利用嵌入相似度。这些努力共同致力于在RAG系统中实现检索效率和上下文信息深度之间的平衡。</p>
<p><strong>混合搜索探索</strong>(<em>Hybrid Search Exploration</em>)。RAG系统通过智能整合各种技术，包括基于关键词的搜索、语义搜索和向量搜索，来优化其性能。这种方法利用每种方法的独特优势，以适应多样化的查询类型和信息需求，确保检索到高度相关和上下文丰富的信息。混合搜索的使用作为检索策略的有力补充，从而提高了RAG流程的整体效率。</p>
<p><strong>递归检索和查询引擎</strong>(<em>Recursive Retrieval and Query Engine</em>)。递归检索涉及在初始检索阶段获取较小的块以捕获关键语义含义。随后，在过程的后期阶段向LLM提供包含更多上下文信息的较大块。这种两步检索方法有助于在效率和提供上下文丰富响应之间取得平衡。</p>
<p><strong>StepBack-prompt</strong>。该方法鼓励LLM远离具体实例，围绕更广泛的概念和原则进行推理[Zheng等人，2023]。实验结果表明，当使用向后提示时，各种具有挑战性的、基于推理的任务的性能显著提高，突显了它们对RAG流程的自然适应性。这些检索增强步骤既可以应用于生成对向后提示的响应，也可以应用于最终的问题回答过程。</p>
<p><strong>子查询</strong>(<em>Sub-Queries</em>)。根据场景，可以采用各种查询策略，例如使用LlamaIndex等框架提供的查询引擎，利用树查询，使用向量查询，或执行块的简单顺序查询。</p>
<p><strong>假设文档嵌入</strong>(<em>Hypothetical Document Embeddings</em>)。HyDE基于这样一个信念：生成的答案可能在嵌入空间中比直接查询更接近。使用LLM，HyDE创建一个假设的文档（答案）来响应查询，嵌入这个文档，并使用结果嵌入来检索与假设文档相似的真实文档。这种方法不是基于查询的嵌入相似度，而是专注于从一个答案到另一个答案的嵌入相似度[Gao等人，2022]。然而，它可能不会始终产生理想的结果，特别是当语言模型对主题不熟悉时，可能导致更多错误实例。</p>
<h2 id="第4章检索">第4章：检索</h2>
<p>在RAG的背景下，从数据源高效地检索相关文档至关重要。然而，创建一个熟练的检索器面临着重大挑战。本节将探讨三个基本问题：1）我们如何实现准确的语义表示？2）哪些方法可以对齐查询和文档的语义空间？3）检索器的输出如何与大型语言模型（LLM）的偏好对齐？</p>
<h3 id="41-增强语义表示enhancing-semantic-representations">4.1 增强语义表示(<strong>Enhancing Semantic Representations</strong>)</h3>
<p>在RAG中，语义空间至关重要，因为它涉及查询和文档的多维映射。在这个语义空间中的检索准确性显著影响RAG的结果。本节将介绍构建准确语义空间的两种方法。</p>
<h4 id="块优化chunk-optimization">块优化(<strong>Chunk optimization</strong>)</h4>
<p>在处理外部文档时，初始步骤涉及将它们分解成较小的块以提取细粒度特征，然后嵌入这些特征以表示它们的语义。然而，嵌入过大或过小的文本块可能会导致次优结果。因此，确定文档语料库中文档的最佳块大小对于确保检索结果的准确性和相关性至关重要。选择合适的分块策略需要仔细考虑几个关键因素，例如索引内容的性质、嵌入模型及其最佳块大小、用户查询的预期长度和复杂性，以及应用程序对检索结果的具体使用。例如，选择分块模型应基于内容的长度——无论是更长还是更短。此外，不同的嵌入模型在不同的块大小下表现出不同的性能特征。例如，sentence-transformer在处理单个句子时表现更好，而text-embedding-ada-002在处理包含256或512个token的块时表现出色。</p>
<p>此外，用户输入问题的长度和复杂性以及应用程序（例如语义搜索或问答）对检索结果的具体需求也会影响块大小的选择。实际上，获得精确的查询结果涉及灵活应用不同的分块策略。没有一种“最佳”策略适用于所有情况，只有特定情境下最合适的策略。</p>
<p>当前RAG领域的研究探索了各种块优化技术，旨在提高检索效率和准确性。其中一种方法涉及使用滑动窗口技术，通过合并多个检索过程中的全局相关信息来实现分层检索。另一种策略，称为“small2big”方法，利用小文本块进行初始搜索阶段，然后在后续阶段向语言模型提供较大的相关文本块。</p>
<p>抽象嵌入技术优先考虑基于文档摘要（或摘要）的Top K检索，提供对整个文档上下文的全面理解。此外，元数据过滤技术利用文档元数据来增强过滤过程。一种创新的方法，图索引技术，将实体和关系转化为节点和连接，显著提高了相关性，特别是在多跳问题的背景下。</p>
<p>这些多样化方法的结合导致了显著的进步，从而提高了检索结果和RAG性能。</p>
<h4 id="微调嵌入模型fine-tuning-embedding-models">微调嵌入模型(<strong>Fine-tuning Embedding Models</strong>)</h4>
<p>一旦确定了适当的块大小，下一步关键是使用嵌入模型将这些块和查询嵌入到语义空间中。嵌入的有效性至关重要，因为它影响模型表示语料库的能力。最近的研究引入了著名的嵌入模型，如AngIE、Voyage、BGE等[Li和Li，2023，VoyageAI，2023，BAAI，2023]。这些模型已经在广泛的语料库上进行了预训练。然而，当应用于特定领域时，它们准确捕捉领域特定信息的能力可能受到限制。</p>
<p>此外，针对特定任务对嵌入模型进行微调对于确保模型理解用户查询的内容相关性至关重要。没有经过微调的模型可能无法充分满足特定任务的要求。因此，对于下游应用来说，微调嵌入模型变得至关重要。嵌入微调方法主要有两种范式。</p>
<p><strong>领域知识微调</strong>(<em>Domain Knowledge Fine-tuning</em>)。为了确保嵌入模型准确捕捉领域特定信息，利用领域特定数据集进行微调至关重要。这个过程与标准语言模型微调的主要区别在于涉及的数据集的性质。通常，嵌入模型微调的数据集包括三个主要元素：查询、语料库和相关文档。模型使用这些查询在语料库中识别相关文档。然后根据模型检索这些相关文档对查询的能力来评估其有效性。数据集构建、模型微调和评估阶段各自面临不同的挑战。LlamaIndex[Liu，2023]引入了一系列关键的类和函数，旨在简化嵌入模型微调工作流程，从而简化这些复杂的过程。通过策划一个充满领域知识的语料库并利用提供的方法，可以巧妙地微调嵌入模型，使其紧密符合目标领域的特定要求。</p>
<p><strong>针对下游任务的微调</strong>(<em>Fine-tuning for Downstream Tasks</em>)。针对下游任务微调嵌入模型是提高模型性能的关键步骤。在利用RAG处理这些任务的领域中，出现了创新的方法，利用LLMs的能力对嵌入模型进行微调。例如，PROMPTAGATOR[Dai等人，2022]利用LLM作为少次查询生成器来创建特定任务的检索器，解决了监督微调中的挑战，特别是在数据稀缺的领域。另一种方法，LLM-Embedder[Zhang等人，2023a]，利用LLMs为多个下游任务的数据生成奖励信号。检索器通过两种类型的监督信号进行微调：数据集的硬标签和LLMs的软奖励。这种双信号方法促进了更有效的微调过程，使嵌入模型适应于多样化的下游应用。</p>
<p>尽管这些方法通过结合领域知识和针对特定任务的微调提高了语义表示，但检索器可能并不总是与某些LLMs表现出最佳兼容性。为了解决这个问题，一些研究人员探索了使用LLMs的反馈直接监督微调过程。这种直接监督旨在使检索器更紧密地与LLM对齐，从而提高下游任务的性能。关于这个主题的更全面讨论将在第4.3节中介绍。</p>
<h3 id="42-对齐查询和文档aligning-queries-and-documents">4.2 对齐查询和文档(<strong>Aligning Queries and Documents</strong>)</h3>
<p>在RAG应用的背景下，检索器可能使用单一嵌入模型对编码查询和文档，或者为每个使用单独的模型。此外，用户的原始查询可能由于表述不精确和缺乏语义信息而受到影响。因此，将用户查询的语义空间与文档的语义空间对齐至关重要。本节介绍了两种旨在实现这种对齐的基本技术。</p>
<h4 id="查询重写query-rewriting">查询重写(<strong>Query Rewriting</strong>)</h4>
<p>查询重写是将查询和文档的语义对齐的基本方法。Query2Doc和ITER-RETGEN等方法利用LLMs通过结合原始查询和额外指导来创建伪文档[Wang等人，2023c，Shao等人，2023]。HyDE利用文本线索构建查询向量，以生成捕获基本模式的“假设”文档[Gao等人，2022]。RRR引入了一个框架，颠覆了传统的检索和阅读顺序，专注于查询重写[Ma等人，2023a]。STEP-BACKPROMPTING使LLMs能够执行基于高层次概念的抽象推理和检索[Zheng等人，2023]。此外，多查询检索方法利用LLMs同时生成和执行多个搜索查询，对于解决具有多个子问题的复杂问题特别有利。</p>
<h4 id="嵌入转换embedding-transformation">嵌入转换(<strong>Embedding Transformation</strong>)</h4>
<p>除了像查询重写这样的广泛策略外，还有一些更精细的技术专门设计用于嵌入转换。LlamaIndex[Liu，2023]通过引入一个适配器模块来实现这一点，该模块可以在查询编码器之后集成。这个适配器促进了微调，从而优化了查询嵌入的表示，将它们映射到与预期任务更紧密对齐的潜在空间。SANTA[Li等人，2023d]解决了与结构化外部文档对齐查询的问题，特别是处理结构化和非结构化数据之间的不一致性。它通过两种预训练策略增强了检索器对结构化信息的敏感性：首先，通过利用结构化和非结构化数据之间的固有对齐来指导对比学习的结构感知预训练方案；其次，通过实施Masked Entity Prediction。后者利用以实体为中心的掩蔽策略，鼓励语言模型预测并填写掩蔽的实体，从而促进对结构化数据的更深入理解。</p>
<h3 id="43-对齐检索器和llmaligning-retriever-and-llm">4.3 对齐检索器和LLM(<strong>Aligning Retriever and LLM</strong>)</h3>
<p>在RAG流程中，通过各种技术提高检索命中率可能并不一定改善最终结果，因为检索到的文档可能与LLMs的特定要求不一致。因此，本节介绍了两种旨在将检索器输出与LLMs偏好对齐的方法。</p>
<h4 id="微调检索器fine-tuning-retrievers">微调检索器(<strong>Fine-tuning Retrievers</strong>)</h4>
<p>一些研究利用LLMs的反馈信号来完善检索模型。例如，AAR[Yu等人，2023b]通过使用编码器-解码器架构为预训练检索器引入监督信号。这是通过识别LM的首选文档通过FiD交叉注意力分数实现的。随后，检索器通过硬负采样和标准交叉熵损失进行微调。最终，经过改进的检索器可以直接应用于增强目标LLMs，在目标任务中实现更好的性能。此外，有人建议LLMs可能更喜欢关注可读性而不是信息丰富的文档。REPLUG[Shi等人，2023]利用检索器和LLM来计算检索文档的概率分布，然后通过计算KL散度进行监督训练。这种简单有效的训练方法通过使用LM作为监督信号来提高检索模型的性能，消除了对特定交叉注意力机制的需求。UPRISE[Cheng等人，2023a]也利用冻结的LLMs来微调提示检索器。LLM和检索器都以提示输入对作为输入，并利用LLM提供的分数来监督检索器的训练，有效地将LLM视为数据集标签器。此外，Atlas[Izacard等人，2022]提出了四种监督微调嵌入模型的方法：</p>
<ul>
<li>注意力蒸馏(<em>Attention Distillation</em>)。这种方法利用LLM在输出期间生成的交叉注意力分数来提炼模型的知识。</li>
<li>EMDR2。通过使用期望最大化算法，这种方法使用检索文档作为潜在变量来训练模型。</li>
<li>困惑度蒸馏(<em>Perplexity Distillation</em>)直接使用生成标记的困惑度作为指标来训练模型。</li>
<li>LOOP。这种方法提出了一种基于文档删除对LLM预测影响的新损失函数，提供了一种有效的训练策略，以更好地适应特定任务。</li>
</ul>
<p>这些方法旨在改善检索器和LLM之间的协同作用，从而提高检索性能并更准确地响应用户查询。</p>
<h4 id="适配器adapters">适配器(<strong>Adapters</strong>)</h4>
<p>微调模型可能会带来挑战，例如通过API整合功能或解决由于本地计算资源有限而产生的约束。因此，一些方法选择加入外部适配器来帮助对齐。</p>
<p>PRCA通过上下文提取阶段和奖励驱动阶段训练适配器。然后使用基于标记的自回归策略优化检索器的输出[Yang等人，2023b]。标记过滤方法利用交叉注意力分数有效地过滤标记，仅选择得分最高的输入标记[Berchansky等人，2023]。</p>
<p>RECOMP引入了提取性和生成性压缩器用于摘要生成。这些压缩器要么选择相关句子，要么合成文档信息，创建针对多文档查询量身定制的摘要[Xu等人，2023a]。此外，PKG通过指令微调引入了一种创新的方法，将知识集成到白盒模型中[Luo等人，2023]。在这种方法中，检索器模块被直接替换为根据查询生成相关文档。这种方法有助于解决微调过程中遇到的困难，并提高模型性能。</p>
<h2 id="第5章生成">第5章：生成</h2>
<p>RAG的一个关键组成部分是其生成器，它负责将检索到的信息转换为连贯流畅的文本。与传统的语言模型不同，RAG的生成器通过整合检索到的数据来提高准确性和相关性。在RAG中，生成器的输入不仅包括典型的上下文信息，还包括通过检索器获取的相关文本片段。这种全面的输入使生成器能够深入了解问题的上下文，从而产生更具信息性和上下文相关性的响应。此外，生成器受到检索到的文本的指导，以确保生成的内容与获取的信息保持一致。多样化的输入数据导致了生成阶段的针对性努力，旨在完善大型模型对来自查询和文档的输入数据的适应。在以下小节中，我们将通过深入探讨后检索处理和微调方面，介绍生成器的引入。</p>
<h3 id="51-后检索处理与冻结的llmpost-retrieval-with-frozen-llm">5.1 后检索处理与冻结的LLM(<strong>Post-retrieval with Frozen LLM</strong>)</h3>
<p>在不可调的LLM领域，许多研究依赖于像GPT-4这样的成熟模型，利用它们全面的内部知识来系统地综合来自各种文档的检索信息。</p>
<p>然而，这些大型模型仍然存在挑战，包括上下文长度的限制和对冗余信息的敏感性。为了解决这些问题，某些研究工作转向了后检索处理。后检索处理涉及处理、过滤或优化检索器从大型文档数据库检索到的相关信息。其主要目标是提高检索结果的质量，使其更贴近用户需求或后续任务。它可以被视为对检索阶段获得的文档进行的再处理。常见的后检索处理操作通常包括信息压缩和结果重新排名。</p>
<h4 id="信息压缩information-compression">信息压缩(<strong>Information Compression</strong>)</h4>
<p>检索器擅长从庞大的知识库中检索相关信息，但管理检索文档中的大量信息是一个挑战。正在进行的研究旨在扩展大型语言模型的上下文长度以解决这个问题。然而，当前的大型模型仍然难以应对上下文限制。因此，在某些情况下，压缩信息变得必要。信息压缩对于减少噪声、解决上下文长度限制和增强生成效果至关重要。</p>
<p>PRCA通过训练一个信息提取器来解决这个问题[Yang等人，2023b]。在上下文提取阶段，当提供输入文本$S_{input}$时，它能够产生一个输出序列$C_{extracted}$，代表输入文档的压缩上下文。训练过程旨在最小化$C_{extracted}$和实际上下文$C_{truth}$之间的差异。</p>
<p>同样，RECOMP采用类似的方法，通过对比学习训练一个信息压缩器[Xu等人，2023a]。每个训练数据点包括一个正样本和五个负样本，编码器在整个过程中使用对比损失进行训练[Karpukhin等人，2020]。</p>
<p>另一项研究采取了不同的方法，旨在减少文档数量以提高模型答案的准确性。在[Ma等人，2023b]的研究中，他们提出了“Filter-Reranker”范式，结合了LLMs和小型语言模型（SLMs）的优势。在这个范式中，SLMs充当过滤器，而LLMs充当重新排序代理。研究表明，指导LLMs重新排列SLMs识别出的具有挑战性的样本，可以在各种信息提取（IE）任务中取得显著改进。</p>
<h4 id="重新排名reranking">重新排名(<strong>Reranking</strong>)</h4>
<p>重新排名模型在优化检索器检索到的文档集中起着关键作用。语言模型在引入额外上下文时经常面临性能下降，重新排名有效地解决了这个问题。核心概念涉及重新排列文档记录，以优先考虑最相关的项目，从而限制文档总数。这不仅解决了检索过程中上下文窗口扩展的挑战，还提高了检索效率和响应性。</p>
<p>重新排名模型在信息检索过程中扮演双重角色，既是优化器又是精炼器。它为后续的语言模型处理提供了更有效和准确的输入[Zhuang等人，2023]。</p>
<p>上下文压缩被纳入重新排序过程中，以提供更精确的检索信息。这种方法涉及减少单个文档的内容和过滤整个文档，最终目标是在搜索结果中呈现最相关的信息，以便更专注和准确地显示相关内容。</p>
<h3 id="52-为rag微调llm-fine-tuning-llm-for-rag">5.2 为RAG微调LLM( <strong>Fine-tuning LLM for RAG</strong>)</h3>
<p>优化RAG模型中的生成器是其架构的关键方面。生成器的角色是接收检索到的信息并生成相关文本，形成模型的最终输出。生成器的优化旨在确保生成的文本既自然又有效地利用检索到的文档，以更好地满足用户的查询需求。</p>
<p>在标准的LLM生成任务中，输入通常包括一个查询。RAG通过将查询和检索器检索到的各种文档（结构化/非结构化）整合到输入中而脱颖而出。这种额外的信息可以显著影响模型的理解，尤其是对于较小的模型。在这种情况下，微调模型以适应查询和检索到的文档的输入变得至关重要。在将输入呈现给微调模型之前，通常会对检索器检索到的文档进行后检索处理。值得注意的是，RAG中生成器的微调方法与LLMs的一般微调方法一致。以下，我们将简要描述一些涉及数据（格式化/非格式化）和优化功能的代表性工作。</p>
<h4 id="一般优化过程general-optimization-process">一般优化过程(<strong>General Optimization Process</strong>)</h4>
<p>作为一般优化过程的一部分，训练数据通常包括输入-输出对，旨在训练模型在给定输入x的情况下产生输出y。在Self-Mem[Cheng等人，2023b]的工作中，采用了传统的训练过程，其中给定输入x，检索相关文档z（在论文中选择Top-1），并在整合（x，z）后，模型生成输出y。该论文利用了两种常见的微调范式，即Joint-Encoder和Dual-Encoder[Arora等人，2023，Wang等人，2022b，Lewis等人，2020，Xia等人，2019，Cai等人，2021，Cheng等人，2022]。</p>
<p>在Joint-Encoder范式中，使用基于编码器-解码器的标准模型。这里，编码器首先编码输入，然后解码器通过注意力机制结合编码结果以自回归方式生成标记。另一方面，在Dual-Encoder范式中，系统设置两个独立的编码器，每个编码器分别编码输入（查询，上下文）和文档。然后，解码器按顺序对这两个输出进行双向交叉注意力处理。这两种架构都使用Transformer[Vaswani等人，2017]作为基础块，并使用<strong>负对数似然损失</strong>(Negative Log-Likelihood loss)进行优化。</p>
<h4 id="利用对比学习utilizing-contrastive-learning">利用对比学习(<strong>Utilizing Contrastive Learning</strong>)</h4>
<p>在为语言模型准备训练数据的过程中，通常会创建输入和输出的交互对。这种传统方法可能导致“曝光偏差(exposure bias)”，即模型仅在训练数据中遇到的个别正确输出示例上进行训练，从而限制了模型在各种可能输出中的泛化能力。这种限制可能会通过使模型过度拟合训练集中的特定示例，从而降低模型在现实世界性能中的泛化能力。</p>
<p>为了缓解曝光偏差，SURGE[Kang等人，2023]提出了使用图-文本对比学习。这种方法包括一个对比学习目标，促使模型产生一系列合理且连贯的响应，超越了训练数据中遇到的实例。这种方法对于减少过拟合并加强模型的泛化能力至关重要。</p>
<p>对于涉及结构化数据的检索任务，SANTA框架[Li等人，2023d]实施了三部分训练方案，有效地封装了结构和语义细节。初始阶段专注于检索器，其中对比学习被用来完善查询和文档嵌入。</p>
<p>随后，生成器的初步训练阶段使用对比学习将结构化数据与其非结构化文档描述对齐。在生成器训练的进一步阶段，模型认识到实体语义在文本数据表示学习中的关键作用，如[Sciavolino等人，2021，Zhang等人，2019]所强调。这个过程从识别结构化数据中的实体开始，然后在生成器的输入数据中对这些实体应用掩码，为模型预测和预测这些掩蔽元素做好准备。</p>
<p>训练方案随着模型学习利用上下文信息重建掩蔽实体而进展。这种练习培养了模型对文本数据结构语义的理解，并促进了结构化数据中相关实体的对齐。总体优化目标是训练语言模型准确地恢复被掩盖的跨度，从而丰富其对实体语义的理解[Ye等人，2020]。</p>
<h2 id="第6章rag中的增强">第6章：RAG中的增强</h2>
<p>本节围绕三个关键方面展开：增强阶段、增强数据的来源和增强过程。这些方面阐明了对RAG发展至关重要的关键技术。RAG核心组件的分类如图4所示。</p>
<p><img src="https://github.com/weedge/mypic/raw/master/rag/RAG_for_LLMs_A_Survey/4.png" alt=""></p>
<h3 id="61-rag-增强阶段-rag-in-augmentation-stages">6.1 RAG 增强阶段( <strong>RAG in Augmentation Stages</strong>)</h3>
<p>RAG是一个知识密集型的努力(knowledge-intensive endeavor)，它在语言模型训练的预训练、微调和推理阶段采用了各种技术方法。</p>
<h4 id="预训练阶段pre-training-stage">预训练阶段(<strong>Pre-training Stage</strong>)</h4>
<p>在预训练阶段，研究人员已经研究了通过基于检索的策略来增强开放领域问答（QA）的预训练模型（PTMs）的方法。REALM模型采用了一种结构化、可解释的知识嵌入方法，将预训练和微调框定为在掩码语言模型（MLM）框架内的检索然后预测的工作流程[Arora等人，2023]。</p>
<p>RETRO[Borgeaud等人，2022]利用检索增强进行大规模预训练，实现了模型参数的减少，同时在困惑度方面超越了标准GPT模型。RETRO的独特之处在于它设计了一个额外的编码器来处理从外部知识库检索到的实体的特征，这是在GPT模型的基础上构建的。</p>
<p>Atlas[Izacard等人，2022]也在T5架构的预训练和微调阶段引入了检索机制。它使用预训练的T5来初始化编码器-解码器语言模型，并使用预训练的Contriever作为密集检索器，提高了其在复杂语言建模任务中的效率。</p>
<p>此外，COG[Lan等人，2022]引入了一种新颖的文本生成方法，模仿从预先存在的集合中复制文本片段。利用高效的向量搜索工具，COG计算并索引文本片段的上下文意义表示，在问答和领域适应等域中表现出优于RETRO的性能。</p>
<p>随着规模法则的出现，模型参数的增长推动了自回归模型成为主流。研究人员正在将RAG方法扩展到更大的预训练模型，RETRO++就是这一趋势的典范，它扩大了模型参数，同时保持或提高了性能[Wang等人，2023b]。</p>
<p>实证证据强调了在文本生成质量、事实准确性、减少毒性() reduced toxicity和下游任务熟练度方面的显著改进，特别是在开放领域问答等知识密集型应用中。这些结果表明，将检索机制整合到自回归语言模型的预训练中是一个有前景的途径，将复杂的检索技术与庞大的语言模型相结合，以产生更精确、更高效的语言生成。</p>
<p>增强预训练的好处包括一个强大的基础模型，其在困惑度、文本生成质量和特定任务性能方面都优于标准GPT模型，同时使用的参数更少。这种方法特别擅长处理知识密集型任务，并有助于通过在专业语料库上进行训练来开发领域特定模型。</p>
<p>尽管这种方法面临诸如需要大量预训练数据集和资源以及随着模型大小增加更新频率降低等挑战，但这种方法在模型韧性方面提供了显著的优势。一旦训练完成，增强检索模型可以独立于外部库运行，提高生成速度和操作效率。这些潜在的收益使得这种方法成为人工智能和机器学习中持续研究和创新的一个引人入胜的主题。</p>
<h4 id="微调阶段fine-tuning-stage">微调阶段(<strong>Fine-tuning Stage</strong>)</h4>
<p>RAG和微调是增强LLMs的强大工具，将两者结合起来可以满足更特定场景的需求。一方面，微调允许检索具有独特风格的文档，实现更好的语义表达，并调整查询和文档之间的差异。这确保了检索器的输出更适合当前场景。另一方面，微调可以满足生成需求，进行风格化和针对性的调整。此外，微调还可以用于调整检索器和生成器，以提高模型的协同作用。</p>
<p>微调检索器的主要目标是提高语义表示的质量，通过直接微调嵌入模型来实现，使用语料库[Liu，2023]。通过反馈信号将检索器的能力与LLMs的偏好对齐，两者可以更好地协调[Yu等人，2023b，Izacard等人，2022，Yang等人，2023b，Shi等人，2023]。针对特定下游任务微调检索器可以提高适应性。引入任务无关的微调旨在提高检索器在多任务场景中的多功能性[Cheng等人，2023a]。</p>
<p>微调生成器可以产生更风格化和定制化的输出。一方面，它允许针对不同的输入数据格式进行专门的适应。例如，微调LLMs以适应知识图的结构[Kang等人，2023]，文本对的结构[Kang等人，2023，Cheng等人，2023b]，以及其他特定结构[Li等人，2023d]。另一方面，通过构建指令数据集，可以要求LLMs生成特定格式的内容。例如，在适应性或迭代检索场景中，LLMs被微调以生成有助于确定下一步行动时机的内容[Jiang等人，2023b，Asai等人，2023]。</p>
<p>通过协同微调检索器和生成器，我们可以提高模型的泛化能力，避免因单独训练它们而可能出现的过拟合。然而，联合微调也导致了资源消耗的增加。RA-DIT[Lin等人，2023]提出了一个轻量级的双指令微调框架，可以有效地为任何LLMs添加检索能力。检索增强的指令微调更新LLM，引导它更有效地利用检索到的信息，并忽略分散注意力的内容。</p>
<p>尽管它具有优势，但微调也有局限性，包括需要专门的RAG微调数据集和大量的计算资源。然而，这个阶段允许根据特定需求和数据格式定制模型，可能与预训练阶段相比减少资源使用，同时仍能够微调模型的输出风格。</p>
<p>总之，微调阶段对于RAG模型适应特定任务至关重要，它增强了检索器和生成器的多功能性和适应性，尽管资源和数据集要求带来了挑战。RAG模型的战略微调因此是开发高效、有效的检索增强系统的关键组成部分。</p>
<h4 id="推理阶段inference-stage">推理阶段(<strong>Inference Stage</strong>)</h4>
<p>RAG模型的推理阶段至关重要，因为它涉及与LLMs的广泛集成。传统的RAG方法，也称为朴素RAG，涉及在这个阶段将检索内容整合到生成过程中。</p>
<p>为了克服朴素RAG的局限性，先进技术在推理过程中引入了更丰富的上下文信息。DSP框架[Khattab等人，2022]利用冻结的LMs和检索模型（RMs）之间的复杂自然语言文本交换，丰富了上下文，从而提高了生成结果。PKG[Luo等人，2023]方法为LLMs配备了一个知识引导模块，允许检索相关信息，而无需修改LMs的参数，使更复杂的任务执行成为可能。CREAICL[Li等人，2023b]采用跨语言知识的同步检索来增强上下文，而RECITE[Sun等人，2022]通过直接从LLMs中采样段落来生成上下文。</p>
<p>在需要多步推理的任务中，迭代检索方法进一步提高了RAG过程的精细化。ITRG[Feng等人，2023]迭代检索信息以确定正确的推理路径，从而提高了任务适应性。ITERRETGEN[Shao等人，2023]遵循迭代策略，将检索和生成合并在一个循环过程中，交替进行“检索增强生成”和“生成增强检索”。对于非知识密集型（NKI）任务，PGRA[Guo等人，2023]提出了一个两阶段框架，首先是一个任务无关的检索器，然后是一个提示引导的重排器，用于选择和优先考虑证据。相比之下，IRCOT[Trivedi等人，2022]将RAG与思维链（CoT）方法相结合，交替进行CoT引导的检索和检索信息的CoT过程，显著提高了GPT-3在各种问答任务中的性能。</p>
<p>本质上，这些推理阶段的增强提供了轻量级、成本效益高的选择，利用预训练模型的能力，而无需进一步训练。主要优势是在保持静态LLM参数的同时提供与上下文相关的信息以满足特定任务需求。然而，这种方法并非没有局限性，因为它需要精心的数据处理和优化，并且受到基础模型内在能力的约束。为了有效地应对多样化的任务需求，这种方法通常与程序优化技术（如逐步推理、迭代检索和自适应检索策略）结合使用。</p>
<h3 id="62-增强来源augmentation-source">6.2 增强来源(<strong>Augmentation Source</strong>)</h3>
<p>RAG模型的有效性在很大程度上受到增强数据源选择的影响。不同层次的知识和维度需要不同的处理技术。它们被归类为非结构化数据、结构化数据和由LLMs生成的内容。图5展示了具有不同增强方面的代表性RAG研究的技术树。叶子以三种不同的颜色表示，代表使用各种类型数据的增强：非结构化数据、结构化数据和由LLMs生成的内容。图表清楚地表明，最初，增强主要是通过非结构化数据（如纯文本）实现的。这种方法后来扩展到包括使用结构化数据（例如知识图谱）以进一步改进。最近，研究中出现了一个趋势，即使用LLMs自身生成的内容进行检索和增强。</p>
<p><img src="https://github.com/weedge/mypic/raw/master/rag/RAG_for_LLMs_A_Survey/5.png" alt=""></p>
<h4 id="使用非结构化数据增强augmented-with-unstructured-data">使用非结构化数据增强(<strong>Augmented with Unstructured Data</strong>)</h4>
<p>非结构化文本从语料库中收集，例如用于微调大型模型的提示数据[Cheng等人，2023a]和跨语言数据[Li等人，2023b]。检索单元从标记（例如kNN-LM[Khandelwal等人，2019]）到短语（例如NPM，COG[Lee等人，2020，Lan等人，2022]）和文档段落，更细的粒度提供了精确度，但代价是检索复杂性的增加。</p>
<p>FLARE[Jiang等人，2023b]引入了一种主动检索方法，由LM生成低概率词触发。它创建一个临时句子用于文档检索，然后使用检索到的上下文重新生成句子以预测后续句子。RETRO使用前一个块来检索块级别的最近邻，结合前一个块的上下文，指导下一个块的生成。为了保持因果关系，下一个块Ci的生成仅使用前一个块N(Ci−1)的最近邻，而不是N(Ci)。</p>
<h4 id="使用结构化数据增强augmented-with-structured-data">使用结构化数据增强(<strong>Augmented with Structured Data</strong>)</h4>
<p>结构化数据，如知识图谱（KGs），提供高质量的上下文并减轻模型幻觉。RET-LLMs[Modarressi等人，2023]为未来参考构建了一个由过去对话组成的知识图谱记忆。SUGRE[Kang等人，2023]利用图神经网络（GNNs）编码相关的KG子图，通过多模态对比学习确保检索到的事实与生成文本的一致性。KnowledgeGPT[Wang等人，2023d]生成KB搜索查询并将知识存储在个性化数据库中，增强了RAG模型的知识丰富性和上下文性。</p>
<h4 id="llms生成的内容在rag中llms-generated-content-in-rag">LLMs生成的内容在RAG中(<strong>LLMs-Generated Content in RAG</strong>)</h4>
<p>为了解决RAG中外部辅助信息的局限性，一些研究专注于利用LLMs的内部知识。SKR[Wang等人，2023e]将问题分类为已知或未知，并有选择地应用检索增强。GenRead[Yu等人，2022]用LLM生成器替换检索器，发现由于与因果语言建模的预训练目标更好地对齐，LLM生成的上下文通常包含更准确的答案。Selfmem[Cheng等人，2023b]使用检索增强的生成器迭代创建一个无界记忆池，使用记忆选择器选择作为原始问题的双重问题的输出，从而自我增强生成模型。</p>
<p>这些方法强调了在RAG中创新数据源利用的广度，努力提高模型性能和任务效果。</p>
<h3 id="63-增强过程augmentation-process">6.3 增强过程(<strong>Augmentation Process</strong>)</h3>
<p>在RAG领域，标准做法通常涉及单一检索步骤，然后进行生成，这可能导致效率低下。一个值得注意的问题，称为“中间丢失”现象，出现在单一检索产生冗余内容时，可能会稀释或与关键信息相矛盾，从而降低生成质量[Liu等人，2023a]。此外，这种单一检索通常不足以应对需要多步推理的复杂问题，因为它提供的信息范围有限[Yoran等人，2023]。如图5所示，为了规避这些挑战，当代研究提出了改进检索过程的方法：迭代检索、递归检索和自适应检索。迭代检索允许模型基于初始查询和迄今为止生成的文本重复收集文档，为LLMs提供更全面的知识库[Borgeaud等人，2022，Arora等人，2023]。</p>
<p>这种方法已被证明通过提供额外的上下文参考，增强了后续答案生成的鲁棒性。然而，它可能受到语义不连续性和无关信息积累的影响，因为它通常依赖于一个序列的n个标记来划分生成文本和检索文档之间的边界。为了解决特定的数据场景，使用了递归检索和多跳检索技术。递归检索涉及使用结构化索引以分层方式处理和检索数据，这可能包括在执行基于此摘要的检索之前对文档或长篇PDF的部分进行总结。随后，在文档内进行的二次检索细化了搜索，体现了过程的递归性质。相比之下，多跳检索旨在深入挖掘图结构化数据源，提取相互关联的信息[Li等人，2023c]。此外，一些方法整合了检索和生成的步骤。ITER-RETGEN[Shao等人，2023]采用了一种协同方法，利用“检索增强生成”和“生成增强检索”来处理需要复制特定信息的任务。该模型利用所需的内容作为检索相关知识的上下文基础，这反过来又促进了后续迭代中改进响应的生成。</p>
<h4 id="迭代检索iterative-retrieval">迭代检索(<strong>Iterative Retrieval</strong>)</h4>
<p>迭代检索是RAG模型中的一个过程，其中文档会基于初始查询和迄今为止生成的文本重复收集，为LLMs提供更全面的知识库[Borgeaud等人，2022，Arora等人，2023]。这种方法已被证明可以通过提供额外的上下文参考，增强后续答案生成的鲁棒性。然而，它可能受到语义不连续性和无关信息积累的影响，因为它通常依赖于一个序列的n个标记来划分生成文本和检索文档之间的边界。</p>
<p>为了解决特定的数据场景，使用了递归检索和多跳检索技术。递归检索涉及使用结构化索引以分层方式处理和检索数据，这可能包括在执行基于此摘要的检索之前对文档或长篇PDF的部分进行总结。随后，在文档内进行的二次检索细化了搜索，体现了过程的递归性质。相比之下，多跳检索旨在深入挖掘图结构化数据源，提取相互关联的信息[Li等人，2023c]。</p>
<p>此外，一些方法整合了检索和生成的步骤。ITER-RETGEN[Shao等人，2023]采用了一种协同方法，利用“检索增强生成”和“生成增强检索”来处理需要复制特定信息的任务。该模型利用所需的内容作为检索相关知识的上下文基础，这反过来又促进了后续迭代中改进响应的生成。</p>
<h4 id="递归检索recursive-retrieval">递归检索(<strong>Recursive Retrieval</strong>)</h4>
<p>递归检索通常用于信息检索和自然语言处理（NLP），以提高搜索结果的深度和相关性。
该过程涉及根据前一次搜索的结果迭代地完善搜索查询。递归检索旨在通过反馈循环逐步收敛到最相关的信息，从而增强搜索体验。IRCoT[Trivedi等人，2022]使用思维链来指导检索过程，并使用检索结果完善CoT。ToC[Kim等人，2023]创建了一个澄清树，系统地优化查询中的模糊部分。它在用户需求一开始就不完全清楚或所寻求的信息非常专业或微妙的复杂搜索场景中特别有用。该过程的递归性质允许对用户需求进行持续学习和适应，通常会导致对搜索结果的满意度提高。</p>
<h4 id="自适应检索adaptive-retrieval">自适应检索(<strong>Adaptive Retrieval</strong>)</h4>
<p>自适应检索方法，如Flare和SelfRAG[Jiang等人，2023b，Asai等人，2023]，通过使LLMs能够主动确定检索的最佳时机和内容，从而提高RAG框架的效率和相关信息的来源。</p>
<p>这些方法是LLMs在其操作中采用主动判断的更广泛趋势的一部分，如AutoGPT、Toolformer和Graph-Toolformer等模型代理[Yang等人，2023c，Schick等人，2023]。例如，Graph-Toolformer将其检索过程分为不同的步骤，其中LLMs主动使用检索器，应用Self-Ask技术，并使用少次提示来启动搜索查询。这种主动立场使LLMs能够决定何时搜索所需的信息，类似于代理如何使用工具。</p>
<p>WebGPT[Nakano等人，2021]集成了一个强化学习框架，以在文本生成过程中训练GPT-3模型自主使用搜索引擎。它使用特殊标记来促进如搜索引擎查询、浏览结果和引用参考等操作，从而通过使用外部搜索引擎扩展GPT-3的能力。</p>
<p>Flare通过监控生成过程中生成术语的概率来自动化检索时机[Jiang等人，2023b]。当概率低于某个阈值时，会激活检索系统以收集相关信息，从而优化检索周期。</p>
<p>Self-RAG[Asai等人，2023]引入了“反思标记”，允许模型反思其输出。这些标记有两种类型：“检索”和“批评”。模型可以自主决定何时激活检索，或者预定义的阈值可能会触发该过程。在检索期间，生成器在多个段落中进行片段级别的束搜索，以得出最连贯的序列。批评分数用于更新细分分数，具有在推理期间调整这些权重的灵活性，以定制模型的行为。Self-RAG的设计消除了对额外分类器或依赖自然语言推理（NLI）模型的需求，从而简化了决定何时参与检索机制的决策过程，并提高了模型在生成准确响应方面的自主判断能力。</p>
<p>由于LLM的日益普及，LLM优化受到了显著关注。提示工程、微调（FT）和RAG等技术各有特点，如图6所示。虽然提示工程利用了模型的固有能力，但优化LLM通常需要应用RAG和FT方法。RAG和FT的选择应基于场景的具体要求和每种方法的固有属性。表1提供了RAG和FT的详细比较。</p>
<p><img src="https://github.com/weedge/mypic/raw/master/rag/RAG_for_LLMs_A_Survey/6.png" alt=""></p>
<h3 id="64-rag与微调rag-vs-fine-tuning">6.4 RAG与微调(<strong>RAG vs Fine-Tuning</strong>)</h3>
<p>RAG就像给模型一本教科书，用于特定查询的信息检索，非常适合特定查询。另一方面，FT就像学生随着时间的推移内化知识，更适合复制特定的结构、风格或格式。FT可以通过加强基础模型知识、调整输出和教授复杂指令来提高模型的性能和效率。然而，它在整合新知识或快速迭代新用例方面并不擅长。</p>
<p>RAG和FT这两种方法并不互相排斥，可以在不同层次上互补，增强模型的能力。在某些情况下，它们的结合使用可能会产生最佳性能。涉及RAG和FT的优化过程可能需要多次迭代才能达到满意的结果。</p>
<p><img src="https://github.com/weedge/mypic/raw/master/rag/RAG_for_LLMs_A_Survey/tab1.png" alt=""></p>
<h2 id="第7章rag评估">第7章：RAG评估</h2>
<p>RAG在自然语言处理（NLP）领域的快速发展和日益广泛的应用推动了LLMs社区对RAG模型评估的研究。评估的主要目标是理解和优化RAG模型在不同应用场景下的性能。</p>
<p>从历史上看，RAG模型的评估主要集中在其在特定下游任务中的执行上。这些评估采用适合手头任务的既定指标。例如，问答评估可能依赖于EM和F1分数[Wang等人，2023a, Shi等人，2023, Feng等人，2023, Ma等人，2023a]，而事实核查任务通常以准确性为主要指标[Lewis等人，2020, Izacard等人，2022, Shao等人，2023]。为RAG应用设计的自动评估工具，如RALLE，同样基于这些特定于任务的指标进行评估[Hoshi等人，2023]。尽管如此，专门评估RAG模型独特特性的研究仍然很少。</p>
<p>以下部分将从任务特定评估方法和指标的焦点转移到现有文献的综合上，这些文献基于它们的独特属性。这一探索涵盖了RAG评估的目标、评估这些模型的方面以及可用于此类评估的基准和工具。目标是提供一个全面的RAG模型评估概述，概述专门针对这些先进生成系统的独特方面的评估方法。</p>
<h3 id="71-评估目标evaluation-targets">7.1 评估目标(<strong>Evaluation Targets</strong>)</h3>
<p>RAG模型的评估主要围绕两个关键组件：检索和生成模块。这种划分确保了对检索器组件提供的上下文质量和生成器产生的内容质量进行全面评估。</p>
<h4 id="检索质量retrieval-quality">检索质量(<strong>Retrieval Quality</strong>)</h4>
<p>评估检索质量对于确定检索器组件提供的上下文的有效性至关重要。评估RAG检索模块的性能通常采用搜索引擎、推荐系统和信息检索系统中的标准指标。常用的指标包括命中率、MRR和NDCG[Liu，2023, Nguyen，2023]。</p>
<h4 id="生成质量generation-quality">生成质量(<strong>Generation Quality</strong>)</h4>
<p>生成质量的评估集中在生成器从检索到的上下文中合成连贯和相关答案的能力上。这种评估可以根据内容的目标分为两类：未标记和已标记内容。对于未标记内容，评估包括生成答案的真实性、相关性和无害性。相比之下，对于已标记内容，重点是评估模型产生的信息的准确性[Liu，2023]。此外，检索和生成质量评估也可以通过手动或自动评估方法进行[Liu，2023, Lan等人，2022, Leng等人，2023]。</p>
<h3 id="72-评估方面evaluation-aspects">7.2 评估方面(<strong>Evaluation Aspects</strong>)</h3>
<p>当代RAG模型的评估实践强调三个主要质量分数和四种基本能力，这些共同构成了对RAG模型两个主要目标：检索和生成的评估。</p>
<h4 id="质量分数quality-scores">质量分数(<strong>Quality Scores</strong>)</h4>
<p>质量分数包括上下文相关性、答案真实性和答案相关性。这些质量分数从不同角度评估RAG模型在信息检索和生成过程中的效率[Es等人，2023, Saad-Falcon等人，2023, Jarvis和Allard，2023]。</p>
<p><strong>上下文相关性</strong>(<em>Context Relevance</em>)评估检索上下文的精确性和特异性，确保相关性并最小化与无关内容相关的处理成本。</p>
<p><strong>答案真实性</strong>(<em>Answer Faithfulness</em>)确保生成的答案忠实于检索到的上下文，保持一致性并避免矛盾。</p>
<p><strong>答案相关性</strong>(<em>Answer Relevance</em>)要求生成的答案直接相关于提出的问题，有效地解决核心查询。</p>
<h4 id="所需能力required-abilities">所需能力(<strong>Required Abilities</strong>)</h4>
<p>RAG评估还涉及四种能力，表明其适应性和效率：噪声鲁棒性、负面拒绝、信息整合和反事实鲁棒性[Chen等人，2023b, Liu等人，2023b]。这些能力对于模型在各种挑战和复杂场景下的性能至关重要，影响质量分数。</p>
<p><strong>噪声鲁棒性</strong>(<em>Noise Robustness</em>)评估模型处理与问题相关的但缺乏实质性信息的噪声文档的能力。</p>
<p><strong>负面拒绝</strong>(<em>Negative Rejection</em>)评估模型在检索到的文档不包含回答问题所需的知识时避免响应的辨别力。</p>
<p><strong>信息整合</strong>(<em>Information Integration</em>)评估模型从多个文档中综合信息以解决复杂问题的能力。</p>
<p><strong>反事实鲁棒性</strong>(<em>Counterfactual Robustness</em>)测试模型识别并忽略文档中已知不准确性的能力，即使在被告知潜在的错误信息时也是如此。</p>
<p>上下文相关性和噪声鲁棒性对于评估检索质量很重要，而答案真实性、答案相关性、负面拒绝、信息整合和反事实鲁棒性对于评估生成质量很重要。每个评估方面的具体指标总结在表2中。需要注意的是，这些指标来源于相关工作，尚未代表量化RAG评估方面的成熟或标准化方法。一些评估研究还开发了针对RAG模型细微差别的定制指标。</p>
<p><img src="https://github.com/weedge/mypic/raw/master/rag/RAG_for_LLMs_A_Survey/tab2.png" alt=""></p>
<h3 id="73-评估基准和工具evaluation-benchmarks-and-tools">7.3 评估基准和工具(<strong>Evaluation Benchmarks and Tools</strong>)</h3>
<p>本节描述了RAG模型的评估框架，包括基准测试和自动化评估工具。这些工具提供了量化指标，不仅衡量RAG模型的性能，还增强了对模型在各种评估方面能力的理解。著名的基准测试，如RGB和RECALL[Chen等人，2023b, Liu等人，2023b]，专注于评估RAG模型的基本能力。同时，最先进的自动化工具，如RAGAS[Es等人，2023]、ARES[Saad-Falcon等人，2023]和TruLens<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>，利用LLMs来评判质量分数。这些工具和基准共同构成了一个强大的框架，用于系统地评估RAG模型，如表3所总结。</p>
<p><img src="https://github.com/weedge/mypic/raw/master/rag/RAG_for_LLMs_A_Survey/tab3.png" alt=""></p>
<h2 id="第8章未来前景">第8章：未来前景</h2>
<p>本节探讨了RAG的三个未来前景：未来的挑战、模态扩展和RAG生态系统。</p>
<h3 id="81-rag的未来挑战future-challenges-of-rag">8.1 RAG的未来挑战(<strong>Future Challenges of RAG</strong>)</h3>
<p>尽管RAG技术取得了显著进展，但仍存在几个挑战需要深入研究：</p>
<ul>
<li><strong>上下文长度</strong>(<em>Context Length</em>)：RAG的有效性受到大型语言模型（LLMs）上下文窗口大小的限制。平衡窗口长度过短可能导致信息不足，过长可能导致信息稀释，这是至关重要的。随着努力扩大LLM上下文窗口到几乎无限大，适应这些变化对RAG提出了重要的研究问题[Xu等人，2023c，Packer等人，2023，Xiao等人，2023]。</li>
<li><strong>鲁棒性</strong>(<em>Robustness</em>)：检索过程中存在的噪声或矛盾信息可能对RAG的输出质量产生不利影响。提高RAG对这种对抗性或虚假输入的抵抗力正在获得研究动力，并已成为关键的性能指标[Yu等人，2023a，Glass等人，2021，Baek等人，2023]。</li>
<li><strong>混合方法</strong>(<em>Hybrid Approaches</em> RAG+FT)：将RAG与微调相结合正成为一种领先策略。确定RAG和微调的最佳整合方式，无论是顺序、交替还是通过端到端的联合训练，以及如何利用参数化和非参数化的优势，都是值得探索的领域[Lin等人，2023]。</li>
<li><strong>扩展LLM角色</strong>(<em>Expanding LLM Roles</em>)：除了生成最终答案外，LLMs在RAG框架中也被用于检索和评估。识别进一步解锁RAG系统中LLMs潜力的方法是一个不断增长的研究方向。</li>
<li><strong>规模法则</strong>(<em>Scaling Laws</em>)：虽然LLMs的规模法则已经确立[Kaplan等人，2020年]，但其对RAG的适用性仍不确定。初步研究已经开始解决这个问题[Wang等人，2023b]，但RAG模型的参数数量仍然落后于LLMs。较小模型在某些情况下可能优于较大模型的逆规模法则特别引人入胜<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>，值得进一步调查。</li>
<li><strong>生产就绪的RAG</strong>(<em>Production-Ready RAG</em>)：RAG的实用性和与工程需求的一致性促进了其采用。然而，提高检索效率、改善大型知识库中的文档召回率以及确保数据安全（例如防止LLMs无意中披露文档来源或元数据）等关键工程挑战仍有待解决[Alon等人，2022年]。</li>
</ul>
<h4 id="模态扩展的ragmodality-extension-of-rag">模态扩展的RAG(<strong>Modality Extension of RAG</strong>)</h4>
<p>RAG已经超越了其最初的基于文本的问答限制，拥抱了各种模态数据。这种扩展催生了创新的多模态模型，这些模型在各个领域整合了RAG概念：</p>
<ul>
<li><strong>图像</strong>(<em>Image</em>)。RA-CM3[Yasunaga等人，2022]是一种先进的多模态模型，能够检索和生成文本和图像。BLIP-2[Li等人，2023a]利用冻结的图像编码器与LLMs一起进行高效的视觉语言预训练，实现了零-shot图像到文本的转换。“在写之前进行可视化”方法[Zhu等人，2022]利用图像生成来引导LM的文本生成，在开放式文本生成任务中表现出潜力。</li>
<li><strong>音频和视频</strong>(<em>Audio and Video</em>)。GSS方法检索并拼接音频片段，将机器翻译数据转换为语音翻译数据[Zhao等人，2022]。UEOP通过引入外部的离线策略，用于语音转文本转换，实现了端到端自动语音识别的重大进步[Chan等人，2023]。此外，基于KNN的注意力融合利用音频嵌入和语义相关的文本嵌入来改进ASR，从而加速领域适应。Vid2Seq通过引入专门的时间标记，为语言模型增加了语义，以便在统一的输出序列中预测事件边界和文本描述[Yang等人，2023a]。</li>
<li><strong>代码</strong>(<em>Code</em>)。RBPS[Nashid等人，2023]在小规模学习任务中表现出色，通过编码和频率分析检索与开发人员目标一致的代码示例。这种方法在测试断言生成和程序修复等任务中已经证明了其有效性。对于结构化知识，CoK方法[Li等人，2023c]首先从知识图谱中提取与输入查询相关的事实，然后将这些事实作为提示集成到输入中，在知识图谱问答任务中提高了性能。</li>
</ul>
<h3 id="82-rag生态系统-ecosystem-of-rag">8.2 RAG生态系统( <strong>Ecosystem of RAG</strong>)</h3>
<h4 id="下游任务和评估downstream-tasks-and-evaluation">下游任务和评估(<strong>Downstream Tasks and Evaluation</strong>)</h4>
<p>RAG在丰富语言模型处理复杂查询和生成详细响应的能力方面显示出相当的前景。RAG不仅增强了响应的精确性和相关性，还增强了其多样性和深度。RAG在多个领域的可扩展性和多功能性需要进一步研究，特别是在医学、法律和教育等专业领域。在这些领域，RAG可能比传统的微调方法更能降低训练成本并提高性能。同时，完善RAG的评估框架对于最大化其在不同任务中的有效性和实用性至关重要。这需要开发能够衡量上下文相关性、内容创造性和非恶意性等方面细微差别的指标和评估工具。此外，提高RAG驱动模型的可解释性仍然是一个关键目标。这样做将允许用户理解模型生成响应背后的推理过程，从而促进对RAG应用的信任和透明度。</p>
<h4 id="技术栈technical-stack">技术栈(<strong>Technical Stack</strong>)</h4>
<p>RAG生态系统的发展受到其技术栈进展的极大影响。像LangChain和LLamaIndex这样的关键工具随着ChatGPT的出现迅速流行，提供了广泛的RAG相关API，并成为LLMs领域的重要工具。</p>
<p>新兴的技术栈，虽然功能不如LangChain和LLamaIndex丰富，但以其专业服务脱颖而出。例如，Flowise AI<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>优先考虑低代码方法，使用户能够通过用户友好的拖放界面部署AI应用，包括RAG。其他技术如HayStack、Meltano<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>和Cohere Coral<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>也因其在该领域的特殊贡献而受到关注。</p>
<p>除了AI专业提供商外，传统的软件和云服务提供商也在扩展其产品，包括以RAG为中心的服务。Weaviate的Verba<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>专为个人助理应用设计，而Amazon的Kendra提供智能企业搜索服务，允许用户使用内置连接器浏览各种内容库。在RAG技术景观的演变过程中，明显出现了不同的专业化方向，例如：1）定制化。根据特定要求定制RAG。2）简化。使RAG更易于使用，从而降低初始学习曲线。3）专业化。优化RAG以更有效地服务于生产环境。</p>
<p>RAG模型及其技术栈的相互增长显而易见；技术进步不断为现有基础设施设定新的标准。反过来，技术栈的增强推动了RAG能力的进化。RAG工具包正在汇聚成一个基础技术栈，为先进的企业应用奠定基础。然而，一个完全集成的、全面平台的概念仍然在地平线上，等待进一步的创新和发展。</p>
<h2 id="第9章结论">第9章：结论</h2>
<p>本文的总结，如图7所示，突出了RAG在通过整合来自语言模型的参数化知识与来自外部知识库的非参数化数据，显著提升了LLMs的能力。我们的综述展示了RAG技术的演变及其对知识密集型任务的影响。我们的分析描绘了RAG框架内的三个发展范式：朴素RAG、高级RAG和模块化RAG，每个都标志着对其前身的逐步增强。高级RAG范式通过引入复杂的架构元素，如查询重写、块重排和提示摘要，超越了朴素方法。这些创新导致了更细致和模块化的架构，提高了LLMs的性能和可解释性。RAG与其他AI方法（如微调和强化学习）的技术整合进一步扩展了其能力。在内容检索方面，一种混合方法正在兴起，它利用结构化和非结构化数据源，提供了更丰富的检索过程。RAG框架内的最新研究正在探索诸如从LLMs自我检索和信息检索的动态时机等新概念。</p>
<p><img src="https://github.com/weedge/mypic/raw/master/rag/RAG_for_LLMs_A_Survey/7.png" alt=""></p>
<p>尽管RAG技术取得了进步，但在提高其鲁棒性和管理扩展上下文的能力方面仍有许多研究机会。RAG的应用范围也在向多模态领域扩展，将其原理适应于解释和处理各种数据形式，如图像、视频和代码。这种扩展强调了RAG在AI部署中的重要实际意义，吸引了学术界和工业界的关注。RAG的生态系统正在增长，RAG中心的AI应用和支持工具的持续开发。然而，随着RAG应用领域的扩大，迫切需要完善评估方法以跟上其发展的步伐。确保性能评估保持准确和代表性对于捕捉RAG对AI研究和发展社区的贡献至关重要。</p>
<h2 id="references">References</h2>
<p>[Alon等人，2022] Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. Neurosymbolic language modeling with automaton-augmented retrieval. In International Conference on Machine Learning, pages 468–485. PMLR, 2022.</p>
<p>[Anderson等人，2022] Nathan Anderson, Caleb Wilson, and Stephen D. Richardson. Lingua: Addressing scenarios for live interpretation and automatic dubbing. In Janice Campbell, Stephen Larocca, Jay Marciano, Konstantin Savenkov, and Alex Yanishevsky, editors, Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track), pages 202–209, Orlando, USA, September 2022. Association for Machine Translation in the Americas.</p>
<p>[Arora等人，2023] Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gaurav Sinha, and Amit Sharma. Gar-meets-rag paradigm for zero-shot information retrieval. arXiv preprint arXiv:2310.20158, 2023.</p>
<p>[Asai等人，2023] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023.</p>
<p>[BAAI, 2023] BAAI. Flagembedding. <a href="https://github.com/FlagOpen/FlagEmbedding">https://github.com/FlagOpen/FlagEmbedding</a>, 2023.</p>
<p>[Baek等人，2023] Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C Park, and Sung Ju Hwang. Knowledge-augmented language model verification. arXiv preprint arXiv:2310.12836, 2023.</p>
<p>[Berchansky等人，2023] Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. Optimizing retrieval-augmented reader models via token elimination. arXiv preprint arXiv:2310.13682, 2023.</p>
<p>[Blagojevi, 2023] Vladimir Blagojevi. Enhancing rag pipelines in haystack: Introducing diversityranker and lostinthemiddleranker. <a href="https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5">https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5</a>, 2023.</p>
<p>[Borgeaud等人，2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206–2240. PMLR, 2022.</p>
<p>[Brown等人，2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.</p>
<p>[Cai等人，2021] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. arXiv preprint arXiv:2105.11269, 2021.</p>
<p>[Chan等人，2023] David M Chan, Shalini Ghosh, Ariya Rastrow, and Björn Hoffmeister. Using external off-policy speech-to-text mappings in contextual end-to-end automated speech recognition. arXiv preprint arXiv:2301.02736, 2023.</p>
<p>[Chen等人，2023a] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading. arXiv preprint arXiv:2310.05029, 2023.</p>
<p>[Chen等人，2023b] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. arXiv preprint arXiv:2309.01431, 2023.</p>
<p>[Cheng等人，2022] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation with contrastive translation memories. arXiv preprint arXiv:2212.03140, 2022.</p>
<p>[Cheng等人，2023a] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, and Qi Zhang. Uprise: Universal prompt retrieval for improving zero-shot evaluation. arXiv preprint arXiv:2303.08518, 2023.</p>
<p>[Cheng等人，2023b] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself up: Retrieval-augmented text generation with self memory. arXiv preprint arXiv:2305.02437, 2023.</p>
<p>[Cohere, 2023] Cohere. Say goodbye to irrelevant search results: Cohere rerank is here. <a href="https://txt.cohere.com/rerank/">https://txt.cohere.com/rerank/</a>, 2023.</p>
<p>[Dai等人，2022] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022.</p>
<p>[Es等人，2023] Shahul Es, Jithin James, Luis EspinosaAnke, and Steven Schockaert. Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023.</p>
<p>[Feng等人，2023] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. Retrieval-generation synergy augmented large language models. arXiv preprint arXiv:2310.05149, 2023.</p>
<p>[Gao等人，2022] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496, 2022.</p>
<p>[Glass等人，2021] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, and Alfio Gliozzo. Robust retrieval augmented generation for zero-shot slot filling. arXiv preprint arXiv:2108.13934, 2021.</p>
<p>[Google, 2023] Google. Gemini: A family of highly capable multimodal models. <a href="https://goo.gle/GeminiPaper">https://goo.gle/GeminiPaper</a>, 2023.</p>
<p>[Guo等人，2023] Zhicheng Guo, Sijie Cheng, Yile Wang, Peng Li, and Yang Liu. Prompt-guided retrieval augmentation for non-knowledge-intensive tasks. arXiv preprint arXiv:2305.17653, 2023.</p>
<p>[Hendrycks等人，2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>[Hoshi等人，2023] Yasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, and Jun Deguchi. Ralle: A framework for developing and evaluating retrieval-augmented large language models. arXiv preprint arXiv:2308.10633, 2023.</p>
<p>[Huang等人，2023] Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, and Bryan Catanzaro. Raven: In-context learning with retrieval augmented encoder-decoder language models. arXiv preprint arXiv:2308.07922, 2023.</p>
<p>[ILIN, 2023] IVAN ILIN. Advanced rag techniques: an illustrated overview. <a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6">https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6</a>, 2023.</p>
<p>[Izacard等人，2022] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022.</p>
<p>[Jarvis和Allard，2023] Colin Jarvis and John Allard. A survey of techniques for maximizing LLM performance. <a href="https://community.openai.com/t/openai-dev-day-2023-breakout-sessions/505213#a-survey-of-techniques-for-maximizing-llm-performance-2">https://community.openai.com/t/openai-dev-day-2023-breakout-sessions/505213#a-survey-of-techniques-for-maximizing-llm-performance-2</a>, 2023.</p>
<p>[Jiang等人，2023a] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023.</p>
<p>[Jiang等人，2023b] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023.</p>
<p>[Kandpal等人，2023] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pages 15696–15707. PMLR, 2023.</p>
<p>[Kang等人，2023] Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. Knowledge graph-augmented language models for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846, 2023.</p>
<p>[Kaplan等人，2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.</p>
<p>[Karpukhin等人，2020] Vladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.</p>
<p>[Khandelwal等人，2019] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.</p>
<p>[Khattab等人，2022] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022.</p>
<p>[Kim等人，2023] Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang. Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models. arXiv preprint arXiv:2310.14696, 2023.</p>
<p>[Lan等人，2022] Tian Lan, Deng Cai, Yan Wang, Heyan Huang, and Xian-Ling Mao. Copy is all you need. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>[Lee等人，2020] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of phrases at scale. arXiv preprint arXiv:2012.12624, 2020.</p>
<p>[Leng等人，2023] Quinn Leng, Kasey Uhlenhuth, and Alkis Polyzotis. Best practices for LLM evaluation of RAG applications. <a href="https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG">https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG</a>, 2023.</p>
<p>[Lewis等人，2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.</p>
<p>[Li and Li, 2023] Xianming Li and Jing Li. Angle-optimized text embeddings. arXiv preprint arXiv:2309.12871, 2023.</p>
<p>[Li等人，2023a] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.</p>
<p>[Li等人，2023b] Xiaoqian Li, Ercong Nie, and Sheng Liang. From classification to generation: Insights into crosslingual retrieval augmented ICL. arXiv preprint arXiv:2311.06595, 2023.</p>
<p>[Li等人，2023c] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq Joty, and Soujanya Poria. Chain of knowledge: A framework for grounding large language models with structured knowledge bases. arXiv preprint arXiv:2305.13269, 2023.</p>
<p>[Li等人，2023d] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu. Structure-aware language model pretraining improves dense retrieval on structured data. arXiv preprint arXiv:2305.19912, 2023.</p>
<p>[Liang等人，2023] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan Xu. Intergen: Diffusion-based multi-human motion generation under complex interactions. arXiv preprint arXiv:2304.05684, 2023.</p>
<p>[Lin等人，2023] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint arXiv:2310.01352, 2023.</p>
<p>[Litman等人，2020] Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman, Shai Mazor, and R Manmatha. Scatter: selective context attentional scene text recognizer. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11962–11972, 2020.</p>
<p>[Liu等人，2023a] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023.</p>
<p>[Liu等人，2023b] Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Recall: A benchmark for LLMs robustness against external counterfactual knowledge. arXiv preprint arXiv:2311.08147, 2023.</p>
<p>[Liu, 2023] Jerry Liu. Building production-ready RAG applications. <a href="https://www.ai.engineer/summit/schedule/building-production-ready-rag-applications">https://www.ai.engineer/summit/schedule/building-production-ready-rag-applications</a>, 2023.</p>
<p>[Luo等人，2023] Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Augmented large language models with parametric knowledge guiding. arXiv preprint arXiv:2305.04757, 2023.</p>
<p>[Ma等人，2023a] Xinbei Ma, Yeyun Gong, PengchengHe, Hai Zhao, and Nan Duan. Query rewriting for retrieval-augmented large language models. arXiv preprint arXiv:2305.14283, 2023.</p>
<p>[Ma等人，2023b] Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! ArXiv, abs/2303.08559, 2023.</p>
<p>[Modarressi等人，2023] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards a general read-write memory for large language models. arXiv preprint arXiv:2305.14322, 2023.</p>
<p>[Nakano等人，2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>[Nashid等人，2023] Noor Nashid, Mifta Sintaha, and Ali Mesbah. Retrieval-based prompt selection for code-related few-shot learning. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pages 2450– 2462, 2023.</p>
<p>[Nguyen, 2023] Isabelle Nguyen. Evaluating RAG part i: How to evaluate document retrieval. <a href="https://www.deepset.ai/">https://www.deepset.ai/</a> blog/rag-evaluation-retrieval, 2023.</p>
<p>[Nishikawa等人，2022] Sosuke Nishikawa, Ryokan Ri, Ikuya Yamada, Yoshimasa Tsuruoka, and Isao Echizen. EASE: Entity-aware contrastive learning of sentence embedding. arXiv preprint arXiv:2205.04260, 2022.</p>
<p>[OpenAI, 2023] OpenAI. GPT-4 technical report. https://cdn. openai.com/papers/gpt-4.pdf, 2023.</p>
<p>[Packer等人，2023] Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. Memgpt: Towards LLMs as operating systems. arXiv preprint arXiv:2310.08560, 2023.</p>
<p>[Raffel等人，2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485– 5551, 2020.</p>
<p>[Ram等人，2023] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083, 2023.</p>
<p>[Raudaschl, 2023] Adrian H. Raudaschl. Forget RAG, the future is RAG-Fusion. <a href="https://towardsdatascience.com/">https://towardsdatascience.com/</a> forget-RAG-the-future-is-RAG-Fusion-1147298d8ad1, 2023.</p>
<p>[Saad-Falcon等人，2023] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: An automated evaluation framework for retrieval-augmented generation systems. arXiv preprint arXiv:2311.09476, 2023.</p>
<p>[Schick等人，2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.</p>
<p>[Sciavolino等人，2021] Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. Simple entity-centric questions challenge dense retrievers. arXiv preprint arXiv:2109.08535, 2021.</p>
<p>[Shao等人，2023] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294, 2023.</p>
<p>[Shi等人，2023] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.</p>
<p>[Srivastava等人，2022] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>[Sun等人，2022] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. arXiv preprint arXiv:2210.01296, 2022.</p>
<p>[Touvron等人，2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>[Trivedi等人，2022] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509, 2022.</p>
<p>[Vaswani等人，2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>[VoyageAI, 2023] VoyageAI. Voyage’s embedding models. <a href="https://docs.voyageai.com/embeddings/">https://docs.voyageai.com/embeddings/</a>, 2023.</p>
<p>[Wang等人，2019] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.</p>
<p>[Wang等人，2022a] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. arXiv preprint arXiv:2203.08773, 2022.</p>
<p>[Wang等人，2022b] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3170– 3179, Dublin, Ireland, May 2022. Association for Computational Linguistics.</p>
<p>[Wang等人，2023a] Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan Catanzaro. Instructretro: Instruction tuning post retrieval-augmented pretraining. arX-print arXiv:2310.07713, 2023.</p>
<p>[Wang等人，2023b] Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models with retrieval? A comprehensive study. arXiv preprint arXiv:2304.06762, 2023.</p>
<p>[Wang等人，2023c] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language models. arXiv preprint arXiv:2303.07678, 2023.</p>
<p>[Wang等人，2023d] Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang. Knowledgpt: Enhancing large language models with retrieval and storage access on knowledge bases. arXiv preprint arXiv:2308.11761, 2023.</p>
<p>[Wang等人，2023e] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. Self-knowledge guided retrieval augmentation for large language models. arXiv preprint arXiv:2310.05002, 2023.</p>
<p>[Xia等人，2019] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. Graph based translation memory for neural machine translation. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7297–7304, 2019.</p>
<p>[Xiao等人，2023] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.</p>
<p>[Xu等人，2023a] Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented LMs with compression and selective augmentation. arXiv preprint arXiv:2310.04408, 2023.</p>
<p>[Xu等人，2023b] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025, 2023.</p>
<p>[Xu等人，2023c] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025, 2023.</p>
<p>[Yang等人，2023a] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10714–10726, 2023.</p>
<p>[Yang等人，2023b] Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. PRCA: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter. arXiv preprint arXiv:2310.18347, 2023.</p>
<p>[Yang等人，2023c] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023.</p>
<p>[Yasunaga等人，2022] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561, 2022.</p>
<p>[Ye等人，2020] Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun, and Zhiyuan Liu. Coreferential reasoning learning for language representation. arXiv preprint arXiv:2004.06870, 2020.</p>
<p>[Yoran等人，2023] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint arXiv:2310.01558, 2023.</p>
<p>[Yu等人，2022] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators. arXiv preprint arXiv:2209.10063, 2022.</p>
<p>[Yu等人，2023a] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chainof-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210, 2023.</p>
<p>[Yu等人，2023b] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. Augmentation-adapted retriever improves generalization of language models as generic plug-in. arXiv preprint arXiv:2305.17331, 2023.</p>
<p>[Zhang等人，2019] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE: Enhanced language representation with informative entities. arXiv preprint arXiv:1905.07129, 2019.</p>
<p>[Zhang等人，2023a] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to augment large language models. arXiv preprint arXiv:2310.07554, 2023.</p>
<p>[Zhang等人，2023b] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the AI ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.</p>
<p>[Zhang, 2023] Jiawei Zhang. Graph-toolformer: To empower LLMs with graph reasoning ability via prompt augmented by ChatGPT. arXiv preprint arXiv:2304.11116, 2023.</p>
<p>[Zhao等人，2022] Jinming Zhao, Gholamreza Haffar, and Ehsan Shareghi. Generating synthetic speech from spoken-vocab for speech translation. arXiv preprint arXiv:2210.08174, 2022.</p>
<p>[Zheng等人，2023] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny Zhou. Take a step back: Evoking reasoning via abstraction in large language models. arXiv preprint arXiv:2310.06117, 2023.</p>
<p>[Zhu等人，2022] Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, and William Yang Wang. Visualize before you write: Imagination-guided open-ended text generation. arXiv preprint arXiv:2210.03765, 2022.</p>
<p>[Zhuang等人，2023] Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon. Open-source large language models are strong zero-shot query likelihood models for document ranking. arXiv preprint arXiv:2310.13243, 2023.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://github.com/Tongji-KGLLM/RAG-Survey">https://github.com/Tongji-KGLLM/RAG-Survey</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://huggingface.co/BAAI/bge-large-en">https://huggingface.co/BAAI/bge-large-en</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://platform.openai.com/docs/guides/embeddings">https://platform.openai.com/docs/guides/embeddings</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://www.llamaindex.ai">https://www.llamaindex.ai</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p><a href="https://www.langchain.com/">https://www.langchain.com/</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p><a href="https://haystack.deepset.ai/blog/enhancing-rag-pipelines-in-haystack">https://haystack.deepset.ai/blog/enhancing-rag-pipelines-in-haystack</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p><a href="https://huggingface.co/BAAI/bge-reranker-large">https://huggingface.co/BAAI/bge-reranker-large</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p><a href="https://www.trulens.org/trulens_eval/core_concepts_rag_triad/">https://www.trulens.org/trulens_eval/core_concepts_rag_triad/</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p><a href="https://github.com/inverse-scaling/prize">https://github.com/inverse-scaling/prize</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p><a href="https://flowiseai.com">https://flowiseai.com</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p><a href="https://meltano.com">https://meltano.com</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12" role="doc-endnote">
<p><a href="https://cohere.com/coral">https://cohere.com/coral</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13" role="doc-endnote">
<p><a href="https://github.com/weaviate/Verba">https://github.com/weaviate/Verba</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
    </div>

    
    


    
    

    <footer class="post-footer">
      <div class="post-tags">
          <a href="https://weedge.github.io/tags/rag/">RAG</a>
          <a href="https://weedge.github.io/tags/llm/">LLM</a>
          
        </div>

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/doraemon/redis_gcp_rag_stack/">
            
            <i class="iconfont">
              <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

            </i>
            <span class="prev-text nav-default">Redis和GCP AI服务搭建RAG参考架构解决方案</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        
          <a class="next" href="/post/cpu/memory_profiling/">
            <span class="next-text nav-default">译：内存分析</span>
            <span class="prev-text nav-mobile">下一篇</span>
            
            <i class="iconfont">
              <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

            </i>
          </a>
      </nav>
    </footer>
  </article>

  
  

  
  

  

  
  

  

  

  <div class="disqus-comment">
  <div class="disqus-button" id="load_disqus" onclick="load_disqus()">
    显示 Disqus 评论
  </div>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = "https://weedge.github.io/post/paper/rag/rag-for-llms-a-survey/";
    };
    function load_disqus() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'weedge';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

      $('#load_disqus').remove();
    };
  </script>
  <noscript>Please enable JavaScript to view the
    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  </div>

    

  

        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="icon-links">
  
  
    <a href="mailto:weege007@gmail.com" rel="me noopener" class="iconfont"
      title="email" >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://github.com/weedge" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://weibo.com/weedge" rel="me noopener" class="iconfont"
      title="weibo"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M385.714286 733.714286q12-19.428571 6.285714-39.428571t-25.714286-28.571429q-19.428571-8-41.714286-0.571429t-34.285714 26.285714q-12.571429 19.428571-7.428571 39.142857t24.571429 28.857143 42.571429 1.428571 35.714286-27.142857zm53.714286-69.142857q4.571429-7.428571 2-15.142857t-10-10.571429q-8-2.857143-16.285714 2.857143t-12.285714 10.571429q-9.714286 17.714286 7.428571 25.714286 8 2.857143 16.571429 2.857143t12.571429-10.571429zm99.428571 61.142857q-25.714286 58.285714-90.285714 85.714286t-128 6.857143q-61.142857-19.428571-84.285714-72.285714t3.714286-107.142857q26.857143-53.142857 86.571429-79.428571t120.285714-10.857143q63.428571 16.571429 90.571429 68.285714t1.428571 108.857143zm178.285714-91.428571q-5.142857-54.857143-50.857143-97.142857t-119.142857-62.285714-156.857143-12q-127.428571 13.142857-211.142857 80.857143t-75.714286 151.142857q5.142857 54.857143 50.857143 97.142857t119.142857 62.285714 156.857143 12q127.428571-13.142857 211.142857-80.857143t75.714286-151.142857zm176 2.285714q0 38.857143-21.142857 79.714286t-62.285714 78.285714-96.285714 67.142857-129.142857 47.428571-154.571429 17.714286-157.142857-19.142857-137.428571-53.142857-98-86.285714-37.142857-114q0-65.714286 39.714286-140t112.857143-147.428571q96.571429-96.571429 195.142857-134.857143t140.857143 4q37.142857 36.571429 11.428571 119.428571-2.285714 8-0.571429 11.428571t5.714286 4 8.285714 2.857143 7.714286-2l3.428571-1.142857q79.428571-33.714286 140.571429-33.714286t87.428571 34.857143q25.714286 36 0 101.714286-1.142857 7.428571-2.571429 11.428571t2.571429 7.142857 6.857143 4.285714 9.714286 3.428571q32.571429 10.285714 58.857143 26.857143t45.714286 46.571429 19.428571 66.571429zm-42.285714-356.571429q24 26.857143 31.142857 62t-3.714286 67.142857q-4.571429 13.142857-16.857143 19.428571t-25.428571 2.285714q-13.142857-4.571429-19.428571-16.857143t-2.285714-25.428571q11.428571-36-13.714286-63.428571t-61.142857-20q-13.714286 2.857143-25.714286-4.571429t-14.285714-21.142857q-2.857143-13.714286 4.571429-25.428571t21.142857-14.571429q34.285714-7.428571 68 3.142857t57.714286 37.428571zm103.428571-93.142857q49.714286 54.857143 64.285714 127.142857t-7.714286 138q-5.142857 15.428571-19.428571 22.857143t-29.714286 2.285714-22.857143-19.428571-2.857143-29.714286q16-46.857143 5.714286-98.285714t-45.714286-90.285714q-35.428571-39.428571-84.571429-54.571429t-98.857143-4.857143q-16 3.428571-29.714286-5.428571t-17.142857-24.857143 5.428571-29.428571 24.857143-16.857143q70.285714-14.857143 139.428571 6.571429t118.857143 76.857143z"></path>
</svg>

    </a>


<a href="https://weedge.github.io/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
   
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2013 -
    2024
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        weedge
        
      </span></span>

  
  

  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont">
        
        <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

      </i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js" integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin="anonymous"></script>












  
    <script type="text/javascript" src="/js/load-photoswipe.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  









  <script id="dsq-count-scr" src="//weedge.disqus.com/count.js" async></script>






  <script src="/js/copy-to-clipboard.js"></script>


</body>
</html>
