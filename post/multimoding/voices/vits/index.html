<!DOCTYPE html>
<html lang="zh-cn" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>论文解读 VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech - 时间飘过</title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="weedge" />
  <meta name="description" content="前文讲到OpenVoicev2，补充下细节，然后梳理使用的基础模型VITS：
 📓
melo-tts 生成原始音频：
 OpenVoice 版本1不依赖melo-tts, 升级后的V2版本依赖melo-tts, 主要是生成原始音频质量加强了(由melo-tts生成); 默认配置使用了TransformerCouplingBlock list作为flow 和 reverse flow, 而第一版的OpenVoice 模型使用的 ResidualCouplingBlock ; melo-tts的模型权重支持多语言，更具语言区分，比如 ZH: myshell-ai/MeloTTS-Chinese, EN_NEWEST: myshell-ai/MeloTTS-English-v3;  音色转换生成目标音频：
 通过训练好的音色抽取器抽取目标说话者的音色 (myshell-ai/OpenVoiceV2/converter)； 生成的原始音频信息通过 训练抽取好的基础说话者的音色(myshell-ai/OpenVoiceV2/base_speakers/ses)，将原始音频中的音色去除 （flow）； 将去除原始音色的音频 和 抽取好的目标说话者的音色 合并 （reverse flow）； 最终通过 vocoder(也是论文中的Decoder,使用的 HiFi-Gan模型)合成目标音频。  额外注意的是，由melo-tts生成原始音频sample rate是 44100， 而通过音色提取器 提取 并且 生成目标音频sample rate是 22050
 这里简单概括如下：
AE(Autoencoder): 自编码器是自监督系统，其训练目标是通过降维来压缩（或编码）输入数据，然后使用该压缩后的表示准确重建（或解码）其原始输入。无泛化生成能力，但是可以执行特定任务：常用于数据压缩、图像去噪、异常检测和面部识别等任务。
VAE(Variational Autoencoder) :与其他自编码器(Autoencoder(AE)的区别在于它们对潜在空间进行编码的独特方式，以及可以应用其概率编码的不同用例，即随机生成训练数据的变体。具有泛化生成能力。
CVAE(Conditional Variational Autoencoder): 条件变分自编码器 可以以特定输入为条件进行输出，而不仅仅是随机生成训练数据的变体。这是通过将监督学习（或半监督学习）的元素与常规自编码器的传统无监督训练目标相结合来实现的。具有指定特征的泛化能力。
  VAE 与 GAN的区别：
VAE 经常与生成式对抗网络 (GAN) 进行比较，GAN 是另一种模型架构，用于生成类似于训练数据的样本，尤其是图像。
与 VAE 类似，GAN 是结合两种神经网络的联合架构：一个生成器网络，负责输出与训练数据集中的图像相似的图像样本，另一个判别器网络，负责确定特定图像是训练数据中的“真实”图像还是来自生成器网络的“虚假”图像。
这两个网络在零和博弈中进行对抗性训练：来自判别器的反馈用于改进生成器的输出，直到判别器不再能够区分真假样本。
就图像合成而言，两者各有优劣：
 GAN 可以生成更清晰的图像，但由于两种复合模型之间的对抗性权衡，在训练中并不稳定。 VAE 更容易训练，但由于其根据训练数据的“平均”特征生成图像的性质，往往会生成比较模糊的图像。    VAE-GAN 两者结合 顾名思义，VAE-GAN 是变分自编码器 (VAE) 和生成式对抗网络 (GAN) 的混合体。通过用判别器网络替换 VAE 模型的重建损失项，来降低 VAE 生成图像的模糊性，提高生成质量。
  VITS 使用了 条件变分自编码器 (Conditional Variational Autoencoder (CVAE)) 和生成式对抗网络 (Generative adversarial network(GAN)) 两个模型架构。 至于VAE和GAN的细节可以关注下baby-llm这个学习项目中的对应模块PR学习资料:
 VAE: https://github.com/ai-bot-pro/baby-llm/tree/main/modules/VAE | PR: https://github.com/ai-bot-pro/baby-llm/pull/13 GAN: https://github.com/ai-bot-pro/baby-llm/tree/main/modules/GAN | PR: https://github.com/ai-bot-pro/baby-llm/pull/12  这篇文章是讲解VITS，是现在工业上TTS常用的基础方案(NAR模型，成本相对AR模型低， 推理快，生成质量尽可能追平或超越SOTA AR模型)。作者来自韩国现代汽车公司的 AIR 实验室（人工智能研究实验室），论文结合了以前的研究成果：
 2018. FloWaveNet : A Generative Flow for Raw Audio 2020. Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search 2020. HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis  VITS  2021. Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech | paper coder  主要贡献：
 提出了一种并行的端到端 TTS 方法，它可以生成比当前两阶段模型更自然的音频； 采用通过归一化流程和对抗性训练过程增强的变分推理，提高了生成模型的表达能力； 一个随机持续时间预测器（stochastic duration predictor）来从输入文本中合成具有不同节奏的语音； 通过对潜在变量的不确定性建模和随机持续时间预测器，表达了自然的一对多关系，其中文本输入可以以不同的音调（pitches）和节奏（rhythms）以多种方式说出。  通过利用条件变分自编码器 CVAE，模型特点：
 学习直接从文本合成原始波形，而不需要额外的输入条件； 使用动态编程方法 MAS 来搜索最佳对齐方式，而不是与计算损失相比,不需要任何外部对齐器； 并行生成样本； 高效的端到端训练方法, 并且生成质量优于最好的公开可用的两阶段模型。附两阶段的数据处理过程：  第一阶段是从预处理的文本中生成中间语音表示，例如梅尔谱图(mel-spectrograms)或语言特征(linguistic features) 第二阶段是生成以中间表示为条件的原始波形。 两阶段的相关模型都是独立开发的。    结构：
PS： achatbot 集成了OpenVoiceV2 with meloTTS(meloTTS代码大部分来自VITS，Flow 采用 Transformer Encoder 结构来自 [VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design](https://arxiv.org/abs/2307.16430) | paper code
PR地址： https://github.com/ai-bot-pro/achatbot/pull/103
" />

  <meta name="keywords" content="工作, 技术, 生活" />






<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://weedge.github.io/post/multimoding/voices/vits/" />





<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css" integrity="sha256-&#43;ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media="screen" crossorigin="anonymous">




<link rel="stylesheet" href="/css/copy-to-clipboard.css">


<meta property="og:title" content="论文解读 VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech" />
<meta property="og:description" content="前文讲到OpenVoicev2，补充下细节，然后梳理使用的基础模型VITS：

📓
melo-tts 生成原始音频：

OpenVoice 版本1不依赖melo-tts, 升级后的V2版本依赖melo-tts, 主要是生成原始音频质量加强了(由melo-tts生成);
默认配置使用了TransformerCouplingBlock list作为flow 和 reverse flow, 而第一版的OpenVoice 模型使用的 ResidualCouplingBlock ;
melo-tts的模型权重支持多语言，更具语言区分，比如 ZH: myshell-ai/MeloTTS-Chinese, EN_NEWEST: myshell-ai/MeloTTS-English-v3;

音色转换生成目标音频：

通过训练好的音色抽取器抽取目标说话者的音色 (myshell-ai/OpenVoiceV2/converter)；
生成的原始音频信息通过 训练抽取好的基础说话者的音色(myshell-ai/OpenVoiceV2/base_speakers/ses)，将原始音频中的音色去除 （flow）；
将去除原始音色的音频 和 抽取好的目标说话者的音色 合并 （reverse flow）； 最终通过 vocoder(也是论文中的Decoder,使用的 HiFi-Gan模型)合成目标音频。

额外注意的是，由melo-tts生成原始音频sample rate是 44100， 而通过音色提取器 提取 并且 生成目标音频sample rate是 22050

这里简单概括如下：
AE(Autoencoder): 自编码器是自监督系统，其训练目标是通过降维来压缩（或编码）输入数据，然后使用该压缩后的表示准确重建（或解码）其原始输入。无泛化生成能力，但是可以执行特定任务：常用于数据压缩、图像去噪、异常检测和面部识别等任务。
VAE(Variational Autoencoder) :与其他自编码器(Autoencoder(AE)的区别在于它们对潜在空间进行编码的独特方式，以及可以应用其概率编码的不同用例，即随机生成训练数据的变体。具有泛化生成能力。
CVAE(Conditional Variational Autoencoder): 条件变分自编码器 可以以特定输入为条件进行输出，而不仅仅是随机生成训练数据的变体。这是通过将监督学习（或半监督学习）的元素与常规自编码器的传统无监督训练目标相结合来实现的。具有指定特征的泛化能力。


VAE 与 GAN的区别：
VAE 经常与生成式对抗网络 (GAN) 进行比较，GAN 是另一种模型架构，用于生成类似于训练数据的样本，尤其是图像。
与 VAE 类似，GAN 是结合两种神经网络的联合架构：一个生成器网络，负责输出与训练数据集中的图像相似的图像样本，另一个判别器网络，负责确定特定图像是训练数据中的“真实”图像还是来自生成器网络的“虚假”图像。
这两个网络在零和博弈中进行对抗性训练：来自判别器的反馈用于改进生成器的输出，直到判别器不再能够区分真假样本。
就图像合成而言，两者各有优劣：

GAN 可以生成更清晰的图像，但由于两种复合模型之间的对抗性权衡，在训练中并不稳定。
VAE 更容易训练，但由于其根据训练数据的“平均”特征生成图像的性质，往往会生成比较模糊的图像。



VAE-GAN 两者结合
顾名思义，VAE-GAN 是变分自编码器 (VAE) 和生成式对抗网络 (GAN) 的混合体。通过用判别器网络替换 VAE 模型的重建损失项，来降低 VAE 生成图像的模糊性，提高生成质量。


VITS 使用了 条件变分自编码器 (Conditional Variational Autoencoder (CVAE)) 和生成式对抗网络 (Generative adversarial network(GAN)) 两个模型架构。 至于VAE和GAN的细节可以关注下baby-llm这个学习项目中的对应模块PR学习资料:

VAE: https://github.com/ai-bot-pro/baby-llm/tree/main/modules/VAE | PR:  https://github.com/ai-bot-pro/baby-llm/pull/13
GAN: https://github.com/ai-bot-pro/baby-llm/tree/main/modules/GAN | PR: https://github.com/ai-bot-pro/baby-llm/pull/12

这篇文章是讲解VITS，是现在工业上TTS常用的基础方案(NAR模型，成本相对AR模型低， 推理快，生成质量尽可能追平或超越SOTA AR模型)。作者来自韩国现代汽车公司的 AIR 实验室（人工智能研究实验室），论文结合了以前的研究成果：

2018. FloWaveNet : A Generative Flow for Raw Audio
2020. Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search
2020. HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis

VITS

2021. Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech  | paper coder

主要贡献：

提出了一种并行的端到端 TTS 方法，它可以生成比当前两阶段模型更自然的音频；
采用通过归一化流程和对抗性训练过程增强的变分推理，提高了生成模型的表达能力；
一个随机持续时间预测器（stochastic duration predictor）来从输入文本中合成具有不同节奏的语音；
通过对潜在变量的不确定性建模和随机持续时间预测器，表达了自然的一对多关系，其中文本输入可以以不同的音调（pitches）和节奏（rhythms）以多种方式说出。

通过利用条件变分自编码器 CVAE，模型特点：

学习直接从文本合成原始波形，而不需要额外的输入条件；
使用动态编程方法 MAS 来搜索最佳对齐方式，而不是与计算损失相比,不需要任何外部对齐器；
并行生成样本；
高效的端到端训练方法, 并且生成质量优于最好的公开可用的两阶段模型。附两阶段的数据处理过程：

第一阶段是从预处理的文本中生成中间语音表示，例如梅尔谱图(mel-spectrograms)或语言特征(linguistic features)
第二阶段是生成以中间表示为条件的原始波形。
两阶段的相关模型都是独立开发的。



结构：

PS： achatbot 集成了OpenVoiceV2 with meloTTS(meloTTS代码大部分来自VITS，Flow 采用 Transformer Encoder 结构来自 [VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design](https://arxiv.org/abs/2307.16430) | paper code
PR地址： https://github.com/ai-bot-pro/achatbot/pull/103" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://weedge.github.io/post/multimoding/voices/vits/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2025-01-11T10:26:23+08:00" />
<meta property="article:modified_time" content="2025-01-11T10:26:23+08:00" />

<meta itemprop="name" content="论文解读 VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech">
<meta itemprop="description" content="前文讲到OpenVoicev2，补充下细节，然后梳理使用的基础模型VITS：

📓
melo-tts 生成原始音频：

OpenVoice 版本1不依赖melo-tts, 升级后的V2版本依赖melo-tts, 主要是生成原始音频质量加强了(由melo-tts生成);
默认配置使用了TransformerCouplingBlock list作为flow 和 reverse flow, 而第一版的OpenVoice 模型使用的 ResidualCouplingBlock ;
melo-tts的模型权重支持多语言，更具语言区分，比如 ZH: myshell-ai/MeloTTS-Chinese, EN_NEWEST: myshell-ai/MeloTTS-English-v3;

音色转换生成目标音频：

通过训练好的音色抽取器抽取目标说话者的音色 (myshell-ai/OpenVoiceV2/converter)；
生成的原始音频信息通过 训练抽取好的基础说话者的音色(myshell-ai/OpenVoiceV2/base_speakers/ses)，将原始音频中的音色去除 （flow）；
将去除原始音色的音频 和 抽取好的目标说话者的音色 合并 （reverse flow）； 最终通过 vocoder(也是论文中的Decoder,使用的 HiFi-Gan模型)合成目标音频。

额外注意的是，由melo-tts生成原始音频sample rate是 44100， 而通过音色提取器 提取 并且 生成目标音频sample rate是 22050

这里简单概括如下：
AE(Autoencoder): 自编码器是自监督系统，其训练目标是通过降维来压缩（或编码）输入数据，然后使用该压缩后的表示准确重建（或解码）其原始输入。无泛化生成能力，但是可以执行特定任务：常用于数据压缩、图像去噪、异常检测和面部识别等任务。
VAE(Variational Autoencoder) :与其他自编码器(Autoencoder(AE)的区别在于它们对潜在空间进行编码的独特方式，以及可以应用其概率编码的不同用例，即随机生成训练数据的变体。具有泛化生成能力。
CVAE(Conditional Variational Autoencoder): 条件变分自编码器 可以以特定输入为条件进行输出，而不仅仅是随机生成训练数据的变体。这是通过将监督学习（或半监督学习）的元素与常规自编码器的传统无监督训练目标相结合来实现的。具有指定特征的泛化能力。


VAE 与 GAN的区别：
VAE 经常与生成式对抗网络 (GAN) 进行比较，GAN 是另一种模型架构，用于生成类似于训练数据的样本，尤其是图像。
与 VAE 类似，GAN 是结合两种神经网络的联合架构：一个生成器网络，负责输出与训练数据集中的图像相似的图像样本，另一个判别器网络，负责确定特定图像是训练数据中的“真实”图像还是来自生成器网络的“虚假”图像。
这两个网络在零和博弈中进行对抗性训练：来自判别器的反馈用于改进生成器的输出，直到判别器不再能够区分真假样本。
就图像合成而言，两者各有优劣：

GAN 可以生成更清晰的图像，但由于两种复合模型之间的对抗性权衡，在训练中并不稳定。
VAE 更容易训练，但由于其根据训练数据的“平均”特征生成图像的性质，往往会生成比较模糊的图像。



VAE-GAN 两者结合
顾名思义，VAE-GAN 是变分自编码器 (VAE) 和生成式对抗网络 (GAN) 的混合体。通过用判别器网络替换 VAE 模型的重建损失项，来降低 VAE 生成图像的模糊性，提高生成质量。


VITS 使用了 条件变分自编码器 (Conditional Variational Autoencoder (CVAE)) 和生成式对抗网络 (Generative adversarial network(GAN)) 两个模型架构。 至于VAE和GAN的细节可以关注下baby-llm这个学习项目中的对应模块PR学习资料:

VAE: https://github.com/ai-bot-pro/baby-llm/tree/main/modules/VAE | PR:  https://github.com/ai-bot-pro/baby-llm/pull/13
GAN: https://github.com/ai-bot-pro/baby-llm/tree/main/modules/GAN | PR: https://github.com/ai-bot-pro/baby-llm/pull/12

这篇文章是讲解VITS，是现在工业上TTS常用的基础方案(NAR模型，成本相对AR模型低， 推理快，生成质量尽可能追平或超越SOTA AR模型)。作者来自韩国现代汽车公司的 AIR 实验室（人工智能研究实验室），论文结合了以前的研究成果：

2018. FloWaveNet : A Generative Flow for Raw Audio
2020. Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search
2020. HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis

VITS

2021. Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech  | paper coder

主要贡献：

提出了一种并行的端到端 TTS 方法，它可以生成比当前两阶段模型更自然的音频；
采用通过归一化流程和对抗性训练过程增强的变分推理，提高了生成模型的表达能力；
一个随机持续时间预测器（stochastic duration predictor）来从输入文本中合成具有不同节奏的语音；
通过对潜在变量的不确定性建模和随机持续时间预测器，表达了自然的一对多关系，其中文本输入可以以不同的音调（pitches）和节奏（rhythms）以多种方式说出。

通过利用条件变分自编码器 CVAE，模型特点：

学习直接从文本合成原始波形，而不需要额外的输入条件；
使用动态编程方法 MAS 来搜索最佳对齐方式，而不是与计算损失相比,不需要任何外部对齐器；
并行生成样本；
高效的端到端训练方法, 并且生成质量优于最好的公开可用的两阶段模型。附两阶段的数据处理过程：

第一阶段是从预处理的文本中生成中间语音表示，例如梅尔谱图(mel-spectrograms)或语言特征(linguistic features)
第二阶段是生成以中间表示为条件的原始波形。
两阶段的相关模型都是独立开发的。



结构：

PS： achatbot 集成了OpenVoiceV2 with meloTTS(meloTTS代码大部分来自VITS，Flow 采用 Transformer Encoder 结构来自 [VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design](https://arxiv.org/abs/2307.16430) | paper code
PR地址： https://github.com/ai-bot-pro/achatbot/pull/103"><meta itemprop="datePublished" content="2025-01-11T10:26:23+08:00" />
<meta itemprop="dateModified" content="2025-01-11T10:26:23+08:00" />
<meta itemprop="wordCount" content="4278">
<meta itemprop="keywords" content="flow,NAR,VITS," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="论文解读 VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"/>
<meta name="twitter:description" content="前文讲到OpenVoicev2，补充下细节，然后梳理使用的基础模型VITS：

📓
melo-tts 生成原始音频：

OpenVoice 版本1不依赖melo-tts, 升级后的V2版本依赖melo-tts, 主要是生成原始音频质量加强了(由melo-tts生成);
默认配置使用了TransformerCouplingBlock list作为flow 和 reverse flow, 而第一版的OpenVoice 模型使用的 ResidualCouplingBlock ;
melo-tts的模型权重支持多语言，更具语言区分，比如 ZH: myshell-ai/MeloTTS-Chinese, EN_NEWEST: myshell-ai/MeloTTS-English-v3;

音色转换生成目标音频：

通过训练好的音色抽取器抽取目标说话者的音色 (myshell-ai/OpenVoiceV2/converter)；
生成的原始音频信息通过 训练抽取好的基础说话者的音色(myshell-ai/OpenVoiceV2/base_speakers/ses)，将原始音频中的音色去除 （flow）；
将去除原始音色的音频 和 抽取好的目标说话者的音色 合并 （reverse flow）； 最终通过 vocoder(也是论文中的Decoder,使用的 HiFi-Gan模型)合成目标音频。

额外注意的是，由melo-tts生成原始音频sample rate是 44100， 而通过音色提取器 提取 并且 生成目标音频sample rate是 22050

这里简单概括如下：
AE(Autoencoder): 自编码器是自监督系统，其训练目标是通过降维来压缩（或编码）输入数据，然后使用该压缩后的表示准确重建（或解码）其原始输入。无泛化生成能力，但是可以执行特定任务：常用于数据压缩、图像去噪、异常检测和面部识别等任务。
VAE(Variational Autoencoder) :与其他自编码器(Autoencoder(AE)的区别在于它们对潜在空间进行编码的独特方式，以及可以应用其概率编码的不同用例，即随机生成训练数据的变体。具有泛化生成能力。
CVAE(Conditional Variational Autoencoder): 条件变分自编码器 可以以特定输入为条件进行输出，而不仅仅是随机生成训练数据的变体。这是通过将监督学习（或半监督学习）的元素与常规自编码器的传统无监督训练目标相结合来实现的。具有指定特征的泛化能力。


VAE 与 GAN的区别：
VAE 经常与生成式对抗网络 (GAN) 进行比较，GAN 是另一种模型架构，用于生成类似于训练数据的样本，尤其是图像。
与 VAE 类似，GAN 是结合两种神经网络的联合架构：一个生成器网络，负责输出与训练数据集中的图像相似的图像样本，另一个判别器网络，负责确定特定图像是训练数据中的“真实”图像还是来自生成器网络的“虚假”图像。
这两个网络在零和博弈中进行对抗性训练：来自判别器的反馈用于改进生成器的输出，直到判别器不再能够区分真假样本。
就图像合成而言，两者各有优劣：

GAN 可以生成更清晰的图像，但由于两种复合模型之间的对抗性权衡，在训练中并不稳定。
VAE 更容易训练，但由于其根据训练数据的“平均”特征生成图像的性质，往往会生成比较模糊的图像。



VAE-GAN 两者结合
顾名思义，VAE-GAN 是变分自编码器 (VAE) 和生成式对抗网络 (GAN) 的混合体。通过用判别器网络替换 VAE 模型的重建损失项，来降低 VAE 生成图像的模糊性，提高生成质量。


VITS 使用了 条件变分自编码器 (Conditional Variational Autoencoder (CVAE)) 和生成式对抗网络 (Generative adversarial network(GAN)) 两个模型架构。 至于VAE和GAN的细节可以关注下baby-llm这个学习项目中的对应模块PR学习资料:

VAE: https://github.com/ai-bot-pro/baby-llm/tree/main/modules/VAE | PR:  https://github.com/ai-bot-pro/baby-llm/pull/13
GAN: https://github.com/ai-bot-pro/baby-llm/tree/main/modules/GAN | PR: https://github.com/ai-bot-pro/baby-llm/pull/12

这篇文章是讲解VITS，是现在工业上TTS常用的基础方案(NAR模型，成本相对AR模型低， 推理快，生成质量尽可能追平或超越SOTA AR模型)。作者来自韩国现代汽车公司的 AIR 实验室（人工智能研究实验室），论文结合了以前的研究成果：

2018. FloWaveNet : A Generative Flow for Raw Audio
2020. Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search
2020. HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis

VITS

2021. Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech  | paper coder

主要贡献：

提出了一种并行的端到端 TTS 方法，它可以生成比当前两阶段模型更自然的音频；
采用通过归一化流程和对抗性训练过程增强的变分推理，提高了生成模型的表达能力；
一个随机持续时间预测器（stochastic duration predictor）来从输入文本中合成具有不同节奏的语音；
通过对潜在变量的不确定性建模和随机持续时间预测器，表达了自然的一对多关系，其中文本输入可以以不同的音调（pitches）和节奏（rhythms）以多种方式说出。

通过利用条件变分自编码器 CVAE，模型特点：

学习直接从文本合成原始波形，而不需要额外的输入条件；
使用动态编程方法 MAS 来搜索最佳对齐方式，而不是与计算损失相比,不需要任何外部对齐器；
并行生成样本；
高效的端到端训练方法, 并且生成质量优于最好的公开可用的两阶段模型。附两阶段的数据处理过程：

第一阶段是从预处理的文本中生成中间语音表示，例如梅尔谱图(mel-spectrograms)或语言特征(linguistic features)
第二阶段是生成以中间表示为条件的原始波形。
两阶段的相关模型都是独立开发的。



结构：

PS： achatbot 集成了OpenVoiceV2 with meloTTS(meloTTS代码大部分来自VITS，Flow 采用 Transformer Encoder 结构来自 [VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design](https://arxiv.org/abs/2307.16430) | paper code
PR地址： https://github.com/ai-bot-pro/achatbot/pull/103"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->



<script>
  MathJax = {
    tex: {
      inlineMath: [["$", "$"]],
    },
    displayMath: [
      ["$$", "$$"],
      ["\[\[", "\]\]"],
    ],
    svg: {
      fontCache: "global",
    },
  };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>





</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">时间飘过</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/">主页</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/post/">归档</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/tags/">标签</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/categories/">分类</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/about/">About</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/perf-book-cn/zh/" rel="noopener" target="_blank">
              《现代CPU性能分析与优化》
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          
        
      </li>
    

    
  </ul>
</nav>


  
    






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

  

  

  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      时间飘过
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/">主页</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/post/">归档</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/tags/">标签</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/categories/">分类</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/about/">About</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/perf-book-cn/zh/" rel="noopener" target="_blank">
              《现代CPU性能分析与优化》
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">论文解读 VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</h1>
      
      <div class="post-meta">
        <time datetime="2025-01-11" class="post-time">
          2025-01-11
        </time>
        <div class="post-category">
            <a href="https://weedge.github.io/categories/%E6%8A%80%E6%9C%AF/"> 技术 </a>
            <a href="https://weedge.github.io/categories/tts/"> TTS </a>
            
          </div>
        

        
        

        
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#cvae">CVAE:</a></li>
    <li><a href="#posterior-encoder">Posterior Encoder:</a></li>
    <li><a href="#prior-encoder">Prior Encoder:</a></li>
    <li><a href="#flow-the-normalizing-flow">Flow (The Normalizing Flow):</a></li>
    <li><a href="#masmonotonic-alignment-search">MAS(Monotonic Alignment Search):</a></li>
    <li><a href="#stochastic-duration-predictor">Stochastic Duration Predictor:</a></li>
    <li><a href="#decoder-hifi-gan-generator">Decoder (HiFi-Gan Generator):</a></li>
    <li><a href="#discriminatorhifi-gan-multi-period-discriminator">Discriminator(HiFi-Gan Multi-Period Discriminator)</a></li>
  </ul>
</nav>
  </div>
</div>

    
    <div class="post-content">
      <p>前文讲到OpenVoicev2，补充下细节，然后梳理使用的基础模型VITS：</p>
<blockquote>
<p>📓</p>
<p>melo-tts 生成原始音频：</p>
<ul>
<li>OpenVoice 版本1不依赖melo-tts, 升级后的V2版本依赖melo-tts, 主要是生成原始音频质量加强了(由melo-tts生成);</li>
<li>默认配置使用了TransformerCouplingBlock list作为flow 和 reverse flow, 而第一版的OpenVoice 模型使用的 ResidualCouplingBlock ;</li>
<li>melo-tts的模型权重支持多语言，更具语言区分，比如 ZH: <a href="https://huggingface.co/myshell-ai/MeloTTS-Chinese">myshell-ai/MeloTTS-Chinese</a>, EN_NEWEST: <a href="https://huggingface.co/myshell-ai/MeloTTS-English-v3">myshell-ai/MeloTTS-English-v3</a>;</li>
</ul>
<p>音色转换生成目标音频：</p>
<ul>
<li>通过训练好的<strong>音色抽取器</strong>抽取目标说话者的音色 (<a href="https://huggingface.co/myshell-ai/OpenVoiceV2/tree/main/base_speakers/ses">myshell-ai/OpenVoiceV2/converter</a>)；</li>
<li>生成的原始音频信息通过 训练抽取好的基础说话者的音色(<a href="https://huggingface.co/myshell-ai/OpenVoiceV2/tree/main/converter">myshell-ai/OpenVoiceV2/base_speakers/ses</a>)，将原始音频中的音色去除 （flow）；</li>
<li>将去除原始音色的音频 和 抽取好的目标说话者的音色 合并 （reverse flow）； 最终通过 vocoder(也是论文中的Decoder,使用的 HiFi-Gan模型)合成目标音频。</li>
</ul>
<p>额外注意的是，由melo-tts生成原始音频sample rate是 44100， 而通过音色提取器 提取 并且 生成目标音频sample rate是 22050</p>
</blockquote>
<p>这里简单概括如下：</p>
<p>AE(Autoencoder): 自编码器是<a href="https://www.ibm.com/cn-zh/topics/self-supervised-learning">自监督</a>系统，其训练目标是通过降维来压缩（或<em>编码</em>）输入数据，然后使用该压缩后的表示准确重建（或<em>解码</em>）其原始输入。无泛化生成能力，但是可以执行特定任务：常用于数据压缩、图像去噪、异常检测和面部识别等任务。</p>
<p><strong>VAE(Variational Autoencoder)</strong> :与其他自编码器(Autoencoder(AE)的区别在于它们对潜在空间进行编码的独特方式，以及可以应用其概率编码的不同用例，即随机生成训练数据的变体。具有泛化生成能力。</p>
<p><strong>CVAE(Conditional Variational Autoencoder)</strong>: 条件变分自编码器 可以以特定输入为条件进行输出，而不仅仅是随机生成训练数据的变体。这是通过将监督学习（或半监督学习）的元素与常规自编码器的传统无监督训练目标相结合来实现的。具有指定特征的泛化能力。</p>
<ul>
<li>
<p>VAE 与 GAN的区别：</p>
<p>VAE 经常与生成式对抗网络 (GAN) 进行比较，GAN 是另一种模型架构，用于生成类似于训练数据的样本，尤其是图像。</p>
<p>与 VAE 类似，GAN 是结合两种神经网络的联合架构：一个生成器网络，负责输出与训练数据集中的图像相似的图像样本，另一个判别器网络，负责确定特定图像是训练数据中的“真实”图像还是来自生成器网络的“虚假”图像。</p>
<p>这两个网络在零和博弈中进行对抗性训练：来自判别器的反馈用于改进生成器的输出，直到判别器不再能够区分真假样本。</p>
<p>就图像合成而言，两者各有优劣：</p>
<ul>
<li>GAN 可以生成更清晰的图像，但由于两种复合模型之间的对抗性权衡，在训练中并不稳定。</li>
<li>VAE 更容易训练，但由于其根据训练数据的“平均”特征生成图像的性质，往往会生成比较模糊的图像。</li>
</ul>
</li>
<li>
<p>VAE-GAN 两者结合
顾名思义，VAE-GAN 是变分自编码器 (VAE) 和生成式对抗网络 (GAN) 的混合体。通过用判别器网络替换 VAE 模型的重建损失项，来降低 VAE 生成图像的模糊性，提高生成质量。</p>
</li>
</ul>
<p>VITS 使用了 条件变分自编码器 (Conditional Variational Autoencoder (CVAE)) 和生成式对抗网络 (<a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative adversarial network</a>(GAN)) 两个模型架构。 至于VAE和GAN的细节可以关注下baby-llm这个学习项目中的对应模块PR学习资料:</p>
<ul>
<li>VAE: <a href="https://github.com/ai-bot-pro/baby-llm/tree/main/modules/VAE">https://github.com/ai-bot-pro/baby-llm/tree/main/modules/VAE</a> | PR:  <a href="https://github.com/ai-bot-pro/baby-llm/pull/13">https://github.com/ai-bot-pro/baby-llm/pull/13</a></li>
<li>GAN: <a href="https://github.com/ai-bot-pro/baby-llm/tree/main/modules/GAN">https://github.com/ai-bot-pro/baby-llm/tree/main/modules/GAN</a> | PR: <a href="https://github.com/ai-bot-pro/baby-llm/pull/12">https://github.com/ai-bot-pro/baby-llm/pull/12</a></li>
</ul>
<p>这篇文章是讲解VITS，是现在工业上TTS常用的基础方案(NAR模型，成本相对AR模型低， 推理快，生成质量尽可能追平或超越SOTA AR模型)。作者来自韩国现代汽车公司的 AIR 实验室（人工智能研究实验室），论文结合了以前的研究成果：</p>
<ul>
<li><a href="https://arxiv.org/abs/1811.02155">2018. <strong>FloWaveNet : A Generative Flow for Raw Audio</strong></a></li>
<li><a href="https://arxiv.org/abs/2005.11129">2020. <strong>Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search</strong></a></li>
<li><a href="https://arxiv.org/abs/2010.05646">2020. <strong>HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</strong></a></li>
</ul>
<h1 id="vits">VITS</h1>
<ul>
<li><a href="https://arxiv.org/abs/2106.06103">2021. <strong>Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</strong></a>  | <a href="https://github.com/jaywalnut310/vits">paper coder</a></li>
</ul>
<p>主要贡献：</p>
<ul>
<li>提出了一种并行的端到端 TTS 方法，它可以生成比当前两阶段模型更自然的音频；</li>
<li>采用通过归一化流程和对抗性训练过程增强的变分推理，提高了生成模型的表达能力；</li>
<li>一个随机持续时间预测器（stochastic duration predictor）来从输入文本中合成具有不同节奏的语音；</li>
<li>通过对潜在变量的不确定性建模和随机持续时间预测器，表达了自然的一对多关系，其中文本输入可以以不同的音调（pitches）和节奏（rhythms）以多种方式说出。</li>
</ul>
<p>通过利用条件变分自编码器 CVAE，模型特点：</p>
<ol>
<li>学习直接从文本合成原始波形，而不需要额外的输入条件；</li>
<li>使用动态编程方法 MAS 来搜索最佳对齐方式，而不是与计算损失相比,不需要任何外部对齐器；</li>
<li>并行生成样本；</li>
<li>高效的端到端训练方法, 并且生成质量优于最好的公开可用的两阶段模型。附两阶段的数据处理过程：
<ul>
<li>第一阶段是从预处理的文本中生成中间语音表示，例如梅尔谱图(mel-spectrograms)或语言特征(linguistic features)</li>
<li>第二阶段是生成以中间表示为条件的原始波形。</li>
<li>两阶段的相关模型都是独立开发的。</li>
</ul>
</li>
</ol>
<p>结构：</p>
<p><img src="https://github.com/user-attachments/assets/3ddec975-a9fd-460c-91fa-894b8ebd8c8c" alt="VITS"></p>
<p>PS： <a href="https://github.com/ai-bot-pro/achatbot">achatbot</a> 集成了OpenVoiceV2 with meloTTS(meloTTS代码大部分来自VITS，Flow 采用 Transformer Encoder 结构来自 [<a href="https://arxiv.org/abs/2307.16430">VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design</a>](<a href="https://arxiv.org/abs/2307.16430">https://arxiv.org/abs/2307.16430</a>) | <a href="https://github.com/daniilrobnikov/vits2">paper code</a></p>
<p>PR地址： <a href="https://github.com/ai-bot-pro/achatbot/pull/103">https://github.com/ai-bot-pro/achatbot/pull/103</a></p>
<h2 id="cvae">CVAE:</h2>
<p>条件 VAE:
传统“原始” VAE 的一个缺点是用户无法控制自动编码器生成的特定输出。例如，使用前面提到的 MNIST 数据集训练的传统 VAE，可以生成从 0 到 9 的手写数字新样本，但不能局限于仅输出 4 和 7。</p>
<p>顾名思义，条件 VAE (CVAE) 可以以特定输入为条件进行输出，而不仅仅是随机生成训练数据的变体。这是通过将监督学习（或半监督学习）的元素与常规自编码器的传统无监督训练目标相结合来实现的。</p>
<p>通过在特定变量的标记示例上进一步训练模型，这些变量可用于调节解码器的输出。例如，CVAE 可首先在面部图像的大数据集上进行训练，然后使用监督学习进行训练，学习“胡须”的潜在编码，从而输出新的有胡须面部的图像。</p>
<ul>
<li><a href="https://arxiv.org/abs/1312.6114">2013. Auto-Encoding Variational Bayes</a></li>
<li>来自 <a href="http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generati">2015. Learning Structured Output Representation using Deep Conditional Generative Models</a>(CVAE)</li>
<li><a href="https://github.com/ucals/cvae">https://github.com/ucals/cvae</a></li>
<li><a href="https://www.ibm.com/cn-zh/think/topics/variational-autoencoder">https://www.ibm.com/cn-zh/think/topics/variational-autoencoder</a></li>
</ul>
<h2 id="posterior-encoder">Posterior Encoder:</h2>
<p>WaveNet residual block由具有门控激活单元和跳跃连接的扩张卷积层组成(LSTM+CNN)。块上方的线性投影层产生正态后验分布的均值μ和方差δ。对于多说话人的情况，我们在残差块中使用全局调节(global conditioning)来添加说话人嵌入。</p>
<ul>
<li><a href="https://arxiv.org/abs/1609.03499">2016. <strong>WaveNet: A Generative Model for Raw Audio</strong></a>(WaveNet residual block) PS: 在OpenVoice(meloTTS)中的实现采用的Transformer Encoder, 来自 [<a href="https://arxiv.org/abs/2307.16430">VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design</a>](<a href="https://arxiv.org/abs/2307.16430">https://arxiv.org/abs/2307.16430</a>) | <a href="https://github.com/daniilrobnikov/vits2">paper code</a></li>
</ul>
<h2 id="prior-encoder">Prior Encoder:</h2>
<p>为了提高先验分布的表示能力，我们将归一化流添加到条件先验网络中，从而生成更真实的样本。</p>
<ul>
<li>Text Encoder from Transformer Encoder <a href="https://arxiv.org/abs/1706.03762">2017. <strong>Attention Is All You Need</strong></a></li>
<li>Relative Position Representations <a href="https://arxiv.org/abs/1803.02155">2018. Self-attention with relative position representations</a></li>
<li>The Normalizing Flow <a href="https://arxiv.org/abs/1605.08803">2017. <strong>Density estimation using Real NVP</strong></a> 一堆 WaveNet 残差块组成的仿射耦合层堆栈</li>
</ul>
<h2 id="flow-the-normalizing-flow">Flow (The Normalizing Flow):</h2>
<ul>
<li><a href="https://arxiv.org/abs/1505.05770">2015. Variational Inference with Normalizing Flows</a> 通过标准化流增强先验和后验分布的表达能力来提高 VAE 性能</li>
<li>来自 <a href="https://arxiv.org/abs/1811.02155">2018. <strong>FloWaveNet : A Generative Flow for Raw Audio</strong></a> | <a href="https://github.com/ksw0306/FloWaveNet">paper code</a>
FloWaveNet，这是一种基于流的方法，是原始音频合成的实时并行生成模型的替代方案。 FloWaveNet<strong>只需要单个最大似然损失，不需要任何辅助损失项</strong>，同时保持训练的稳定性。它具有<strong>简化的单阶段训练方案</strong>，因为它<strong>不需要教师网络并且可以进行端到端训练</strong>。该模型本质上是<strong>并行的</strong>，因为<strong>基于流的生成可以实现实时波形合成</strong>。 FloWaveNet 可以作为 WaveNet 声码器的直接替代品，后者用于各种文本转语音架构。除了上述所有优点之外，FloWaveNet 样本的质量和保真度与两阶段模型相当。</li>
</ul>
<p><img src="https://github.com/user-attachments/assets/d7f27c1a-3c68-463c-b76f-1568c0fab811" alt="FlowWaveNet"></p>
<h2 id="masmonotonic-alignment-search">MAS(Monotonic Alignment Search):</h2>
<ul>
<li>来自 <a href="https://arxiv.org/abs/2005.11129">2020. <strong>Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search</strong></a>
Glow-TTS: 一种基于流(FloWaveNet)的并行 TTS 生成模型，<strong>不需要任何外部对齐器</strong>(FastSpeech 和 ParaNet 之类的文本转语音 (TTS) 模型来并行地从文本生成梅尔频谱图。但如果没有自回归 TTS 模型作为外部对齐器的指导，并行 TTS 模型就无法进行训练)。通过结合流和动态规划的特性，所提出的模型搜索文本和语音的潜在表示之间最可能的单调对齐。我们证明，强制执行硬单调对齐可以实现稳健的 TTS，从而泛化到长话语，并且采用生成流可以实现快速、多样化且可控的语音合成。</li>
</ul>
<p><img src="https://github.com/user-attachments/assets/47e3a2ed-ce3c-4644-ae26-565e3ce1b69d" alt="GlowTTS"></p>
<h2 id="stochastic-duration-predictor">Stochastic Duration Predictor:</h2>
<p>生成类似人类的语音节奏，其样本遵循给定音素的持续时间分布；</p>
<p>随机持续时间预测器是基于流的生成模型，通常通过最大似然估计进行训练。然而，最大似然估计的直接应用很困难，因为每个输入音素的持续时间是 1) 一个离散整数，需要对其进行反量化以使用连续归一化流；2) 一个标量，因为可逆性，这会阻止高维变换。应用变分反量化<a href="https://arxiv.org/abs/1902.00275">Flow++: Improving flow-based generative models with variational dequantization and architecture design</a>和变分数据增强<a href="https://arxiv.org/abs/2002.09741">VFlow: More Expressive Generative Flows with Variational Data Augmentation</a>来解决这些问题。</p>
<ul>
<li>基于duration predictor 来自<a href="https://arxiv.org/abs/1905.09263">2019. FastSpeech: Fast, Robust and Controllable Text to Speech</a>，用于Length Regulator模块中。
<ul>
<li>Length Regulator 解决前馈变换器中音素和频谱图序列长度不匹配的问题，以及控制语音速度和部分韵律。音素序列的长度通常小于其梅尔谱图序列的长度，并且每个音素对应于多个梅尔谱图。我们将与音素相对应的梅尔谱图的长度称为音素持续时间。</li>
<li>duration predictor 持续时间预测器由一个带有 ReLU 激活的 2 层 1D 卷积网络组成，每个网络后面都有归一化层和 dropout 层，以及一个额外的线性层来输出标量，这正是预测音素持续时间。</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/user-attachments/assets/0571d8cd-52e0-48f9-8db6-7b129c645bf1" alt="FastSpeech"></p>
<ul>
<li>后续对FastSpeech的改进<a href="https://arxiv.org/abs/2006.04558">2020. FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a> 其中FastSpeech 2s 是第一次完全并行地从音素序列直接生成波形的尝试，而不是语言特征或梅尔谱图，端到端的生成，直接从文本生成波形，无需级联梅尔频谱图生成（声学模型）和波形生成（声码器）；通常标准的非自回归声码器，转换时间对齐的语言特征到波形，并且需要单独的语言模型将输入文本转换为语言特征或声学模型将输入文本转换为声学特征（例如，梅尔谱图mel-spectrograms）</li>
</ul>
<p><img src="https://github.com/user-attachments/assets/fdc5ca0b-b099-4eed-a765-b2253ade3aca" alt="FastSpeech2s"></p>
<h2 id="decoder-hifi-gan-generator">Decoder (HiFi-Gan Generator):</h2>
<p>由一堆转置卷积组成，每个转置卷积后跟一个多感受野融合模块（MRF）。 MRF的输出是具有不同感受野大小的残差块的输出之和。对于多说话人设置，我们添加一个线性层来转换说话人嵌入并将其添加到输入潜在变量z中</p>
<ul>
<li><a href="https://arxiv.org/abs/2010.05646">2020. <strong>HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</strong></a></li>
</ul>
<p>HiFi-Gan Generator(Decoder)</p>
<p><img src="https://github.com/user-attachments/assets/8b09b501-bfef-48eb-b823-ba8d78307c31" alt="HiFi-Gan Generator"></p>
<h2 id="discriminatorhifi-gan-multi-period-discriminator">Discriminator(HiFi-Gan Multi-Period Discriminator)</h2>
<p>遵循 HiFi-GAN 中提出的多周期鉴别器的鉴别器架构。多周期鉴别器是基于多个MelGAN马尔可夫窗口的子鉴别器(其来自<a href="https://arxiv.org/abs/1611.07004">Image-to-Image Translation with Conditional Adversarial Networks</a>中的Markovian discriminator (PatchGAN)，由一系列具有大内核的跨步卷积层组成。我们利用分组卷积来允许使用更大的内核大小，同时保持较小的参数数量。)，每个子鉴别器都对输入波形的不同周期模式进行操作。</p>
<p><img src="https://github.com/user-attachments/assets/66bc011c-697d-47ab-9a56-d0bbe177341e" alt="HiFi-GAN MPD"></p>
<ul>
<li><a href="https://arxiv.org/abs/1910.06711">2019. <strong>Melgan: Generative adversarial networks for conditional waveform synthesis</strong></a> | <a href="https://github.com/descriptinc/melgan-neurips">paper code</a> | <a href="https://github.com/seungwonpark/melgan">NVIDIA/tacotron2 melgan</a> (第一个成功训练 GAN 生成原始音频的工作，无需额外的蒸馏或感知损失函数，同时仍能产生高质量的文本到语音合成模型。 结合硬件加速推理)
<img src="https://github.com/user-attachments/assets/79dcb765-2cf6-4270-b75a-83fa5d22eefa" alt="MelGAN"></li>
<li>判别器(Discriminator)(用于训练阶段)
<ul>
<li>Multi-Period Discriminator(MPD) 多周期判别器 (由于语音音频由不同周期的正弦信号组成，因此需要识别音频数据中潜在的不同周期模式)</li>
<li>Multi-Scale Discriminator(MSD) 多尺度鉴别器 (为了捕获连续模式和长期依赖性,连续评估不同级别的音频样本, 借鉴了<a href="https://arxiv.org/abs/1910.06711">2019. Melgan: Generative adversarial networks for conditional waveform synthesis</a>)</li>
<li>MPD 对原始波形的不相交样本进行操作(离散)，而 MSD 对平滑波形进行操作(连续)</li>
</ul>
</li>
</ul>
<hr>
<h1 id="ipainternational-phonetic-alphabet">IPA(International Phonetic Alphabet)</h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/International_Phonetic_Alphabet">https://en.wikipedia.org/wiki/International_Phonetic_Alphabet</a></li>
<li><a href="https://github.com/bootphon/phonemizer">https://github.com/bootphon/phonemizer</a></li>
</ul>
    </div>

    
    


    
    

    <footer class="post-footer">
      <div class="post-tags">
          <a href="https://weedge.github.io/tags/flow/">flow</a>
          <a href="https://weedge.github.io/tags/nar/">NAR</a>
          <a href="https://weedge.github.io/tags/vits/">VITS</a>
          
        </div>

      
      <nav class="post-nav">
        
        
          <a class="next" href="/post/multimoding/voices/open_voice_extra_se_and_convert/">
            <span class="next-text nav-default">论文解读 OpenVoice: Versatile Instant Voice Cloning</span>
            <span class="prev-text nav-mobile">下一篇</span>
            
            <i class="iconfont">
              <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

            </i>
          </a>
      </nav>
    </footer>
  </article>

  
  

  
  

  

  
  

  

  

  <div class="disqus-comment">
  <div class="disqus-button" id="load_disqus" onclick="load_disqus()">
    显示 Disqus 评论
  </div>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = "https://weedge.github.io/post/multimoding/voices/vits/";
    };
    function load_disqus() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'weedge';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

      $('#load_disqus').remove();
    };
  </script>
  <noscript>Please enable JavaScript to view the
    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  </div>

    

  

        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="icon-links">
  
  
    <a href="mailto:weege007@gmail.com" rel="me noopener" class="iconfont"
      title="email" >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://github.com/weedge" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://weibo.com/weedge" rel="me noopener" class="iconfont"
      title="weibo"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M385.714286 733.714286q12-19.428571 6.285714-39.428571t-25.714286-28.571429q-19.428571-8-41.714286-0.571429t-34.285714 26.285714q-12.571429 19.428571-7.428571 39.142857t24.571429 28.857143 42.571429 1.428571 35.714286-27.142857zm53.714286-69.142857q4.571429-7.428571 2-15.142857t-10-10.571429q-8-2.857143-16.285714 2.857143t-12.285714 10.571429q-9.714286 17.714286 7.428571 25.714286 8 2.857143 16.571429 2.857143t12.571429-10.571429zm99.428571 61.142857q-25.714286 58.285714-90.285714 85.714286t-128 6.857143q-61.142857-19.428571-84.285714-72.285714t3.714286-107.142857q26.857143-53.142857 86.571429-79.428571t120.285714-10.857143q63.428571 16.571429 90.571429 68.285714t1.428571 108.857143zm178.285714-91.428571q-5.142857-54.857143-50.857143-97.142857t-119.142857-62.285714-156.857143-12q-127.428571 13.142857-211.142857 80.857143t-75.714286 151.142857q5.142857 54.857143 50.857143 97.142857t119.142857 62.285714 156.857143 12q127.428571-13.142857 211.142857-80.857143t75.714286-151.142857zm176 2.285714q0 38.857143-21.142857 79.714286t-62.285714 78.285714-96.285714 67.142857-129.142857 47.428571-154.571429 17.714286-157.142857-19.142857-137.428571-53.142857-98-86.285714-37.142857-114q0-65.714286 39.714286-140t112.857143-147.428571q96.571429-96.571429 195.142857-134.857143t140.857143 4q37.142857 36.571429 11.428571 119.428571-2.285714 8-0.571429 11.428571t5.714286 4 8.285714 2.857143 7.714286-2l3.428571-1.142857q79.428571-33.714286 140.571429-33.714286t87.428571 34.857143q25.714286 36 0 101.714286-1.142857 7.428571-2.571429 11.428571t2.571429 7.142857 6.857143 4.285714 9.714286 3.428571q32.571429 10.285714 58.857143 26.857143t45.714286 46.571429 19.428571 66.571429zm-42.285714-356.571429q24 26.857143 31.142857 62t-3.714286 67.142857q-4.571429 13.142857-16.857143 19.428571t-25.428571 2.285714q-13.142857-4.571429-19.428571-16.857143t-2.285714-25.428571q11.428571-36-13.714286-63.428571t-61.142857-20q-13.714286 2.857143-25.714286-4.571429t-14.285714-21.142857q-2.857143-13.714286 4.571429-25.428571t21.142857-14.571429q34.285714-7.428571 68 3.142857t57.714286 37.428571zm103.428571-93.142857q49.714286 54.857143 64.285714 127.142857t-7.714286 138q-5.142857 15.428571-19.428571 22.857143t-29.714286 2.285714-22.857143-19.428571-2.857143-29.714286q16-46.857143 5.714286-98.285714t-45.714286-90.285714q-35.428571-39.428571-84.571429-54.571429t-98.857143-4.857143q-16 3.428571-29.714286-5.428571t-17.142857-24.857143 5.428571-29.428571 24.857143-16.857143q70.285714-14.857143 139.428571 6.571429t118.857143 76.857143z"></path>
</svg>

    </a>


<a href="https://weedge.github.io/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
   
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2013 -
    2025
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        weedge
        
      </span></span>

  
  

  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont">
        
        <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

      </i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js" integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin="anonymous"></script>












  
    <script type="text/javascript" src="/js/load-photoswipe.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  









  <script id="dsq-count-scr" src="//weedge.disqus.com/count.js" async></script>






  <script src="/js/copy-to-clipboard.js"></script>


</body>
</html>
