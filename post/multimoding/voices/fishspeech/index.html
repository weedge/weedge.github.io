<!DOCTYPE html>
<html lang="zh-cn" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>论文解读：Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis - 时间飘过</title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="weedge" />
  <meta name="description" content="相关论文 base: 基础普适研究
 hourglass transformers: 2021. Hierarchical Transformers Are More Efficient Language Models | lucidrains/simple-hierarchical-transformer vanilla layers and shortened layers use GPT AR GLM 🤞 2023.5 MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers (分别在文本，图片，语音建模)| lucidrains/MEGABYTE-pytorch  PS: 想法和自己整理的simple LM相似~
扩展阅读：2024.12 Byte Latent Transformer: Patches Scale Better Than Tokens | paper code
 audio speech: 场景研究
  ⭐️2023.10 UniAudio: An Audio Foundation Model Toward Universal Audio Generation (灵感来自 MEGABYTE，将其应用于语音模型)| paper code (代码可扩展任务进行训练, 已扩展了音乐数据)
  ⭐️2024. Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis (论文中介绍的Daul-AR, GFSQ, Firefly-GAN(FF-GAN) 对EVA-GAN改版，细节需要结合代码理解) | paper code
 " />

  <meta name="keywords" content="工作, 技术, 生活" />






<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://weedge.github.io/post/multimoding/voices/fishspeech/" />





<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css" integrity="sha256-&#43;ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media="screen" crossorigin="anonymous">




<link rel="stylesheet" href="/css/copy-to-clipboard.css">


<meta property="og:title" content="论文解读：Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis" />
<meta property="og:description" content="相关论文
base: 基础普适研究

hourglass transformers: 2021. Hierarchical Transformers Are More Efficient Language Models | lucidrains/simple-hierarchical-transformer vanilla layers and shortened layers use GPT  AR  GLM 🤞
2023.5 MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers (分别在文本，图片，语音建模)| lucidrains/MEGABYTE-pytorch

PS: 想法和自己整理的simple LM相似~
扩展阅读：2024.12 Byte Latent Transformer: Patches Scale Better Than Tokens | paper code

audio speech: 场景研究


⭐️2023.10 UniAudio: An Audio Foundation Model Toward Universal Audio Generation (灵感来自 MEGABYTE，将其应用于语音模型)| paper code  (代码可扩展任务进行训练, 已扩展了音乐数据)



⭐️2024. Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis (论文中介绍的Daul-AR, GFSQ, Firefly-GAN(FF-GAN) 对EVA-GAN改版，细节需要结合代码理解) | paper code

" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://weedge.github.io/post/multimoding/voices/fishspeech/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2025-01-19T10:26:23+08:00" />
<meta property="article:modified_time" content="2025-01-19T10:26:23+08:00" />

<meta itemprop="name" content="论文解读：Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis">
<meta itemprop="description" content="相关论文
base: 基础普适研究

hourglass transformers: 2021. Hierarchical Transformers Are More Efficient Language Models | lucidrains/simple-hierarchical-transformer vanilla layers and shortened layers use GPT  AR  GLM 🤞
2023.5 MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers (分别在文本，图片，语音建模)| lucidrains/MEGABYTE-pytorch

PS: 想法和自己整理的simple LM相似~
扩展阅读：2024.12 Byte Latent Transformer: Patches Scale Better Than Tokens | paper code

audio speech: 场景研究


⭐️2023.10 UniAudio: An Audio Foundation Model Toward Universal Audio Generation (灵感来自 MEGABYTE，将其应用于语音模型)| paper code  (代码可扩展任务进行训练, 已扩展了音乐数据)



⭐️2024. Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis (论文中介绍的Daul-AR, GFSQ, Firefly-GAN(FF-GAN) 对EVA-GAN改版，细节需要结合代码理解) | paper code

"><meta itemprop="datePublished" content="2025-01-19T10:26:23+08:00" />
<meta itemprop="dateModified" content="2025-01-19T10:26:23+08:00" />
<meta itemprop="wordCount" content="9808">
<meta itemprop="keywords" content="non-G2P,BPE,llama,AR,dual-AR,VQ-GAN,Firefly-GAN,FSQ,GFSQ,mel-spectrogram,streaming," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="论文解读：Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis"/>
<meta name="twitter:description" content="相关论文
base: 基础普适研究

hourglass transformers: 2021. Hierarchical Transformers Are More Efficient Language Models | lucidrains/simple-hierarchical-transformer vanilla layers and shortened layers use GPT  AR  GLM 🤞
2023.5 MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers (分别在文本，图片，语音建模)| lucidrains/MEGABYTE-pytorch

PS: 想法和自己整理的simple LM相似~
扩展阅读：2024.12 Byte Latent Transformer: Patches Scale Better Than Tokens | paper code

audio speech: 场景研究


⭐️2023.10 UniAudio: An Audio Foundation Model Toward Universal Audio Generation (灵感来自 MEGABYTE，将其应用于语音模型)| paper code  (代码可扩展任务进行训练, 已扩展了音乐数据)



⭐️2024. Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis (论文中介绍的Daul-AR, GFSQ, Firefly-GAN(FF-GAN) 对EVA-GAN改版，细节需要结合代码理解) | paper code

"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->



<script>
  MathJax = {
    tex: {
      inlineMath: [["$", "$"]],
    },
    displayMath: [
      ["$$", "$$"],
      ["\[\[", "\]\]"],
    ],
    svg: {
      fontCache: "global",
    },
  };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>





</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">时间飘过</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/">主页</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/post/">归档</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/tags/">标签</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/categories/">分类</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/about/">About</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/perf-book-cn/zh/" rel="noopener" target="_blank">
              《现代CPU性能分析与优化》
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.us.kg/" rel="noopener" target="_blank">
              Podcast AI
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          
        
      </li>
    

    
  </ul>
</nav>


  
    






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

  

  

  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      时间飘过
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/">主页</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/post/">归档</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/tags/">标签</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/categories/">分类</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/about/">About</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/perf-book-cn/zh/" rel="noopener" target="_blank">
              《现代CPU性能分析与优化》
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.us.kg/" rel="noopener" target="_blank">
              Podcast AI
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">论文解读：Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis</h1>
      
      <div class="post-meta">
        <time datetime="2025-01-19" class="post-time">
          2025-01-19
        </time>
        <div class="post-category">
            <a href="https://weedge.github.io/categories/%E6%8A%80%E6%9C%AF/"> 技术 </a>
            <a href="https://weedge.github.io/categories/tts/"> TTS </a>
            
          </div>
        

        
        

        
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#相关论文">相关论文</a></li>
    <li><a href="#扩展阅读">扩展阅读</a>
      <ul>
        <li><a href="#neural-vocoder"><strong>Neural</strong> Vocoder</a></li>
      </ul>
    </li>
    <li><a href="#vq-vector-quantize-矢量量化">VQ vector-quantize 矢量量化</a>
      <ul>
        <li><a href="#vq-vaes">VQ-VAEs</a></li>
        <li><a href="#vq-gans">VQ-GANs</a></li>
        <li><a href="#audio-codec-with-vq">Audio Codec with VQ</a></li>
        <li><a href="#visual-tokenizer-with-vq">Visual Tokenizer with VQ</a></li>
        <li><a href="#vector-compressionstorage-and-searchann-with-vq-for-vector-embeddings">Vector Compression(storage) and Search(ANN) with VQ for Vector embeddings</a></li>
      </ul>
    </li>
    <li><a href="#fishspeech">FishSpeech</a>
      <ul>
        <li><a href="#主要贡献">主要贡献</a></li>
        <li><a href="#fish-speech的双自回归dual-ar模型结构">Fish-Speech的双自回归（Dual-AR）模型结构</a></li>
        <li><a href="#firefly-gan">Firefly-GAN</a></li>
        <li><a href="#训练与推理">训练与推理</a></li>
        <li><a href="#dataset-数据集">Dataset 数据集</a></li>
        <li><a href="#experimental-evaluation-实验评估">Experimental Evaluation 实验评估</a></li>
      </ul>
    </li>
    <li><a href="#结论-1">结论</a></li>
    <li><a href="#附录">附录</a>
      <ul>
        <li><a href="#fréchet-inception-distance-fid">Fréchet Inception Distance (FID)</a></li>
        <li><a href="#residual-information残差信息">residual information（残差信息）</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>

    
    <div class="post-content">
      <h2 id="相关论文">相关论文</h2>
<p><strong>base</strong>: 基础普适研究</p>
<ul>
<li>hourglass transformers: <a href="https://arxiv.org/abs/2110.13711">2021. Hierarchical Transformers Are More Efficient Language Models</a> | <a href="https://github.com/lucidrains/simple-hierarchical-transformer">lucidrains/simple-hierarchical-transformer</a> vanilla layers and shortened layers use GPT  AR  GLM 🤞</li>
<li><a href="https://arxiv.org/abs/2305.07185">2023.5 <strong>MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers</strong></a> (分别在文本，图片，语音建模)| <a href="https://github.com/lucidrains/MEGABYTE-pytorch">lucidrains/MEGABYTE-pytorch</a></li>
</ul>
<p>PS: 想法和自己整理的<a href="https://github.com/ai-bot-pro/baby-llm/tree/main/simpleLM">simple LM</a>相似~</p>
<p>扩展阅读：<a href="https://arxiv.org/abs/2412.09871">2024.12 <strong>Byte Latent Transformer: Patches Scale Better Than Tokens</strong></a> | <a href="https://github.com/facebookresearch/blt">paper code</a></p>
<hr>
<p><strong>audio speech</strong>: 场景研究</p>
<ul>
<li>
<p>⭐️<a href="https://arxiv.org/abs/2310.00704">2023.10 UniAudio: <strong>An Audio Foundation Model Toward Universal Audio Generation</strong></a> (灵感来自 MEGABYTE，将其应用于语音模型)| <a href="https://github.com/yangdongchao/UniAudio">paper code</a>  (代码可扩展任务进行训练, 已扩展了音乐数据)</p>
<p><img src="https://github.com/user-attachments/assets/d0bc3097-3daa-47db-8bbb-f24f5aec800d" alt=""></p>
</li>
<li>
<p>⭐️<a href="https://arxiv.org/abs/2411.01156">2024. <strong>Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis</strong></a> (论文中介绍的Daul-AR, GFSQ, Firefly-GAN(FF-GAN) 对EVA-GAN改版，细节需要结合代码理解) | <a href="https://github.com/fishaudio/fish-speech">paper code</a></p>
</li>
</ul>
<h2 id="扩展阅读">扩展阅读</h2>
<h3 id="neural-vocoder"><strong>Neural</strong> Vocoder</h3>
<ul>
<li>
<p><a href="https://arxiv.org/abs/1609.03499">2016.9 <strong>WaveNet: A Generative Model for Raw Audio</strong></a> (首次引入了用于音频生成的自回归模型(CNN+LSTM))| <a href="https://github.com/vincentherrmann/pytorch-wavenet">pytorch code</a></p>
</li>
<li>
<p>⭐️<a href="https://arxiv.org/abs/2010.05646">2020.10 <strong>HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</strong></a> (引入了对抗性训练，在音频质量和计算效率方面树立了新标准, multi-scale discriminator (MSD) +   multi-period discriminator (MPD)) | <a href="https://github.com/jik876/hifi-gan">paper code</a></p>
<p>HiFi-Gan Generator(Decoder)</p>
<p><img src="https://github.com/user-attachments/assets/8b09b501-bfef-48eb-b823-ba8d78307c31" alt="HiFi-Gan Generator"></p>
<hr>
<p><em><strong>inference speed</strong></em>：(模型参数小，适合实时在线场景下的研究，韩国和日本比较关注(专注)这个方面)</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2306.00814">2023.6 <strong>Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis</strong></a> | <a href="https://github.com/gemelo-ai/vocos">paper code</a> | 推理速度快：运行速度比 HiFi-GAN 快约 13 倍，比 BigVGAN 快近 70 倍。在没有 GPU 加速的情况下运行时，这种速度优势尤其明显。这主要是由于使用了短时傅里叶逆变换（ISTFT）算法而不是转置卷积。还评估了 Vocos 的一个变体，它利用 ResBlock 的扩张卷积而不是 ConvNeXt 块。在 GPU 上执行时，深度可分离卷积可提供额外的加速。</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/10389765">2023.12 <strong>WaveNeXt: ConvNeXt-Based Fast Neural Vocoder Without ISTFT layer</strong></a> | <a href="https://ast-astrec.nict.go.jp/demo_samples/asru_2023_okamoto/index.html">demo samples</a> | paper code基于 <a href="https://arxiv.org/abs/2110.07840">ESPNet2-TTS</a>  | 一种新型的基于ConvNeXt的快速神经声码器WaveNeXt，它通过替换Vocos中的逆短时傅里叶变换（iSTFT）层为可训练的线性层，直接预测语音波形样本，而不依赖于STFT频谱。这一改进不仅保持了Vocos的快速推理速度，还提高了语音合成的质量。文章还探讨了如何将WaveNeXt与基于JETS的端到端文本到语音（E2E TTS）框架集成，并研究了采样频率为48kHz的全带模型（Full-band Model：能够处理和生成覆盖整个音频频谱范围的模型，通常是指能够处理从最低频到最高频的完整音频信号的模型）。实验结果表明，WaveNeXt在分析-合成和E2E TTS条件下均优于Vocos，同时保持了快速推理的能力。</p>
<p><img src="https://github.com/user-attachments/assets/4a1eeaff-528f-4706-b5fc-210caea2c13b" alt="image"></p>
<hr>
<p><em><strong>Scaling</strong></em>：（模型参数大，适合线上、离线场景下高质量音频生成的研究，国内和老美喜欢研究这个方向，探索模型边界，泛化能力）</p>
</li>
<li>
<p>NVIDIA <a href="https://arxiv.org/abs/2206.04658">2023. BigVGAN: A Universal Neural Vocoder with Large-Scale Training</a> (scaling of datasets and models) 112M params and  LibriTTS dataset(low-fidelity 24kHz)，high-fidelity out-of-distribution (OOD) generation without fine-tuning | <a href="https://github.com/NVIDIA/BigVGAN">paper code</a></p>
</li>
<li>
<p>⭐️NVIDIA <a href="https://arxiv.org/abs/2402.00892">2024.1 <strong>EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks</strong></a>  (BigVGAN more model and data scaling) ~200M params and 36,000-hour HiFi (44.1kHz) dataset <strong>for music and singing generation</strong>.</p>
<p>EVA-GAN as an advancement over HiFi-GAN，characterized by a larger context window, an improved structure, increased capacity, and an expanded dataset.</p>
<ul>
<li>Dataset Scaling: base (LJSpeech + LibriTTS + VCTK + M4Singer)   + <a href="https://huggingface.co/datasets/fish-audio-private/hifi-16kh"><strong>fish-audio-private/hifi-16kh</strong></a> + <a href="https://huggingface.co/datasets/fish-audio-private/playerfm-20000h"><strong>fish-audio-private/playerfm-20000h</strong></a> scaling 36,000 hour dataset</li>
<li>Model Scaling: 将 EVA-GAN base模型中Generator参数从 16.3 M参数扩大到 EVA-GAN-big 174.4 M, 提高了鲁棒性和整体性能.</li>
</ul>
<p><img src="https://github.com/user-attachments/assets/eded57e5-67a3-45c0-8660-e97500ef2b3c" alt="image"></p>
</li>
</ul>
<h2 id="vq-vector-quantize-矢量量化">VQ vector-quantize 矢量量化</h2>
<p>VQ 让你脑洞大开~</p>
<p>常与 VAE, GAN 框架；Codec(tokenizer) 结合。以及Vector Compression storage and ANN search for Vector embeddings。</p>
<p><a href="https://github.com/lucidrains/vector-quantize-pytorch">https://github.com/lucidrains/vector-quantize-pytorch</a></p>
<hr>
<h3 id="vq-vaes">VQ-VAEs</h3>
<ul>
<li>
<p>VQ-VAE 来自 <a href="https://arxiv.org/abs/1711.00937">2017.11 <strong>Neural Discrete Representation Learning</strong> 值得细品</a> | <a href="https://github.com/google-deepmind/sonnet/blob/v1/sonnet/python/modules/nets/vqvae.py">paper code</a>  (将 VAE 与VQ相结合以获得离散的潜在表示(discrete latent representation/variables)，论文中将VQ-VAE应用于图像,音频,视频 无监督生成)</p>
<p>PS：语言本质上是离散的，类似地，语音通常表示为符号序列，图像通常可以用语言简洁地描述。此外，离散表示非常适合复杂的推理、规划和预测学习（例如，如果下雨，我会使用雨伞）。虽然在深度学习中使用离散潜在变量已被证明具有挑战性，但可以用强大的自回归模型来对离散变量的分布进行建模：<a href="https://arxiv.org/abs/1609.03499">2016.9 <strong>WaveNet: A Generative Model for Raw Audio</strong></a> (首先引入了用于音频生成的自回归模型) （谷歌大法，法力无边~）; 后续进行了扩展研究 VQ-VAE2 来自 <a href="https://arxiv.org/abs/1906.00446">2019.6 Generating Diverse High-Fidelity Images with VQ-VAE-2</a> (引入多尺度分层VQ-VAE来增强局部特征)</p>
</li>
<li>
<p>RQ-VAE 来自 <a href="https://arxiv.org/abs/2203.01941">2022.5 <strong>Autoregressive Image Generation using Residual Quantization</strong></a> (使用Residual-VQ 构造VAE)| <a href="https://github.com/kakaobrain/rq-vae-transformer">paper code</a></p>
</li>
<li>
<p>Latent Quantization 来自 <a href="https://arxiv.org/abs/2305.18378">2023.5 Disentanglement via Latent Quantization</a> | <a href="https://github.com/kylehkhsu/latent_quantization/blob/main/scripts/train_ae.py">paper code</a></p>
</li>
<li>
<p>⭐️FSQ: <a href="https://arxiv.org/abs/2309.15505">2023.9 <strong>Finite Scalar Quantization: VQ-VAE Made Simple</strong></a> | <a href="https://github.com/google-research/google-research/tree/master/fsq">paper code</a> |  <a href="https://github.com/lucidrains/vector-quantize-pytorch?#finite-scalar-quantization">lucidrains/vector-quantize-pytorch?#finite-scalar-quantization</a> （在大大简化生成建模的矢量量化方式，消除承诺损失、码本 EMA 更新的需要，并解决码本崩溃或利用率不足的问题。将每个标量舍入为具有直通梯度的离散级别；code成为超立方体中的均匀点。）</p>
</li>
<li>
<p>VQ-VAE with Rotation Trick: <a href="https://arxiv.org/abs/2410.06424">2024.10 Restructuring Vector Quantization with the Rotation Trick</a> (通过 VQ 层对梯度进行变换，以便输入向量和量化输出之间的相对角度和大小被编码到梯度中)| <a href="https://github.com/cfifty/rotation_trick">paper code</a></p>
</li>
</ul>
<hr>
<h3 id="vq-gans">VQ-GANs</h3>
<ul>
<li>VQ-GAN 来自 <a href="https://arxiv.org/abs/2012.09841">2021. <strong>Taming Transformers for High-Resolution Image Synthesis</strong></a> | <a href="https://github.com/CompVis/taming-transformers">paper code</a></li>
<li>Improved VQ-GAN 来自 <a href="https://arxiv.org/abs/2110.04627">2022. <strong>Vector-quantized Image Modeling with Improved VQGAN</strong></a></li>
<li>Latent Quantization 来自 <a href="https://arxiv.org/abs/2305.18378">2023.5 Disentanglement via Latent Quantization</a> | <a href="https://github.com/kylehkhsu/latent_quantization/blob/main/scripts/train_gan.py">paper code</a></li>
<li>RVQ-GAN 来自 <a href="https://arxiv.org/abs/2306.06546">2023.6 High-Fidelity Audio Compression with Improved RVQGAN</a> |<a href="https://github.com/descriptinc/descript-audio-codec">paper code</a></li>
<li>SimVQ 来自 <a href="https://arxiv.org/abs/2411.02038">2024.11 <strong>Addressing Representation Collapse in Vector Quantized Models with One Linear Layer</strong></a> | <a href="https://github.com/youngsheen/SimVQ">paper code</a> |  <a href="https://github.com/lucidrains/vector-quantize-pytorch?#sim-vq">lucidrains/vector-quantize-pytorch?#sim-vq</a> (作者发现结合Rotation Trick 并将线性投影扩展为小型单层 MLP 时，有更好的效果)</li>
</ul>
<hr>
<h3 id="audio-codec-with-vq">Audio Codec with VQ</h3>
<ul>
<li>
<p>Residual-VQ (RVQ) 来自 <a href="https://arxiv.org/abs/2107.03312">2021. <strong>SoundStream: An End-to-End Neural Audio Codec</strong></a> (可在端侧单个 CPU 线程流式实时运行，加上端侧自研芯片(FPGA,ARM架构)运行情况呢？) | <a href="https://github.com/lucidrains/audiolm-pytorch/blob/main/audiolm_pytorch/soundstream.py">audiolm_pytorch/soundstream</a></p>
</li>
<li>
<p>GroupedResidualVQ 来自 <a href="https://arxiv.org/abs/2305.02765">2023. HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec</a>（对特征维度组进行残差 VQ，显示出与 Meta 的 <a href="https://arxiv.org/abs/2210.13438">Encodec模型</a>(使用RVQ) 等效的结果，同时使用更少的码本）| <a href="https://github.com/yangdongchao/AcademiCodec">paper code</a>  (fishspeech 也使用了类似组的方法，运用在FSQ中)</p>
</li>
<li>
<p>RVQ-GAN 来自 <a href="https://arxiv.org/abs/2306.06546">2023.6 High-Fidelity Audio Compression with Improved RVQGAN</a> |<a href="https://github.com/descriptinc/descript-audio-codec">paper code</a></p>
</li>
<li>
<p>SimVQ 来自 <a href="https://arxiv.org/abs/2411.02038">2024.11.4 <strong>Addressing Representation Collapse in Vector Quantized Models with One Linear Layer</strong></a> | <a href="https://github.com/youngsheen/SimVQ">paper code</a> | <a href="https://github.com/lucidrains/vector-quantize-pytorch?#sim-vq">lucidrains/vector-quantize-pytorch?#sim-vq</a> (作者发现结合Rotation Trick 并将线性投影扩展为小型单层 MLP 时，有更好的效果)</p>
<ul>
<li>
<p><strong>表示坍缩问题</strong>：在VQ模型中，随着代码本（codebook）大小的增加，代码本的利用率降低，导致模型性能无法提升，这就是表示坍缩问题。这个问题限制了VQ模型在大规模训练中的可扩展性。</p>
</li>
<li>
<p>现有的旨在减轻表示坍缩的方法通常会以牺牲模型容量为代价来降低潜在空间的维度，但这并不能完全解决核心问题，比如FSQ。</p>
</li>
<li>
<p>在论文研究中，对 VQ 模型中的表示坍缩问题进行了理论分析，并将其主要原因确定为码本的不相交优化，其中只有一小部分码向量通过梯度下降进行更新。为了解决这个问题，提出了<strong>SimVQ</strong> ，这是一种通过基于可学习潜在基础的线性变换层重新参数化代码向量的新颖方法。这种变换优化了码本跨越的<em>整个线性空间</em>，而不是仅仅更新普通 VQ 模型中最近邻搜索所选择的<em>码向量</em>。</p>
</li>
<li>
<p>SimVQ的一个关键优势在于，<strong>它在代码本大小扩展时能够持续提升性能，而传统的VQ模型在这种情况下往往会遇到性能瓶颈</strong>。这<strong>使得SimVQ能够更好地适应大规模训练的需求，例如在大型语言模型中作为多模态分词器</strong>。(CosyVoice2中的speech tokenizer用的是FSQ, 应该可以替换成最新的SimVQ来按照论文中提到的语音数据LibriTTS-580h 测试验证下codebook size scaling)</p>
<p><img src="https://github.com/user-attachments/assets/6f02f852-3ff1-4f50-b48d-25e7257e8872" alt=""></p>
</li>
</ul>
</li>
<li>
<p>Residual FSQ from <a href="https://arxiv.org/abs/2411.19842">2024.11.29 Scaling Transformers for Low-Bitrate High-Quality Speech Coding</a> | <a href="https://github.com/Stability-AI/stable-codec">paper code</a></p>
</li>
</ul>
<hr>
<h3 id="visual-tokenizer-with-vq">Visual Tokenizer with VQ</h3>
<ul>
<li>3D-VQ from MAGVIT ( VQ-VAE 框架):  <a href="https://arxiv.org/abs/2212.05199">2023. MAGVIT: Masked Generative Video Transformer</a> |<a href="https://github.com/google-research/magvit">paper code</a></li>
<li><strong>Lookup-Free Quantization (LFQ)</strong> from MAGVIT2 ( VQ-VAE 框架): <a href="https://arxiv.org/abs/2310.05737">2024.3 <strong>Language Model Beats Diffusion - Tokenizer is Key to Visual Generation</strong></a> (LM(discrete latent) vs DDM(continuous latent)  很好的一篇论文) | <a href="https://github.com/lucidrains/magvit2-pytorch">magvit2-pytorch code</a>
<ul>
<li>一种新的视频分词器，在三个方面优于之前性能最佳的视频分词器：视觉生成、视频压缩和动作识别。</li>
<li>一种新颖的免查找量化方法，可以通过学习大量词汇来提高语言模型的视觉生成质量。</li>
<li>论文中的实验表明，当提供相同的训练数据、等效的模型大小和相似的训练预算时，语言模型可以优于 ImageNet 上的扩散模型。</li>
<li>根据用户研究，视频压缩器在相似的比特率下比 HEVC(High Efficiency Video Coding H.265) 和下一代video codec VVC (Versatile Video Coding H.266)具有更好的质量（人工评估）。专为视频生成而设计的视觉分词器的第一次成功尝试，以达到与标准编解码器相当的结果。</li>
</ul>
</li>
<li>BSQ (Binary Spherical Quantization) from <a href="https://arxiv.org/abs/2406.07548">2024.6 Image and Video Tokenization with Binary Spherical Quantization</a> (BSQ vs  LFQ BSQ 在量化误差、训练效率、熵计算复杂度以及实验结果等方面均优于 LFQ，尤其是在高维度和高分辨率数据上表现更为突出)|<a href="https://github.com/zhaoyue-zephyrus/bsq-vit">paper code</a></li>
<li><a href="https://arxiv.org/abs/2409.04410">2024.9 Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation</a> (可以练练丹，看看效果)| <a href="https://github.com/TencentARC/Open-MAGVIT2">paper code</a> Open-MAGVIT2(SEED-Voken 后面又引入 IBQ 后变成了 Visual Tokenizer) from magvit2-pytorch</li>
<li><strong>IBQ</strong> (Index Backpropagation Quantization) from <a href="https://arxiv.org/abs/2412.02692">2024.12 Taming Scalable Visual Tokenizer for Autoregressive Image Generation</a> (解决了现有 VQ 方法在扩展性上的瓶颈问题，实现了高利用率的大规模视觉标记化，并在图像重建和生成任务中取得了优异的性能。通过全局更新策略，IBQ在整个训练过程中保持了较高的码本利用率（约96%）；IBQ首次成功训练了大规模码本（2¹⁸，即262,144个码本大小）和高维度（256维）的视觉分词器；ImageNet 上的实验表明，IBQ 能够实现高利用率、大规模的视觉分词器，并在重建(还原修复)（1.00 rFID）和生成(创新)（2.05 gFID）方面提高性能)| <a href="https://github.com/TencentARC/SEED-Voken">paper code</a>  (scaling codebook size, code dimension and model size)</li>
</ul>
<hr>
<h3 id="vector-compressionstorage-and-searchann-with-vq-for-vector-embeddings">Vector Compression(storage) and Search(ANN) with VQ for Vector embeddings</h3>
<ul>
<li>QINCo：<a href="https://arxiv.org/abs/2401.14732">2024. 5 Residual Quantization with Implicit Neural Codebooks</a> | <a href="https://github.com/facebookresearch/Qinco/tree/main/qinco_v1">paper code</a></li>
<li>QINCo2：<a href="https://arxiv.org/abs/2501.03078">2025.1 <strong>QINCo2: Vector Compression and Search with Improved Implicit Neural Codebooks</strong></a> | <a href="https://arxiv.org/abs/2501.03078">paper code</a></li>
</ul>
<h2 id="fishspeech">FishSpeech</h2>
<h3 id="主要贡献">主要贡献</h3>
<ul>
<li>利用LLMs和双AR 结构来取代传统的G2P （文字到音素的转换(Grapheme-to-Phoneme)）转换，提供强大且可扩展的多语言语音合成;</li>
<li>FFGAN vocoder 集成了多种矢量量化技术（Grouped Finite Scalar Vector Quantization, GFSQ），以优化压缩比和码本利用率来实现高保真语音合成;</li>
<li>优化推理，在消费级 NVIDIA RTX 4060 移动平台上实现了约 1:5 的实时因子，在高性能 NVIDIA RTX 4090 配置上实现了 1:15 的实时因子。延迟为 150 毫秒，远低于其他使用 DiT 和 Flow 结构的 TTS 系统。</li>
</ul>
<h3 id="fish-speech的双自回归dual-ar模型结构">Fish-Speech的双自回归（Dual-AR）模型结构</h3>
<p>Dual-AR 模型结构 提高了序列生成过程中码本处理的稳定性和计算效率，特别是在使用分组有限标量矢量量化 (GFSQ) 时，由两个sequential AR transformer(GLM)组成: <strong>Slow Transformer</strong> 和 <strong>Fast Transformer</strong>  (分别对应UniAudio的Global GPT 和 local GPT) ；Transformer 采用 Llama模型结构。</p>
<p><img src="https://github.com/user-attachments/assets/65172ed8-6e10-4c63-ac1c-0813b6493549" alt="image"></p>
<h4 id="slow-transformer"><strong>Slow Transformer</strong></h4>
<p>Slow Transformer 在更高的抽象级别上运行，<strong>处理输入文本嵌入(Text Embeddings 和 Codebook Embeddings)以编码全局语言结构和语义内容</strong>。该模块<strong>负责生成中间隐藏状态并高精度预测语义token</strong>。</p>
<p>假设输入的toen序列为$x = [x₁, x₂, &hellip;, xₜ]$，Slow Transformer 生成隐藏状态 $h ∈ ℝ^(T×D)$ 和token对数 $z$，通过以下变换实现：
$$
h = SlowTransformer(x)
$$</p>
<p>$$
z = Wtok · Norm(h)
$$</p>
<p>其中，$Norm(·)$表示层归一化，$W_{tok}$ 是标记预测层的可学习参数。</p>
<h4 id="fast-transformer">Fast Transformer</h4>
<p>Fast Transformer通过Codebook Embedding处理来细化Slow Transformer的输出，<strong>捕捉自然语音所需的详细声学特征(acoustic features)</strong>。<strong>处理残差信息并优化Codebook的使用</strong>。</p>
<p>Fast Transformer的输入是隐藏状态 $h$ 和codebook embedding $c$ 的拼接序列，具体如下：
$$
h̃ = [h; c]，(h^{fast})
$$</p>
<p>$$
h^{fast} = FastTransformer(ĥ, (h^{fast}))
$$</p>
<p>$$
y = W_{cbk} · Norm(h^{fast})
$$</p>
<p>其中，$[h; c]$ 表示 $h$ 和 $c$ 的拼接操作，$W_{cbk}$ 是codebook预测层的可学习参数，$y$ 是最终的码本对数。</p>
<h4 id="dual-ar结构的优势">Dual AR结构的优势</h4>
<ul>
<li><strong>增强的序列生成稳定性</strong>：通过分层处理全局和局部信息，显著提高了GFSQ在序列生成任务中的稳定性。</li>
<li><strong>优化的码本处理</strong>：快速变换器实现了一种高效的码本嵌入处理机制，在不增加显著计算开销的情况下提升了性能，特别是对于规模为7B或更大的模型。</li>
<li><strong>高保真语音合成质量</strong>：Slow和Fast Transformer之间的协同作用使得系统能够以高保真度合成语音，并能够处理复杂的语言现象。</li>
<li><strong>多语言处理能力</strong>：通过大语言模型（LLM）生成语言特征，消除了传统文字到音素转换的依赖，从而简化了合成流程并增强了多语言能力。通过混合文本数据，理解能力将进一步提升。</li>
</ul>
<h3 id="firefly-gan">Firefly-GAN</h3>
<p>Firefly-GAN（FF-GAN）是 <a href="https://arxiv.org/abs/2402.00892">2024.1 <strong>EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks</strong></a>  的一个显著改进版本。它用更高效的设计替换了HiFi-GAN中的传统卷积组件，引入了并行块（ParallelBlock）以替代多感受野（Multi-Receptive Field, MRF）模块。通过引入分组有限标量矢量量化（Grouped Finite Scalar Vector Quantization GFSQ），<strong>FF-GAN在序列生成稳定性以及语言变化处理方面表现出色，尤其适用于多语言合成的AI应用</strong>。</p>
<p><img src="https://github.com/user-attachments/assets/f5f8cd7f-2463-4c1e-a8cd-af77b48fe9fa" alt="image"></p>
<h4 id="firefly-generator">Firefly Generator</h4>
<p>FF-GAN采用了增强的卷积结构，包括<strong>深度可分离卷积(depth-wise separable convolution)</strong> <a href="https://arxiv.org/abs/1704.04861">2017.4 Mobilenets: Efficient convolutional neural networks for mobile vision applications</a> 和 <strong>扩张卷积(dilated convolution)</strong> <a href="https://arxiv.org/abs/1511.07122">2015.11 Multi-Scale Context Aggregation by Dilated Convolutions</a>，以替代传统的Conv1d层。这种架构改进<strong>增强了模型捕捉和合成复杂音频特征的能力</strong>。</p>
<p><strong>并行块</strong>（ParallelBlock 来自 <a href="https://arxiv.org/abs/2402.00892">2024.1 EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks</a> ）<strong>替代传统的多感受野（MRF）模块</strong>，进行改进：</p>
<ul>
<li>优化了“typo-codebook(错误码本)”输入的处理效率。</li>
<li>实现了可配置的卷积核大小和扩张率，并采用 stack-and-average 机制处理来自三个残差块（ResBlocks）的输出，而不是直接进行加法操作。</li>
<li>提供了增强的感受野（Multi-Receptive Field, MRF）覆盖、优越的特征提取能力和更好的可配置性，从而提升了音频合成的质量。</li>
</ul>
<h4 id="grouped-finite-scalar-vector-quantization-gfsq">Grouped Finite Scalar Vector Quantization (GFSQ)</h4>
<p>为了适应“typo-codebook”任务，引入了分组有限标量矢量量化（ Grouped Finite Scalar Vector Quantization GFSQ）。</p>
<p>以下是GFSQ的详细开发过程：</p>
<p>假设输入张量为 $z ∈ ℝ^{B×C×L}$，整个过程包括以下步骤：</p>
<h5 id="下采样downsampling">下采样（Downsampling）</h5>
<p>使用下采样函数 $f_{down}$ 对输入张量 $z$ 进行下采样，得到下采样后的张量 $z_d ∈ ℝ^{B×C_d×L_d}$：
$$
z_d = f_{down}(z)
$$</p>
<h5 id="gfsq过程">GFSQ过程</h5>
<ul>
<li>
<p><strong>特征分组(Feature Grouping)</strong> 输入特征矩阵 $Z$ 被划分为$G$ 组：
$$
Z = [Z^{(1)}, Z^{(2)}, &hellip;, Z^{(G)}]
$$</p>
</li>
<li>
<p><strong>标量量化(Scalar Quantization)</strong> 对于每个标量 $z_{b,c,l}^{(g)}$：
$$
ẑ_{b,c,l}^{(g)} = Q(z_{b,c,l}^{(g)})
$$</p>
</li>
<li>
<p><strong>索引生成(Index Generation)</strong> 每个标量映射到索引 $k_{b,c,l}^{(g)}$</p>
</li>
<li>
<p><strong>解码(Decoding)</strong>
$$
ẑ_{b,c,l}^{(g)} = Codebook^{(g)}[k_{b,c,l}^{(g)}]
$$</p>
</li>
</ul>
<h5 id="重构量化下采样张量reconstruct-the-quantized-downsampled-tensor">重构量化下采样张量（Reconstruct the Quantized Downsampled Tensor）</h5>
<p>将所有组的量化向量沿通道维度拼接，得到量化下采样张量 $z_{qd} ∈ ℝ^{(B×C_d×L_d)}$：
$$
z_{qd}(b, :, l) = [ẑ_{q_d}^{(1)}(b, :, l); ẑ_{q_d}^{(2)}(b, :, l); &hellip;; ẑ_{q_d}^{(G)}(b, :, l)]
$$</p>
<h5 id="上采样upsampling">上采样（Upsampling）</h5>
<p>使用上采样函数 $f_{up}$ 将量化下采样张量恢复到其原始大小，得到最终的量化张量 $z_q ∈ ℝ^{B×C×L}$：
$$
z_q = f_{up}(z_{q_d})
$$
目标是使 $z_q$ 尽可能接近原始输入 $z$：
$$
z_q ≈ z
$$</p>
<h4 id="结论">结论</h4>
<p>GFSQ技术实现了接近100%的码本利用率，并在内部消融实验中，相比于其他量化技术（如RFSQ、RVQ和GRFSQ）获得了更好的客观和主观评分。FF-GAN显著增强了“typo-codebook”操作的稳定性，并确保在多情感和多语言任务中保留全面的中间变量信息。</p>
<h3 id="训练与推理">训练与推理</h3>
<p><img src="https://github.com/user-attachments/assets/a3302fff-211f-467e-a53f-a0916984ae0c" alt="image"></p>
<h4 id="training-训练"><strong>Training</strong> 训练</h4>
<p>Fish-Speech采用了一个三阶段的训练方法：</p>
<ul>
<li>首先使用<strong>大规模的标准数据进行预训练(PT)</strong>，</li>
<li>然后通过<strong>小批量的高质量数据进行有监督微调（SFT）</strong>，</li>
<li>最后<strong>使用手动标记的正负样本对 进行DPO（Discriminator Pessimistic Optimization）训练</strong>。</li>
</ul>
<p>训练基础设施分为两个部分（见上图）：</p>
<ul>
<li>自回归（AR）训练使用<strong>8块H100 GPU（80G显存）</strong>，<strong>持续一周</strong>。(Training Stage2)</li>
<li>声码器（vocoder）训练则使用<strong>8块RTX 4090 GPU</strong>，再<strong>持续一周</strong>。(Training Stage1)</li>
</ul>
<blockquote>
<p>需要注意的是，这些训练时间不包括DPO阶段。</p>
</blockquote>
<h4 id="inference-推理">Inference 推理</h4>
<p>训练后推理过程见上图Inference。通过采用fish-tech技术（包括KV缓存⭐️<a href="2211.05102">2023. <strong>Efficiently Scaling Transformer Inference</strong></a>、PyTorch编译等加速方法），系统在<strong>消费级NVIDIA RTX 4060移动平台上实现了大约1:5的实时因子（real-time factors RTF），在高性能NVIDIA RTX 4090配置上实现了1:15的实时因子</strong>。这些架构优化显著降低了推理延迟，<strong>实现了150毫秒的首包延迟(first-packet latency)</strong>。</p>
<p>此外，该系统可以<strong>流式处理信息</strong>，便于与现代AI工具结合，并在不同场景中使用。</p>
<h3 id="dataset-数据集">Dataset 数据集</h3>
<p>训练数据包括来自公共数据源和 数据收集过程的大量语音样本。数据集包含约<strong>72万（720k）小时的多语言语音数据</strong>，其中<strong>英语和普通话是主要组成部分，各占30万小时</strong>。还包含了其他语系的语音数据，<strong>每种语系各占2万小时，包括德语（日耳曼语系）、法语和意大利语（罗曼语系）、日语和韩语（东亚语系）以及阿拉伯语（闪米特语系）</strong>。</p>
<p>仔细平衡了不同语言的数据，以帮助模型同时学习多种语言。这种方法有助于模型在生成混合语言内容时表现良好。数据集的庞大规模和多样性显著提升了模型处理多语言的自然性。</p>
<p>PS: scaling 数据集 到 72万小时；而 后续 CosyVoice2公布的训练数据集时 20万训练speech tokenizer, 16.68万小时训练cosyvoice2 LM</p>
<h3 id="experimental-evaluation-实验评估">Experimental Evaluation 实验评估</h3>
<p>为了评估论文中的模型相对于基线模型的效果，进行了说话人克隆任务的实验。</p>
<p>为了实验验证，将分析限制在单语言语音克隆场景，不包括跨语言合成的任务。<strong>评估语料库由10个不同的说话者（包括不同的语言）身份组成，其中30个是综合的每个说话者的话语，产生300个样本的综合评估集</strong>。应该指出的是，<strong>跨语言综合是不包括在本次评估中</strong>。</p>
<p>评估方法包括客观和主观指标：</p>
<ul>
<li>词错误率（WER）用于评估语音的可理解性，<strong>使用OpenAI Whisper-medium ASR模型进行转录评价的结果</strong>。</li>
<li>说话人嵌入相似度（speaker embedding similarity ）用于评估语音克隆的保真度，工具包：
<ul>
<li>Resemblyzer： <a href="https://github.com/resemble-ai/Resemblyzer">https://github.com/resemble-ai/Resemblyzer</a></li>
<li><strong>Speechbrain</strong>： <a href="https://github.com/speechbrain/speechbrain">https://github.com/speechbrain/speechbrain</a> | <a href="https://arxiv.org/abs/2407.00463">2024.6 Open-Source Conversational AI with SpeechBrain 1.0</a></li>
</ul>
</li>
<li>以及平均意见得分（mean opinion score MOS）用于量化感知质量，<strong>人工评估</strong>。</li>
</ul>
<p>这一评估框架旨在评估模型在保持说话人身份的同时，维持高保真语音合成的能力。</p>
<h4 id="word-error-rate-analysis-词错误率分析">Word Error Rate Analysis 词错误率分析</h4>
<table>
<thead>
<tr>
<th>模型名称</th>
<th>WER (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>真实录音</td>
<td><strong>9.22</strong></td>
</tr>
<tr>
<td>Fish-Speech</td>
<td><strong>6.89</strong></td>
</tr>
<tr>
<td>Reecho1</td>
<td>11.92</td>
</tr>
<tr>
<td>F5-TTS</td>
<td>13.98</td>
</tr>
<tr>
<td>CosyVoice</td>
<td>22.20</td>
</tr>
</tbody>
</table>
<p>表1：语音克隆任务的词错误率（WER）结果</p>
<p>表1的分析显示，Fish-Speech 模型在语音克隆任务中实现了6.89%的词错误率，这不仅显著低于基线模型，甚至优于真实录音（9.22%）。这一性能为Fish-Speech模型在语音克隆场景中的能力提供了有力证据。该模型与其他竞争模型（WER范围从11.92%到22.20%）之间的差距突显了Fish-Speech模型方法在合成稳定性和内容保真度方面的改进。</p>
<h4 id="speaker-similarity-analysis-说话人相似度分析">Speaker Similarity Analysis 说话人相似度分析</h4>
<table>
<thead>
<tr>
<th>模型名称</th>
<th>Resemblyzer</th>
<th>SpeechBrain</th>
</tr>
</thead>
<tbody>
<tr>
<td>真实录音</td>
<td><strong>0.921</strong></td>
<td><strong>0.770</strong></td>
</tr>
<tr>
<td>CosyVoice</td>
<td>0.936</td>
<td>0.813</td>
</tr>
<tr>
<td>Fish-Speech</td>
<td><strong>0.914</strong></td>
<td><strong>0.762</strong></td>
</tr>
<tr>
<td>F5-TTS</td>
<td>0.905</td>
<td>0.787</td>
</tr>
<tr>
<td>Reecho</td>
<td>0.887</td>
<td>0.636</td>
</tr>
</tbody>
</table>
<p>表2：不同模型的说话人相似度评分，包括真实录音</p>
<p>表2展示了“typo-codebook”策略对说话人相似度指标的影响。Fish-Speech模型在Resemblyzer和SpeechBrain上的相似度评分分别为0.914和0.762，与真实录音的表现（0.921和0.770）非常接近。在Resemblyzer评估中，与真实录音的差距仅为0.76%，在SpeechBrain评估中为1.04%。这表明<strong>Fish-Speech模型在捕捉自然语音特征方面具有卓越的能力</strong>。结果强烈表明，“typo-codebook”架构<strong>能够更全面地捕捉声学状态，从而提高合成语音的音色保真度</strong>。Fish-Speech模型方法显著优于基线模型，例如F5-TTS（0.905和0.787）和Reecho（0.887和0.636）。在<strong>两个评估框架中的一致表现证明了Fish-Speech模型方法在保留说话人特征方面的有效性</strong>，这对于高质量的文本到语音合成和代理任务至关重要。</p>
<h4 id="perceptual-quality-assessment感知质量评估"><strong>Perceptual Quality Assessment</strong>感知质量评估</h4>
<table>
<thead>
<tr>
<th>模型名称</th>
<th>MOS</th>
</tr>
</thead>
<tbody>
<tr>
<td>真实录音</td>
<td>5.00</td>
</tr>
<tr>
<td>Fish-Speech</td>
<td>4.05</td>
</tr>
<tr>
<td>CosyVoice</td>
<td>3.80</td>
</tr>
<tr>
<td>F5-TTS</td>
<td>2.90</td>
</tr>
<tr>
<td>Reecho</td>
<td>3.76</td>
</tr>
</tbody>
</table>
<p>表3：克隆语音质量的五级平均意见得分（MOS）评分</p>
<p>为了评估合成音频的感知质量，进行了全面的平均意见得分（MOS）听测实验，参与者为没有音频处理经验的普通听众。<strong>评估遵循双盲、随机化方法</strong>，以确保评估的公正性。结果显示，Fish-Speech在主观评分上显著高于其他基线模型（p &lt; 0.05），在语音自然度和说话人相似度方面表现出色。这一人类感知指标的评估强烈表明，<strong>Fish-Speech能够更好地捕捉和再现人类语音的自然特征，尤其是在语音克隆任务的背景下</strong>。</p>
<h2 id="结论-1">结论</h2>
<p>Fish-Speech论文研究在文本到语音（TTS）领域取得了显著进展，通过引入一种新型的多语言和多情感稳定化解决方案，为未来的人工智能应用提供了更自然、更高质量的语音合成技术。核心创新在于开发了一种结合了双自回归（Dual-AR）生成结构的“typo-codebook”声码器。这种结构组合在合成过程中表现出稳定性，同时保留了生成语音中的声学特征。</p>
<p>此外，Fish-Speech论文中采用了非文字到音素（non-G2P）结构，有效解决了传统基于音素系统固有的局限性，并为跨语言和情感多样化的TTS应用提供了坚实的基础，特别是在人工智能代理交互的背景下。</p>
<hr>
<h2 id="附录">附录</h2>
<h3 id="fréchet-inception-distance-fid">Fréchet Inception Distance (FID)</h3>
<p>FID 是 <strong>Fréchet Inception Distance</strong> 的缩写，是一种用于评估生成模型（如 GAN 和扩散模型）生成图像质量的指标。它通过比较生成图像和真实图像在特征空间中的分布差异来量化两者的相似度。具体来说，FID 的计算过程如下：</p>
<ol>
<li><strong>特征提取</strong>：使用预训练的 Inception v3 网络提取真实图像和生成图像的特征向量。</li>
<li><strong>统计分布</strong>：计算这些特征向量的均值和协方差矩阵。</li>
<li><strong>距离计算</strong>：使用 Fréchet 距离公式计算两个分布之间的距离。</li>
</ol>
<p>FID 的公式为：
$$
FID=∥μ_r−μ_g∥^2+Tr(Σ_r+Σ_g−2(Σ_rΣ_g)^{1/2})
$$
其中，μr 和 μg 分别是真实图像和生成图像的特征均值，$Σr$ 和 $Σg$ 是它们的协方差矩阵。</p>
<p><strong>FID 的物理含义</strong>：</p>
<ul>
<li>FID 越小，表示生成图像与真实图像越相似。</li>
<li>它同时衡量了生成图像的质量和多样性。</li>
</ul>
<p>FID 是目前广泛使用的生成模型评估指标之一，因为它能够更准确地反映生成图像与真实图像之间的分布差异。</p>
<hr>
<h3 id="residual-information残差信息">residual information（残差信息）</h3>
<p>在深度学习和语音合成的上下文中，**residual information（残差信息）**是一个非常重要的概念，尤其是在处理复杂的序列生成任务时。它通常用于描述模型中未被完全捕捉或处理的信息，这些信息可能包含重要的细节或未被充分利用的特征。以下是对残差信息的详细解释及其在语音合成中的应用。</p>
<h4 id="1-残差信息的定义"><strong>1. 残差信息的定义</strong></h4>
<p>在深度学习中，<strong>残差信息</strong>通常指的是输入数据与模型当前输出之间的差异。这种差异可能包含以下内容：</p>
<ul>
<li><strong>未被模型捕捉的细节</strong>：模型可能未能完全理解输入数据中的某些复杂特征。</li>
<li><strong>噪声或误差</strong>：输入数据中可能存在的噪声或模型预测中的误差。</li>
<li><strong>未充分利用的特征</strong>：输入数据中某些特征可能未被模型充分利用，残差信息可以提供额外的线索来优化这些特征的处理。</li>
</ul>
<h4 id="2-残差信息在语音合成中的应用"><strong>2. 残差信息在语音合成中的应用</strong></h4>
<p>在语音合成（TTS）系统中，残差信息可以用于优化生成的语音质量和自然度。以下是残差信息在语音合成中的具体应用：</p>
<h4 id="21-在双自回归架构中的作用"><strong>2.1 在双自回归架构中的作用</strong></h4>
<p>在Fish-Speech框架中，双自回归架构（Dual-AR）通过慢速变换器（Slow Transformer）和快速变换器（Fast Transformer）协同工作，处理全局和局部信息。残差信息在这个过程中扮演了重要角色：</p>
<ol>
<li><strong>慢速变换器的输出</strong>：慢速变换器生成的隐藏状态包含了全局语义信息，但可能未能完全捕捉到所有细节。这些未被完全处理的信息就是残差信息。</li>
<li><strong>快速变换器的细化</strong>：快速变换器接收慢速变换器的输出，并进一步处理残差信息。通过处理这些残差信息，快速变换器能够补充细节，优化生成的语音质量。例如：
<ul>
<li><strong>细节特征的补充</strong>：快速变换器可以处理残差信息中的声学细节，如音调、语调、情感表达等。</li>
<li><strong>自然度的提升</strong>：通过处理残差信息，快速变换器能够生成更自然、更流畅的语音。</li>
</ul>
</li>
</ol>
<h4 id="22-在矢量量化中的作用"><strong>2.2 在矢量量化中的作用</strong></h4>
<p>在GFSQ（Grouped Finite Scalar Vector Quantization）中，残差信息也起到重要作用。具体来说：</p>
<ul>
<li><strong>量化过程中的残差</strong>：在量化过程中，输入特征被映射到码本中的最近邻向量。这个映射过程可能会丢失一些细节信息，这些未被完全映射的信息就是残差信息。</li>
<li><strong>优化码本利用</strong>：通过处理这些残差信息，模型可以更好地调整码本的使用，从而提高合成语音的自然度和保真度。</li>
</ul>
<h4 id="23-在神经声码器中的作用"><strong>2.3 在神经声码器中的作用</strong></h4>
<p>在神经声码器（如FF-GAN）中，残差信息用于优化生成的音频信号。例如：</p>
<ul>
<li><strong>频谱细节的补充</strong>：神经声码器通过处理残差信息，补充频谱中的细节，从而生成更自然的音频。</li>
<li><strong>高频重建</strong>：残差信息可以帮助神经声码器更好地重建高频成分，这对于语音的自然度和清晰度至关重要。</li>
</ul>
<h4 id="3-残差信息的处理方法">3. 残差信息的处理方法</h4>
<p>在实际应用中，残差信息可以通过以下几种方式处理：</p>
<ol>
<li><strong>残差连接（Residual Connections）</strong>：在深度学习模型中，残差连接允许模型直接传递输入信息到后续层，从而保留未被处理的细节。例如，在Transformer架构中，残差连接被广泛使用。</li>
<li><strong>注意力机制（Attention Mechanisms）</strong>：通过注意力机制，模型可以动态地关注输入数据中的重要部分，从而更好地处理残差信息。</li>
<li><strong>多尺度特征提取</strong>：通过多尺度特征提取，模型可以同时处理全局和局部信息，从而更好地利用残差信息。</li>
</ol>
<h4 id="4-总结"><strong>4. 总结</strong></h4>
<p>残差信息在语音合成中扮演着重要角色，它包含了未被模型完全处理的细节和特征。通过合理处理残差信息，模型可以显著提升生成语音的质量和自然度。在Fish-Speech框架中，残差信息通过双自回归架构和GFSQ技术被充分利用，从而实现了高效、高质量的语音合成。</p>
    </div>

    
    
<div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">weedge</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
      2025-01-19
      
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">许可协议</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>


    
    

    <footer class="post-footer">
      <div class="post-tags">
          <a href="https://weedge.github.io/tags/non-g2p/">non-G2P</a>
          <a href="https://weedge.github.io/tags/bpe/">BPE</a>
          <a href="https://weedge.github.io/tags/llama/">llama</a>
          <a href="https://weedge.github.io/tags/ar/">AR</a>
          <a href="https://weedge.github.io/tags/dual-ar/">dual-AR</a>
          <a href="https://weedge.github.io/tags/vq-gan/">VQ-GAN</a>
          <a href="https://weedge.github.io/tags/firefly-gan/">Firefly-GAN</a>
          <a href="https://weedge.github.io/tags/fsq/">FSQ</a>
          <a href="https://weedge.github.io/tags/gfsq/">GFSQ</a>
          <a href="https://weedge.github.io/tags/mel-spectrogram/">mel-spectrogram</a>
          <a href="https://weedge.github.io/tags/streaming/">streaming</a>
          
        </div>

      
      <nav class="post-nav">
        
        
          <a class="next" href="/post/multimoding/voices/cosyvoice2/">
            <span class="next-text nav-default">论文解读：CosyVoice2: Scalable Streaming Speech Synthesis with Large Language Models</span>
            <span class="prev-text nav-mobile">下一篇</span>
            
            <i class="iconfont">
              <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

            </i>
          </a>
      </nav>
    </footer>
  </article>

  
  

  
  

  

  
  

  

  

  <div class="disqus-comment">
  <div class="disqus-button" id="load_disqus" onclick="load_disqus()">
    显示 Disqus 评论
  </div>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = "https://weedge.github.io/post/multimoding/voices/fishspeech/";
    };
    function load_disqus() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'weedge';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

      $('#load_disqus').remove();
    };
  </script>
  <noscript>Please enable JavaScript to view the
    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  </div>

    

  

        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="icon-links">
  
  
    <a href="mailto:weege007@gmail.com" rel="me noopener" class="iconfont"
      title="email" >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://github.com/weedge" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://weibo.com/weedge" rel="me noopener" class="iconfont"
      title="weibo"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M385.714286 733.714286q12-19.428571 6.285714-39.428571t-25.714286-28.571429q-19.428571-8-41.714286-0.571429t-34.285714 26.285714q-12.571429 19.428571-7.428571 39.142857t24.571429 28.857143 42.571429 1.428571 35.714286-27.142857zm53.714286-69.142857q4.571429-7.428571 2-15.142857t-10-10.571429q-8-2.857143-16.285714 2.857143t-12.285714 10.571429q-9.714286 17.714286 7.428571 25.714286 8 2.857143 16.571429 2.857143t12.571429-10.571429zm99.428571 61.142857q-25.714286 58.285714-90.285714 85.714286t-128 6.857143q-61.142857-19.428571-84.285714-72.285714t3.714286-107.142857q26.857143-53.142857 86.571429-79.428571t120.285714-10.857143q63.428571 16.571429 90.571429 68.285714t1.428571 108.857143zm178.285714-91.428571q-5.142857-54.857143-50.857143-97.142857t-119.142857-62.285714-156.857143-12q-127.428571 13.142857-211.142857 80.857143t-75.714286 151.142857q5.142857 54.857143 50.857143 97.142857t119.142857 62.285714 156.857143 12q127.428571-13.142857 211.142857-80.857143t75.714286-151.142857zm176 2.285714q0 38.857143-21.142857 79.714286t-62.285714 78.285714-96.285714 67.142857-129.142857 47.428571-154.571429 17.714286-157.142857-19.142857-137.428571-53.142857-98-86.285714-37.142857-114q0-65.714286 39.714286-140t112.857143-147.428571q96.571429-96.571429 195.142857-134.857143t140.857143 4q37.142857 36.571429 11.428571 119.428571-2.285714 8-0.571429 11.428571t5.714286 4 8.285714 2.857143 7.714286-2l3.428571-1.142857q79.428571-33.714286 140.571429-33.714286t87.428571 34.857143q25.714286 36 0 101.714286-1.142857 7.428571-2.571429 11.428571t2.571429 7.142857 6.857143 4.285714 9.714286 3.428571q32.571429 10.285714 58.857143 26.857143t45.714286 46.571429 19.428571 66.571429zm-42.285714-356.571429q24 26.857143 31.142857 62t-3.714286 67.142857q-4.571429 13.142857-16.857143 19.428571t-25.428571 2.285714q-13.142857-4.571429-19.428571-16.857143t-2.285714-25.428571q11.428571-36-13.714286-63.428571t-61.142857-20q-13.714286 2.857143-25.714286-4.571429t-14.285714-21.142857q-2.857143-13.714286 4.571429-25.428571t21.142857-14.571429q34.285714-7.428571 68 3.142857t57.714286 37.428571zm103.428571-93.142857q49.714286 54.857143 64.285714 127.142857t-7.714286 138q-5.142857 15.428571-19.428571 22.857143t-29.714286 2.285714-22.857143-19.428571-2.857143-29.714286q16-46.857143 5.714286-98.285714t-45.714286-90.285714q-35.428571-39.428571-84.571429-54.571429t-98.857143-4.857143q-16 3.428571-29.714286-5.428571t-17.142857-24.857143 5.428571-29.428571 24.857143-16.857143q70.285714-14.857143 139.428571 6.571429t118.857143 76.857143z"></path>
</svg>

    </a>


<a href="https://weedge.github.io/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
   
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2013 -
    2025
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        weedge
        
      </span></span>

  
  

  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont">
        
        <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

      </i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js" integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin="anonymous"></script>












  
    <script type="text/javascript" src="/js/load-photoswipe.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  









  <script id="dsq-count-scr" src="//weedge.disqus.com/count.js" async></script>






  <script src="/js/copy-to-clipboard.js"></script>


</body>
</html>
