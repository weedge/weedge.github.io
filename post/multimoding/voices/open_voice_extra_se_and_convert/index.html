<!DOCTYPE html>
<html lang="zh-cn" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>论文解读 OpenVoice: Versatile Instant Voice Cloning - </title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="weedge" />
  <meta name="description" content="使用meloTTS 本文生成的音频
  使用openVoice clone 自己的声音 阅读本文内容    文件直接上传在github中, 暂未走cdn, 缓存比较慢，可下载播放， 下载地址： http://github.com/weedge/paper-speaker/tree/main/multimoding/voices/open_voice_inference
  openVoiceV2 tone color clone: base TTS &#43; extra tone color &#43; convert
Base TTS: use meloTTS , 支持TTS模型训练，以及load Pre-Trained ckpt 进行TTS, 在 VITS基础上支持多种语言；
论文地址：OpenVoice: Versatile Instant Voice Cloning
论文主作者：Zengyi Qin (同时是JetMoE的作者，站在巨人的肩膀上创新)
公开的权重：
 OpenVoice: https://huggingface.co/myshell-ai/OpenVoice OpenVoiceV2: https://huggingface.co/myshell-ai/OpenVoiceV2  源码：
 https://github.com/myshell-ai/OpenVoice https://github.com/myshell-ai/MeloTTS  训练： MSML dataset 和 训练过程 未公开
附操作笔记： https://github.com/weedge/doraemon-nb/blob/main/myshell_ai_OpenVoiceV2.ipynb
" />

  <meta name="keywords" content="工作, 技术, 生活" />






<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://weedge.github.io/post/multimoding/voices/open_voice_extra_se_and_convert/" />





<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css" integrity="sha256-&#43;ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media="screen" crossorigin="anonymous">




<link rel="stylesheet" href="/css/copy-to-clipboard.css">


<meta property="og:title" content="论文解读 OpenVoice: Versatile Instant Voice Cloning" />
<meta property="og:description" content="使用meloTTS 本文生成的音频




  
    
  
  
  


使用openVoice clone 自己的声音 阅读本文内容 



  
    
  
  
  



文件直接上传在github中, 暂未走cdn, 缓存比较慢，可下载播放， 下载地址： http://github.com/weedge/paper-speaker/tree/main/multimoding/voices/open_voice_inference


openVoiceV2 tone color clone: base TTS &#43; extra tone color &#43; convert
Base TTS: use meloTTS , 支持TTS模型训练，以及load Pre-Trained ckpt 进行TTS,  在 VITS基础上支持多种语言；
论文地址：OpenVoice: Versatile Instant Voice Cloning
论文主作者：Zengyi Qin (同时是JetMoE的作者，站在巨人的肩膀上创新)
公开的权重：

OpenVoice: https://huggingface.co/myshell-ai/OpenVoice
OpenVoiceV2: https://huggingface.co/myshell-ai/OpenVoiceV2

源码：

https://github.com/myshell-ai/OpenVoice
https://github.com/myshell-ai/MeloTTS

训练： MSML dataset 和 训练过程 未公开
附操作笔记： https://github.com/weedge/doraemon-nb/blob/main/myshell_ai_OpenVoiceV2.ipynb" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://weedge.github.io/post/multimoding/voices/open_voice_extra_se_and_convert/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-05-11T10:26:23+08:00" />
<meta property="article:modified_time" content="2024-05-11T10:26:23+08:00" />

<meta itemprop="name" content="论文解读 OpenVoice: Versatile Instant Voice Cloning">
<meta itemprop="description" content="使用meloTTS 本文生成的音频




  
    
  
  
  


使用openVoice clone 自己的声音 阅读本文内容 



  
    
  
  
  



文件直接上传在github中, 暂未走cdn, 缓存比较慢，可下载播放， 下载地址： http://github.com/weedge/paper-speaker/tree/main/multimoding/voices/open_voice_inference


openVoiceV2 tone color clone: base TTS &#43; extra tone color &#43; convert
Base TTS: use meloTTS , 支持TTS模型训练，以及load Pre-Trained ckpt 进行TTS,  在 VITS基础上支持多种语言；
论文地址：OpenVoice: Versatile Instant Voice Cloning
论文主作者：Zengyi Qin (同时是JetMoE的作者，站在巨人的肩膀上创新)
公开的权重：

OpenVoice: https://huggingface.co/myshell-ai/OpenVoice
OpenVoiceV2: https://huggingface.co/myshell-ai/OpenVoiceV2

源码：

https://github.com/myshell-ai/OpenVoice
https://github.com/myshell-ai/MeloTTS

训练： MSML dataset 和 训练过程 未公开
附操作笔记： https://github.com/weedge/doraemon-nb/blob/main/myshell_ai_OpenVoiceV2.ipynb"><meta itemprop="datePublished" content="2024-05-11T10:26:23+08:00" />
<meta itemprop="dateModified" content="2024-05-11T10:26:23+08:00" />
<meta itemprop="wordCount" content="8639">
<meta itemprop="keywords" content="multimoding,voice,openvoice,mel-spectrogram," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="论文解读 OpenVoice: Versatile Instant Voice Cloning"/>
<meta name="twitter:description" content="使用meloTTS 本文生成的音频




  
    
  
  
  


使用openVoice clone 自己的声音 阅读本文内容 



  
    
  
  
  



文件直接上传在github中, 暂未走cdn, 缓存比较慢，可下载播放， 下载地址： http://github.com/weedge/paper-speaker/tree/main/multimoding/voices/open_voice_inference


openVoiceV2 tone color clone: base TTS &#43; extra tone color &#43; convert
Base TTS: use meloTTS , 支持TTS模型训练，以及load Pre-Trained ckpt 进行TTS,  在 VITS基础上支持多种语言；
论文地址：OpenVoice: Versatile Instant Voice Cloning
论文主作者：Zengyi Qin (同时是JetMoE的作者，站在巨人的肩膀上创新)
公开的权重：

OpenVoice: https://huggingface.co/myshell-ai/OpenVoice
OpenVoiceV2: https://huggingface.co/myshell-ai/OpenVoiceV2

源码：

https://github.com/myshell-ai/OpenVoice
https://github.com/myshell-ai/MeloTTS

训练： MSML dataset 和 训练过程 未公开
附操作笔记： https://github.com/weedge/doraemon-nb/blob/main/myshell_ai_OpenVoiceV2.ipynb"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->



<script>
  MathJax = {
    tex: {
      inlineMath: [["$", "$"]],
    },
    displayMath: [
      ["$$", "$$"],
      ["\[\[", "\]\]"],
    ],
    svg: {
      fontCache: "global",
    },
  };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>





</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo"></a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/about/">时间飘过</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/">主页</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/post/">归档</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/tags/">标签</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/categories/">分类</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/perf-book-cn/zh/" rel="noopener" target="_blank">
              《现代CPU性能分析与优化》
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.us.kg/" rel="noopener" target="_blank">
              Podcast AI
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://github.com/weedge/what_are_embeddings/blob/main/embeddings-cn.pdf" rel="noopener" target="_blank">
              What are Embeddings
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          
        
      </li>
    

    
  </ul>
</nav>


  
    






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

  

  

  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/about/">时间飘过</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/">主页</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/post/">归档</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/tags/">标签</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/categories/">分类</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.github.io/perf-book-cn/zh/" rel="noopener" target="_blank">
              《现代CPU性能分析与优化》
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://weedge.us.kg/" rel="noopener" target="_blank">
              Podcast AI
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://github.com/weedge/what_are_embeddings/blob/main/embeddings-cn.pdf" rel="noopener" target="_blank">
              What are Embeddings
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">论文解读 OpenVoice: Versatile Instant Voice Cloning</h1>
      
      <div class="post-meta">
        <time datetime="2024-05-11" class="post-time">
          2024-05-11
        </time>
        <div class="post-category">
            <a href="https://weedge.github.io/categories/%E6%8A%80%E6%9C%AF/"> 技术 </a>
            <a href="https://weedge.github.io/categories/tts/"> TTS </a>
            
          </div>
        

        
        

        
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#0-摘要">0 摘要</a></li>
    <li><a href="#1-介绍">1 介绍</a></li>
    <li><a href="#2-方法">2 方法</a>
      <ul>
        <li><a href="#21-直觉">2.1 直觉</a></li>
        <li><a href="#22-模型结构">2.2 模型结构</a></li>
        <li><a href="#23-训练-training">2.3 训练 training</a></li>
      </ul>
    </li>
    <li><a href="#3-实验-experiment">3 实验 Experiment</a></li>
    <li><a href="#4-讨论">4 讨论</a></li>
    <li><a href="#附模型结构-配置和模块">附：模型结构-配置和模块</a></li>
  </ul>
</nav>
  </div>
</div>

    
    <div class="post-content">
      <p>使用meloTTS 本文生成的音频</p>



<figure >
  <audio controls class="player" preload="">
    <source src="https://media.githubusercontent.com/media/weedge/paper-speaker/main/multimoding/voices/open_voice_inference/zh-tts.wav" type="audio/mpeg">
  </audio>
  
  
</figure>

<p>使用openVoice clone 自己的声音 阅读本文内容 


<figure >
  <audio controls class="player" preload="">
    <source src="https://media.githubusercontent.com/media/weedge/paper-speaker/main/multimoding/voices/open_voice_inference/clone-me-zh-tts.wav" type="audio/mpeg">
  </audio>
  
  
</figure>
</p>
<blockquote>
<p>文件直接上传在github中, 暂未走cdn, 缓存比较慢，可下载播放， 下载地址： <a href="http://github.com/weedge/paper-speaker/tree/main/multimoding/voices/open_voice_inference">http://github.com/weedge/paper-speaker/tree/main/multimoding/voices/open_voice_inference</a></p>
</blockquote>
<hr>
<p>openVoiceV2 tone color clone: base TTS + extra tone color + convert</p>
<p>Base TTS: use meloTTS , 支持TTS模型训练，以及load Pre-Trained ckpt 进行TTS,  在 <a href="https://github.com/jaywalnut310/vits">VITS</a>基础上支持多种语言；</p>
<p>论文地址：<a href="https://arxiv.org/abs/2312.01479">OpenVoice: Versatile Instant Voice Cloning</a></p>
<p>论文主作者：Zengyi Qin (同时是JetMoE的作者，站在巨人的肩膀上创新)</p>
<p>公开的权重：</p>
<ul>
<li>OpenVoice: <a href="https://huggingface.co/myshell-ai/OpenVoice">https://huggingface.co/myshell-ai/OpenVoice</a></li>
<li>OpenVoiceV2: <a href="https://huggingface.co/myshell-ai/OpenVoiceV2">https://huggingface.co/myshell-ai/OpenVoiceV2</a></li>
</ul>
<p>源码：</p>
<ul>
<li><a href="https://github.com/myshell-ai/OpenVoice">https://github.com/myshell-ai/OpenVoice</a></li>
<li><a href="https://github.com/myshell-ai/MeloTTS">https://github.com/myshell-ai/MeloTTS</a></li>
</ul>
<p>训练： MSML dataset 和 训练过程 未公开</p>
<p><strong>附操作笔记</strong>： <a href="https://github.com/weedge/doraemon-nb/blob/main/myshell_ai_OpenVoiceV2.ipynb">https://github.com/weedge/doraemon-nb/blob/main/myshell_ai_OpenVoiceV2.ipynb</a></p>
<p>抽取音色：</p>
<script type="application/javascript" src="https://gist.github.com/weedge/914e71c22aaaca90db1660fb5c5fe645.js"></script>

<p>clone音色转换：</p>
<script type="application/javascript" src="https://gist.github.com/weedge/7b8393990adfa205afbede4d1c8615ec.js"></script>

<p><strong>Tips</strong>: 使用cpu转换的时间比较长，比如本文音色转换时间大概需要1个小时，如果是在线场景，像app.myshell.ai 在线转换的场景中，限制输入文本的最大长度为300，而且也会定义转换的超时时间，防止过度消耗计算资源。如果是离线场景pipeline，可以采用m/r框架来分布式处理音频文件转换。（这个是后续修改加入，音频文件未生成）； 如果对于文章经常更新的场景，需要重新生成文章音频文件，这个是比较消耗资源，暂未找到一种diff方案增量更新音频。</p>
<h2 id="0-摘要">0 摘要</h2>
<p>我们介绍了 OpenVoice，这是一种多功能的即时语音克隆方法，只需来自参考说话者的短音频剪辑即可复制其声音并生成多种语言的语音。OpenVoice 在以下领域中代表了重大进展：</p>
<p><strong>1) 灵活的语音风格控制。</strong> OpenVoice 能够对语音风格进行精细的控制，包括情感、口音、节奏、停顿和语调，除了复制参考说话者的音色外。这些语音风格不是直接从参考说话者的风格复制和受到限制的。以往的方法缺乏在克隆后灵活操纵语音风格的能力。</p>
<p><strong>2) 零样本跨语言语音克隆。</strong> OpenVoice 实现了零样本跨语言语音克隆，适用于未包含在大规模说话者训练集中的语言。与以往的方法不同，以往的方法通常需要为所有语言提供广泛的大规模说话者多语言（MSML）数据集，而 OpenVoice 可以在没有针对该语言的任何大规模说话者训练数据的情况下将语音克隆到新语言中。OpenVoice 还具有<strong>计算效率高</strong>的特点，成本比商业可用的甚至性能较差的 API 低几十倍。为了促进该领域的进一步研究，我们已经将源代码和训练模型公开。我们还在我们的演示网站提供了定性结果。在其公开发布之前，我们内部版本的 OpenVoice 在 2023 年 5 月至 10 月之间被全球用户数千万次使用，作为 MyShell.ai 的后端服务。</p>
<h2 id="1-介绍">1 介绍</h2>
<p>在文本转语音（TTS）合成中，即时语音克隆（IVC）意味着 TTS 模型可以在给定短音频样本的情况下克隆任何参考说话者的声音，而无需对参考说话者进行额外的训练。它也被称为零样本 TTS。IVC 使用户能够灵活定制生成的语音，在各种实际应用中展现出巨大的价值，例如媒体内容创作、定制聊天机器人以及人与计算机或大型语言模型之间的多模式交互。</p>
<p>在 IVC 方面已经进行了大量的研究工作。自回归方法的例子包括 VALLE<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 和 XTTS<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，它们从参考音频中提取声学标记或说话者嵌入作为自回归模型的条件。然后，自回归模型按顺序生成声学标记，然后将其解码为原始音频波形。尽管这些方法可以克隆音色，但它们不允许用户灵活地操纵其他重要的风格参数，如情感、口音、节奏、停顿和语调。而且，自回归模型相对计算成本高，推理速度相对较慢。非自回归方法的例子包括 YourTTS<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>和最近开发的 Voicebox<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>，它们展示了显著更快的推理速度，但仍无法提供除音色以外的风格参数的灵活控制。现有方法的另一个共同缺点是，它们通常需要大量的 MSML 数据集才能实现跨语言语音克隆。这种组合数据要求可能会限制它们包含新语言的灵活性。此外，由于科技巨头的语音克隆研究<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 大多是闭源的，因此研究界没有便捷的方式站在他们的肩膀上，推动该领域的进展。</p>
<p>我们介绍了 OpenVoice，这是一种灵活的即时语音克隆方法，针对该领域的以下关键问题：</p>
<ul>
<li>
<p>除了克隆音色外，如何灵活控制其他重要的风格参数，如情感、口音、节奏、停顿和语调？这些特征对于生成上下文自然的语音和对话非常重要，而不是单调地叙述输入文本。先前的方法<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>只能克隆参考说话者的单调音色和风格，但不允许灵活操纵风格。</p>
</li>
<li>
<p>如何以简单的方式实现零样本跨语言语音克隆。我们提出了两个零样本能力方面的重要问题，这些问题并不是先前研究所解决的：</p>
<ul>
<li>如果参考说话者的语言不在 MSML 数据集中，模型是否能够克隆其语音？</li>
<li>如果生成的语音的语言不在 MSML 数据集中，模型是否能够克隆参考声音并生成该语言的语音？
在先前的研究中<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>，参考说话者的语言和模型生成的语言都应在 MSML 数据集中大量存在。但如果两者都不存在呢？</li>
</ul>
</li>
<li>
<p>如何在不降低质量的情况下实现超快速的实时推理速度，这对于大规模商业生产环境至关重要。</p>
</li>
</ul>
<p>为了解决前两个问题，OpenVoice 被设计为尽可能地解耦语音中的组件。语言、音色和其他重要的语音特征的生成被独立地分开，使得可以灵活操纵单个语音风格和语言类型。这是在不标记任何语音风格的 MSML 训练集中实现的。我们想澄清一点，<strong>本研究中的零样本跨语言任务与 VALLE-X 中的任务不同<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></strong>。在 VALLE-X 中，所有语言的数据都需要包含在 MSML 训练集中，模型无法推广到 MSML 训练集之外的未见过的语言。相比之下，OpenVoice 设计为推广到完全未见过的 MSML 训练集之外的语言。第三个问题默认被解决，因为解耦的结构降低了对模型大小和计算复杂度的要求。我们不需要一个大型模型来学习所有东西。此外，我们避免了自回归或扩散组件以加快推理速度。</p>
<p>我们在此公开发布之前的 OpenVoice 内部版本已在 2023 年 5 月至 10 月期间全球用户使用了数千万次。它为 MyShell.ai 的即时语音克隆后端提供支持，并在该平台上见证了数百倍的用户增长。为了促进该领域的研究进展，我们详细解释了该技术，并公开提供了源代码和模型权重。</p>
<h2 id="2-方法">2 方法</h2>
<p>技术方法 <strong>简单</strong> 实现但效果惊人。我们首先介绍 OpenVoice 背后的直觉，然后详细说明模型结构和训练过程。</p>
<h3 id="21-直觉">2.1 直觉</h3>
<p><strong>难点</strong> 。很明显，同时克隆任何说话者的音色，实现对所有其他风格的灵活控制，并且用很少的工作量添加新语言可能会非常具有挑战性。这需要大量的组合数据集，其中受控参数相交，并且只在一个属性上有差异的数据对，并且有良好标记，以及一个相对较大容量的模型来拟合数据集。</p>
<p><strong>简单的部分</strong> 。我们还注意到，在常规的单说话者 TTS 中，只要不需要语音克隆，就相对容易添加对其他风格参数的控制和添加新语言。例如，记录一个包含 10K 个带有标记的情感和语调的短音频样本的单说话者数据集足以训练出一个提供对情感和语调进行控制的单说话者 TTS 模型。通过在数据集中包含另一个说话者，添加新语言或口音也很简单。</p>
<p>OpenVoice 背后的直觉是将 IVC 任务解耦为单独的子任务，其中每个子任务比耦合任务更容易实现。音色克隆与所有其他风格参数和语言的控制完全解耦。<strong>我们提议使用基础说话者 TTS 模型来控制风格参数和语言，并使用音色转换器将参考音色体现到生成的语音中</strong>。</p>
<h3 id="22-模型结构">2.2 模型结构</h3>
<p>我们在图1 中说明了模型结构。OpenVoice 的两个主要组件是基础说话者 TTS 模型和音色转换器。基础说话者 TTS 模型是单说话者或多说话者模型，允许对风格参数（例如情感、口音、节奏、停顿和语调）、口音和语言进行控制。由该模型生成的语音传递给音色转换器，该转换器将基础说话者的音色改变为参考说话者的音色。</p>
<p><img src="https://raw.githubusercontent.com/weedge/mypic/master/multimoding/voices/open_voice_inference/1.jpg" alt=""></p>
<p><em>图1: OpenVoice 框架示意图。我们使用一个基础说话者模型来控制风格和语言，并使用一个转换器将参考说话者的音色体现到语音中。</em></p>
<p><strong>基础说话者 TTS 模型。</strong> 基础说话者 TTS 模型的选择非常灵活。例如，VITS<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> 模型可以修改为在其文本编码器和持续预测器中接受风格和语言嵌入。其他选择，如 InstructTTS<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  也可以接受风格提示。也可以使用商业可用（且价格低廉）的模型，如 Microsoft TTS，该模型接受指定情感、停顿和发音的语音合成标记语言（SSML）。甚至可以跳过基础说话者 TTS 模型，自己按照所需的风格和语言阅读文本。在我们的 OpenVoice 实现中，默认使用了 VITS<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  模型，但其他选择完全可行。我们将基础模型的输出表示为 $\mathbf{X}(L_I, S_I, C_I)$，其中三个参数分别表示语言、风格和音色。类似地，来自参考说话者的语音音频表示为 $\mathbf{X}(L_O, S_O, C_O)$。</p>
<p>附 vits 模型结构：</p>
<p><img src="https://raw.githubusercontent.com/weedge/mypic/master/multimoding/voices/open_voice_inference/2.jpg" alt=""></p>
<p><strong>音色转换器。</strong> 音色转换器是一个编码器-解码器结构，中间带有可逆正规化流<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> 。编码器是一个 1D 卷积神经网络，接受 $\mathbf{X}(L_I, S_I, C_I)$ 的短时傅里叶变换频谱作为输入。所有卷积都是单步长的。编码器输出的特征图表示为 $\mathbf{Y}(L_I, S_I, C_I)$。音色提取器是一个简单的 2D 卷积神经网络，操作输入语音的梅尔频谱图，并输出一个单一的特征向量，编码了音色信息。我们将其应用于 $\mathbf{X}(L_I, S_I, C_I)$ 以获得向量 $\mathbf{v}(C_I)$，然后将其应用于 $\mathbf{X}(L_O, S_O, C_O)$ 以获得向量 $\mathbf{v}(C_O)$。</p>
<p>正规化流层将 $\mathbf{Y}(L_I, S_I, C_I)$ 和 $\mathbf{v}(C_I)$ 作为输入，并输出消除音色信息但保留所有其他风格属性的特征表示 $\mathbf{Z}(L_I, S_I)$。<strong>特征 $\mathbf{Z}(L_I, S_I)$ 沿着时间维度与国际音标（IPA  International Phonetic Alphabet）<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> 对齐</strong>。关于如何学习这样的特征表示的详细信息将在下一节中解释。然后，我们以逆方向应用正规化流层，将 $\mathbf{Z}(L_I, S_I)$ 和 $\mathbf{v}(C_O)$ 作为输入，并输出 $\mathbf{Y}(L_I, S_I, C_O)$。这是一个关键步骤，参考说话者的音色 $C_O$ 被体现到特征图中。然后，$\mathbf{Y}(L_I, S_I, C_O)$ 通过包含一堆转置 1D 卷积的 HiFi-Gan<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> 解码为原始波形 $\mathbf{X}(L_I, S_I, C_O)$。我们的 OpenVoice 实现中的整个模型都是前向传递的，没有任何自回归组件。音色转换器在概念上类似于语音转换<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> ，但在功能、模型结构和训练目标上有所不同。音色转换器中的流层在结构上类似于基于流的 TTS 方法<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> ，但具有不同的功能和训练目标。</p>
<p><strong>替代方法和缺点。</strong> 尽管有替代方法<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>  可以提取 $\mathbf{Z}(L_I, S_I)$，但我们经验性地发现所提出的方法实现了最佳的音频质量。可以使用 HuBERT<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>  提取离散或连续的声学单元<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>   来消除音色信息，但我们发现这种方法也会消除输入语音的情感和口音。当输入是未见过的语言时，这种方法也存在保留语素自然发音的问题。我们还研究了另一种方法<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> ，它通过仔细构建信息瓶颈仅保留语音内容，但我们观察到这种方法无法完全消除音色。</p>
<p><strong>新颖性的备注。</strong> OpenVoice 并不打算<em>发明</em>模型结构中的子模块。基础说话者 TTS 模型和音色转换器都从现有工作<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  中借鉴了模型结构。OpenVoice 的贡献是 <strong>将声音风格和语言控制与音色克隆分离</strong> 的解耦框架。这非常简单，但非常有效，特别是当想要控制风格、口音或泛化到新语言时。如果要在类似于 XTTS<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>  的耦合框架上实现相同的控制，可能需要大量的数据和计算，而且流利地讲每种语言也相对困难。在 OpenVoice 中，只要单说话者 TTS 说得流利，克隆的语音就会流利。将声音风格和语言的生成与音色的生成分离是 OpenVoice 的核心理念。我们还在我们的选择中提供了使用音色转换器中的流层的见解，以及在我们的实验部分中选择通用音素系统(<strong>IPA: International Phonetic Alphabet</strong>)在语言泛化中的重要性。</p>
<h3 id="23-训练-training">2.3 训练 training</h3>
<p>为了训练基础说话者 TTS 模型，我们从两位英语说话者（美国口音和英国口音）、一位中文说话者和一位日语说话者收集了音频样本。总共有 30K 个句子，平均句子长度为 7 秒。英语和中文数据具有情感分类标签。我们修改了 VITS<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  模型，将情感分类嵌入、语言分类嵌入和说话者 id 输入到文本编码器、持续预测器和流层中。训练遵循 VITS 作者提供的标准流程<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> 。训练后的模型能够通过切换不同的基础说话者来改变口音和语言，并以不同的情感朗读输入文本。我们还尝试了额外的训练数据，并确认节奏、停顿和语调可以像情感一样学习。</p>
<p>为了训练音色转换器，我们从 20K 个人员收集了 300K 个音频样本。大约 18 万个样本是英语，6 万个样本是中文，6 万个样本是日语。这就是我们所谓的 MSML 数据集。音色转换器的训练目标是双重的。首先，我们要求编码器-解码器产生自然声音。在训练期间，我们直接将编码器的输出馈送到解码器，并使用原始波形进行监督，使用梅尔频谱损失和 HiFi-GAN<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>  损失对生成的波形进行评估。我们不会在这里详细说明，因为先前的文献<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> 已经很好地解释了这一点。</p>
<p>其次，我们要求流层尽可能多地消除音色信息。在训练期间，对于每个音频样本，其文本被转换为<strong>国际音标（IPA）中的音素</strong>的序列<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> ，每个音素由一个可学习的向量嵌入表示。向量嵌入的序列被传递到一个 Transformer<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>  编码器，以产生文本内容的特征表示。将这个特征表示表示为 $\mathbf{L}\in\mathbb{R}^{c\times l}$，其中 $c$ 是特征通道数，$l$ 是输入文本中的音素数。音频波形经过编码器和流层处理，产生特征表示 $\mathbf{Z}\in\mathbb{R}^{c\times t}$，其中 $t$ 是特征沿时间维度的长度。然后，我们使用动态时间规整<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> （另一种是单调规整<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> ）将 $\mathbf{L}$ 与 $\mathbf{Z}$ 沿着时间维度对齐，产生 $\mathbf{\bar{L}}\in \mathbb{R}^{c\times t}$，并最小化 $\mathbf{\bar{L}}$ 和 $\mathbf{Z}$ 之间的 KL 散度。由于 $\mathbf{\bar{L}}$ 不包含任何音色信息，最小化目标会鼓励流层从输出 $\mathbf{Z}$ 中消除音色信息。流层被音色编码器的音色信息所条件化，这进一步帮助流层确定需要消除的信息。此外，我们没有为流层提供任何风格或语言信息来进行条件化，这防止了流层除音色外的信息。由于流层是可逆的，将它们条件化为一个新的音色信息并运行其逆过程可以将新的音色添加回特征表示中，然后将其解码为带有新音色的原始波形。</p>
<h2 id="3-实验-experiment">3 实验 Experiment</h2>
<p>评估语音克隆的准确性存在几个客观上的困难。首先，不同的研究（例如 <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> 、<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> ）通常具有不同的训练和测试集。数字比较可能从本质上是不公平的。即使他们的指标如平均意见分数可以通过众包评估，测试集的多样性和难度也会显著影响结果。例如，如果测试集中许多样本都是神经语音，集中在人类语音分布的平均值上，那么大多数方法都相对容易实现良好的语音克隆结果。第二，不同的研究通常具有不同的训练集，规模和多样性会对结果产生重大影响。第三，不同的研究可能在其核心功能上有不同的关注点。OpenVoice 主要旨在音色克隆、对语音风格的灵活控制，并使跨语言语音克隆即使没有新语言的海量说话者数据也变得简单。这与以前关于语音克隆或零-shot TTS 的工作的目标不同。因此，我们主要关注分析 OpenVoice 本身的定性性能，并公开了相关研究人员自由评估的音频样本。</p>
<p><strong>准确的音色克隆。</strong> 我们构建了一个参考说话者的测试集，选择了名人、游戏角色和匿名个体。测试集涵盖了广泛的语音分布，包括具有表现力的独特语音和人类语音分布中的中性样本。使用任何 4 个基础说话者和任何参考说话者，OpenVoice 都能准确克隆参考音色，并以多种语言和口音生成语音。我们邀请读者访问该网站 <a href="https://research.myshell.ai/open-voice/accurate-tone-color-cloning">https://research.myshell.ai/open-voice/accurate-tone-color-cloning</a> 获取定性结果。</p>
<p><strong>对语音风格的灵活控制。</strong> 提出的框架能够灵活控制语音风格的前提是音色转换器能够仅修改音色并保留所有其他风格和语音属性。为了确认这一点，我们使用了我们的基础说话者模型和 Microsoft TTS with SSML 来生成一个包含 1K 个样本的具有多种风格（情感、口音、节奏、停顿和语调）的语音语料库作为基础语音。转换为参考音色后，我们观察到所有风格都被很好地保留。在极少数情况下，情感会略微中性化，我们发现解决这个问题的一种方法是用同一基础说话者的具有不同情感的多个句子的平均向量替换该特定句子的音色嵌入向量。这样做会给流层提供较少的情感信息，以便它们不会消除情感。由于音色转换器能够保留基础声音的所有风格，通过简单操作基础说话者 TTS 模型来控制语音风格变得非常简单。定性结果可在此网站上公开获得 <a href="https://research.myshell.ai/open-voice/flexible-voice-style-control">https://research.myshell.ai/open-voice/flexible-voice-style-control</a>。</p>
<p><strong>轻松的跨语言语音克隆。</strong> OpenVoice 实现了几乎零-shot 的跨语言语音克隆，而不需要使用新语言的海量说话者数据。它确实需要语言的基础说话者，可以通过现成的模型和数据集轻松实现。在我们的网站 <a href="https://research.myshell.ai/open-voice/zero-shot-cross-lingual-voice-cloning">https://research.myshell.ai/open-voice/zero-shot-cross-lingual-voice-cloning</a> 上，我们提供了丰富的样本，展示了所提出方法的跨语言语音克隆能力。跨语言能力有两个方面：</p>
<ul>
<li>当参考说话者的语言在 MSML 数据集中看不到时，模型能够准确克隆参考说话者的音色。</li>
<li>当生成语音的语言在 MSML 数据集中看不到时，模型能够克隆参考声音并以该语言说话，只要基础说话者 TTS 支持该语言。</li>
</ul>
<p><strong>低成本快速推理。</strong> 由于 OpenVoice 是一个前向结构，没有任何自回归组件，它具有非常高的推理速度。我们的实验表明，略微优化的 OpenVoice 版本（包括基础说话者模型和音色转换器）能够在单个 A10G GPU 上实现 $12\times$ 的实时性能，这意味着生成一秒语音只需 85ms。通过详细的 GPU 使用分析，我们估计上限约为 $40\times$ 的实时性能，但我们将这一改进留作将来的工作。</p>
<p><strong>IPA 的重要性。</strong> 我们发现将 IPA 用作音素词典对于音色转换器执行跨语言语音克隆至关重要。正如我们在第2.3节中详细说明的那样，在训练音色转换器时，首先将文本转换为 IPA 中的一系列音素，然后每个音素由一个可学习的向量嵌入表示。嵌入序列通过变压器层编码，并与流层的输出计算损失，旨在消除音色信息。IPA 本身是一个跨语言统一的音素词典，这使得流层能够产生语言中性的表示。即使我们向音色转换器输入具有看不见语言的语音，它仍能顺利处理音频。我们还尝试过其他类型的音素词典，但生成的音色转换器往往会在未知语言中发音不准确。虽然输入音频是正确的，但输出音频很可能有问题，听起来不像是母语人士。</p>
<h2 id="4-讨论">4 讨论</h2>
<p>OpenVoice 展示了出色的实例语音克隆能力，并在语音风格和语言方面比以前的方法更加灵活。该方法背后的直觉是，只要我们不要求模型具有克隆参考说话者音色的能力，训练基础说话者 TTS 模型来控制语音风格和语言相对容易。因此，我们建议将音色克隆与其余语音风格和语言分离，我们认为这是 OpenVoice 的基本设计原则。为了促进未来的研究，我们公开了源代码和模型权重。</p>
<h2 id="附模型结构-配置和模块">附：模型结构-配置和模块</h2>
<p>Config:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;_version_&#34;</span><span class="p">:</span> <span class="s2">&#34;v2&#34;</span><span class="p">,</span>
  <span class="nt">&#34;data&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;sampling_rate&#34;</span><span class="p">:</span> <span class="mi">22050</span><span class="p">,</span>
    <span class="nt">&#34;filter_length&#34;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span>
    <span class="nt">&#34;hop_length&#34;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
    <span class="nt">&#34;win_length&#34;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span>
    <span class="nt">&#34;n_speakers&#34;</span><span class="p">:</span> <span class="mi">0</span>
  <span class="p">},</span>
  <span class="nt">&#34;model&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;zero_g&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
    <span class="nt">&#34;inter_channels&#34;</span><span class="p">:</span> <span class="mi">192</span><span class="p">,</span>
    <span class="nt">&#34;hidden_channels&#34;</span><span class="p">:</span> <span class="mi">192</span><span class="p">,</span>
    <span class="nt">&#34;filter_channels&#34;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>
    <span class="nt">&#34;n_heads&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="nt">&#34;n_layers&#34;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="nt">&#34;kernel_size&#34;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="nt">&#34;p_dropout&#34;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="nt">&#34;resblock&#34;</span><span class="p">:</span> <span class="s2">&#34;1&#34;</span><span class="p">,</span>
    <span class="nt">&#34;resblock_kernel_sizes&#34;</span><span class="p">:</span> <span class="p">[</span>
      <span class="mi">3</span><span class="p">,</span>
      <span class="mi">7</span><span class="p">,</span>
      <span class="mi">11</span>
    <span class="p">],</span>
    <span class="nt">&#34;resblock_dilation_sizes&#34;</span><span class="p">:</span> <span class="p">[</span>
      <span class="p">[</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">,</span>
        <span class="mi">5</span>
      <span class="p">],</span>
      <span class="p">[</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">,</span>
        <span class="mi">5</span>
      <span class="p">],</span>
      <span class="p">[</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">,</span>
        <span class="mi">5</span>
      <span class="p">]</span>
    <span class="p">],</span>
    <span class="nt">&#34;upsample_rates&#34;</span><span class="p">:</span> <span class="p">[</span>
      <span class="mi">8</span><span class="p">,</span>
      <span class="mi">8</span><span class="p">,</span>
      <span class="mi">2</span><span class="p">,</span>
      <span class="mi">2</span>
    <span class="p">],</span>
    <span class="nt">&#34;upsample_initial_channel&#34;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="nt">&#34;upsample_kernel_sizes&#34;</span><span class="p">:</span> <span class="p">[</span>
      <span class="mi">16</span><span class="p">,</span>
      <span class="mi">16</span><span class="p">,</span>
      <span class="mi">4</span><span class="p">,</span>
      <span class="mi">4</span>
    <span class="p">],</span>
    <span class="nt">&#34;gin_channels&#34;</span><span class="p">:</span> <span class="mi">256</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div><hr>
<p>整体推理过程 和 音色转换过程如图代码所示：</p>
<p><img src="https://raw.githubusercontent.com/weedge/mypic/master/multimoding/voices/open_voice_inference/3.jpg" alt=""></p>
<hr>
<p>Reference encoder: ReferenceEncoder (当模型配置为n_speakers==0的时候使用)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">  (ref_enc): ReferenceEncoder(
    (convs): ModuleList(
      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (gru): GRU(1152, 128, batch_first=True)
    (proj): Linear(in_features=128, out_features=256, bias=True)
    (layernorm): LayerNorm((513,), eps=1e-05, elementwise_affine=True)
  )
</code></pre></div><p>forward:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec_channels</span><span class="p">)</span>  <span class="c1"># [N, 1, Ty, n_freqs]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">conv</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="c1"># out = wn(out)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># [N, 128, Ty//2^K, n_mels//2^K]</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [N, Ty//2^K, 128, n_mels//2^K]</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [N, Ty//2^K, 128*n_mels//2^K]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">flatten_parameters</span><span class="p">()</span>
    <span class="n">memory</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># out --- [1, N, 128]</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div><hr>
<p>encoder: PosteriorEncoder</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">  (enc_q): PosteriorEncoder(
    (pre): Conv1d(513, 192, kernel_size=(1,), stride=(1,))
    (enc): WN(
      (in_layers): ModuleList(
        (0-15): 16 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))
      )
      (res_skip_layers): ModuleList(
        (0-14): 15 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))
        (15): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
      )
      (drop): Dropout(p=0, inplace=False)
      (cond_layer): Conv1d(256, 6144, kernel_size=(1,), stride=(1,))
    )
    (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))
  )
</code></pre></div><p>forward:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_lengths</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">x_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">commons</span><span class="o">.</span><span class="n">sequence_mask</span><span class="p">(</span><span class="n">x_lengths</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_mask</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_mask</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_mask</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">logs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">tau</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logs</span><span class="p">))</span> <span class="o">*</span> <span class="n">x_mask</span>
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">logs</span><span class="p">,</span> <span class="n">x_mask</span>
</code></pre></div><hr>
<p>flow: ResidualCouplingBlock</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">  (flow): ResidualCouplingBlock(
    (flows): ModuleList(
      (0): ResidualCouplingLayer(
        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))
        (enc): WN(
          (in_layers): ModuleList(
            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))
          )
          (res_skip_layers): ModuleList(
            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))
            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          )
          (drop): Dropout(p=0, inplace=False)
          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))
        )
        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))
      )
      (1): Flip()
      (2): ResidualCouplingLayer(
        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))
        (enc): WN(
          (in_layers): ModuleList(
            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))
          )
          (res_skip_layers): ModuleList(
            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))
            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          )
          (drop): Dropout(p=0, inplace=False)
          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))
        )
        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))
      )
      (3): Flip()
      (4): ResidualCouplingLayer(
        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))
        (enc): WN(
          (in_layers): ModuleList(
            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))
          )
          (res_skip_layers): ModuleList(
            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))
            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          )
          (drop): Dropout(p=0, inplace=False)
          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))
        )
        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))
      )
      (5): Flip()
      (6): ResidualCouplingLayer(
        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))
        (enc): WN(
          (in_layers): ModuleList(
            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))
          )
          (res_skip_layers): ModuleList(
            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))
            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          )
          (drop): Dropout(p=0, inplace=False)
          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))
        )
        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))
      )
      (7): Flip()
    )
  )
</code></pre></div><p>forward:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_mask</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">reverse</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">flows</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">flow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_mask</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">flows</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_mask</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div><hr>
<p>decoder: Generator (HIFI-GAN Generator)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">  (dec): Generator(
    (conv_pre): Conv1d(192, 512, kernel_size=(7,), stride=(1,), padding=(3,))
    (ups): ModuleList(
      (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))
      (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))
      (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))
      (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))
    )
    (resblocks): ModuleList(
      (0): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
      )
      (1): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))
          (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
          (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))
        )
      )
      (2): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))
          (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
          (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))
        )
      )
      (3): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        )
      )
      (4): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))
          (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
          (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))
        )
      )
      (5): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))
          (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
          (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))
        )
      )
      (6): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
        )
      )
      (7): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
          (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
          (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
        )
      )
      (8): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
          (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
          (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
        )
      )
      (9): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
        )
      )
      (10): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
          (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
          (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
        )
      )
      (11): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
          (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
          (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
        )
      )
    )
    (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)
    (cond): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
  )
</code></pre></div><p>forward:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_pre</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_upsamples</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">modules</span><span class="o">.</span><span class="n">LRELU_SLOPE</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ups</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        <span class="n">xs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">xs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span> <span class="o">+</span> <span class="n">j</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">xs</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span> <span class="o">+</span> <span class="n">j</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_post</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>
</code></pre></div><p>HIFI-GAN generator 结构</p>
<p><img src="https://raw.githubusercontent.com/weedge/mypic/master/multimoding/voices/open_voice_inference/4.jpg" alt=""></p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>CoquiAI. Xtts taking text-to-speech to the next level. Technical Blog, 2023.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. Gölge, and M. A. Ponti. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone. In International Conference on Machine Learning, pages 2709–2720. PMLR, 2022.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, et al. Voicebox: Text-guided multilingual universal speech generation at scale. arXiv preprint arXiv:2306.15687, 2023.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Z. Zhang, L. Zhou, C. Wang, S. Chen, Y. Wu, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, et al. Speak foreign languages with your own voice: Cross-lingual neural codec language modeling. arXiv preprint arXiv:2303.03926, 2023.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>J. Kim, J. Kong, and J. Son. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. In International Conference on Machine Learning, pages 5530–5540. PMLR, 2021.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>D. Yang, S. Liu, R. Huang, G. Lei, C. Weng, H. Meng, and D. Yu. Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt. arXiv preprint arXiv:2301.13662, 2023.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>D. Rezende and S. Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530–1538. PMLR, 2015.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>I. P. Association. Handbook of the International Phonetic Association: A guide to the use of the International Phonetic Alphabet. Cambridge University Press, 1999.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>J. Kong, J. Kim, and J. Bae. HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in Neural Information Processing Systems, 33:17022–17033, 2020.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p>B. van Niekerk, M.-A. Carbonneau, J. Zaïdi, M. Baas, H. Seuté, and H. Kamper. A comparison of discrete and soft speech units for improved voice conversion. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6562–6566. IEEE, 2022.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12" role="doc-endnote">
<p>A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux. Speech resynthesis from discrete disentangled self-supervised representations. arXiv preprint arXiv:2104.00355, 2021.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13" role="doc-endnote">
<p>J. Kim, S. Kim, J. Kong, and S. Yoon. Glow-tts: A generative flow for text-to-speech via monotonic alignment search. Advances in Neural Information Processing Systems, 33:8067–8077, 2020.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14" role="doc-endnote">
<p>W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed. HuBERT: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451–3460, 2021.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15" role="doc-endnote">
<p>J. Li, W. Tu, and L. Xiao. Freevc: Towards high-quality text-free one-shot voice conversion. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE, 2023.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16" role="doc-endnote">
<p>A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17" role="doc-endnote">
<p>P. Senin. Dynamic time warping algorithm review. Information and Computer Science Department University of Hawaii at Manoa Honolulu, USA, 855(1-23):40, 2008.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18" role="doc-endnote">
<p>M. Müller. Dynamic time warping. Information retrieval for music and motion, pages 69–84, 2007.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
    </div>

    
    
<div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">weedge</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
      2024-05-11
      
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">许可协议</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>


    
    

    <footer class="post-footer">
      <div class="post-tags">
          <a href="https://weedge.github.io/tags/multimoding/">multimoding</a>
          <a href="https://weedge.github.io/tags/voice/">voice</a>
          <a href="https://weedge.github.io/tags/openvoice/">openvoice</a>
          <a href="https://weedge.github.io/tags/mel-spectrogram/">mel-spectrogram</a>
          
        </div>

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/multimoding/voices/vits/">
            
            <i class="iconfont">
              <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

            </i>
            <span class="prev-text nav-default">论文解读 VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        
          <a class="next" href="/post/paper/transformer/delight/">
            <span class="next-text nav-default">论文解读 DeLighT: Very Deep and Light-weight Transformers</span>
            <span class="prev-text nav-mobile">下一篇</span>
            
            <i class="iconfont">
              <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

            </i>
          </a>
      </nav>
    </footer>
  </article>

  
  

  
  

  

  
  

  

  

  <div class="disqus-comment">
  <div class="disqus-button" id="load_disqus" onclick="load_disqus()">
    显示 Disqus 评论
  </div>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = "https://weedge.github.io/post/multimoding/voices/open_voice_extra_se_and_convert/";
    };
    function load_disqus() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'weedge';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

      $('#load_disqus').remove();
    };
  </script>
  <noscript>Please enable JavaScript to view the
    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  </div>

    

  

        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="icon-links">
  
  
    <a href="mailto:weege007@gmail.com" rel="me noopener" class="iconfont"
      title="email" >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://github.com/weedge" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://weibo.com/weedge" rel="me noopener" class="iconfont"
      title="weibo"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M385.714286 733.714286q12-19.428571 6.285714-39.428571t-25.714286-28.571429q-19.428571-8-41.714286-0.571429t-34.285714 26.285714q-12.571429 19.428571-7.428571 39.142857t24.571429 28.857143 42.571429 1.428571 35.714286-27.142857zm53.714286-69.142857q4.571429-7.428571 2-15.142857t-10-10.571429q-8-2.857143-16.285714 2.857143t-12.285714 10.571429q-9.714286 17.714286 7.428571 25.714286 8 2.857143 16.571429 2.857143t12.571429-10.571429zm99.428571 61.142857q-25.714286 58.285714-90.285714 85.714286t-128 6.857143q-61.142857-19.428571-84.285714-72.285714t3.714286-107.142857q26.857143-53.142857 86.571429-79.428571t120.285714-10.857143q63.428571 16.571429 90.571429 68.285714t1.428571 108.857143zm178.285714-91.428571q-5.142857-54.857143-50.857143-97.142857t-119.142857-62.285714-156.857143-12q-127.428571 13.142857-211.142857 80.857143t-75.714286 151.142857q5.142857 54.857143 50.857143 97.142857t119.142857 62.285714 156.857143 12q127.428571-13.142857 211.142857-80.857143t75.714286-151.142857zm176 2.285714q0 38.857143-21.142857 79.714286t-62.285714 78.285714-96.285714 67.142857-129.142857 47.428571-154.571429 17.714286-157.142857-19.142857-137.428571-53.142857-98-86.285714-37.142857-114q0-65.714286 39.714286-140t112.857143-147.428571q96.571429-96.571429 195.142857-134.857143t140.857143 4q37.142857 36.571429 11.428571 119.428571-2.285714 8-0.571429 11.428571t5.714286 4 8.285714 2.857143 7.714286-2l3.428571-1.142857q79.428571-33.714286 140.571429-33.714286t87.428571 34.857143q25.714286 36 0 101.714286-1.142857 7.428571-2.571429 11.428571t2.571429 7.142857 6.857143 4.285714 9.714286 3.428571q32.571429 10.285714 58.857143 26.857143t45.714286 46.571429 19.428571 66.571429zm-42.285714-356.571429q24 26.857143 31.142857 62t-3.714286 67.142857q-4.571429 13.142857-16.857143 19.428571t-25.428571 2.285714q-13.142857-4.571429-19.428571-16.857143t-2.285714-25.428571q11.428571-36-13.714286-63.428571t-61.142857-20q-13.714286 2.857143-25.714286-4.571429t-14.285714-21.142857q-2.857143-13.714286 4.571429-25.428571t21.142857-14.571429q34.285714-7.428571 68 3.142857t57.714286 37.428571zm103.428571-93.142857q49.714286 54.857143 64.285714 127.142857t-7.714286 138q-5.142857 15.428571-19.428571 22.857143t-29.714286 2.285714-22.857143-19.428571-2.857143-29.714286q16-46.857143 5.714286-98.285714t-45.714286-90.285714q-35.428571-39.428571-84.571429-54.571429t-98.857143-4.857143q-16 3.428571-29.714286-5.428571t-17.142857-24.857143 5.428571-29.428571 24.857143-16.857143q70.285714-14.857143 139.428571 6.571429t118.857143 76.857143z"></path>
</svg>

    </a>


<a href="https://weedge.github.io/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
   
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2013 -
    2025
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        weedge
        
      </span></span>

  
  

  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont">
        
        <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

      </i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js" integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin="anonymous"></script>












  
    <script type="text/javascript" src="/js/load-photoswipe.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  









  <script id="dsq-count-scr" src="//weedge.disqus.com/count.js" async></script>






  <script src="/js/copy-to-clipboard.js"></script>


</body>
</html>
