<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 时间飘过</title>
    <link>https://weedge.github.io/post/</link>
    <description>Recent content in Posts on 时间飘过</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 17 Jan 2025 10:26:23 +0800</lastBuildDate><atom:link href="https://weedge.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>论文解读：CosyVoice2: Scalable Streaming Speech Synthesis with Large Language Models</title>
      <link>https://weedge.github.io/post/multimoding/voices/cosyvoice2/</link>
      <pubDate>Fri, 17 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/cosyvoice2/</guid>
      <description>&lt;h2 id=&#34;cosyvoice2&#34;&gt;CosyVoice2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.10117&#34;&gt;2024.12  &lt;strong&gt;CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models&lt;/strong&gt;&lt;/a&gt;（流式合成）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/FunAudioLLM/CosyVoice&#34;&gt;paper code&lt;/a&gt;: 公开推理和权重，训练过程需要在CosyVoice的基础上修改下。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;zero-shot-tts-models-零样本-tts-模型&#34;&gt;zero-shot TTS models 零样本 TTS 模型&lt;/h2&gt;
&lt;h3 id=&#34;codec-language-models-编解码器语言模型&#34;&gt;codec language models 编解码器语言模型&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;speech &lt;strong&gt;codec model&lt;/strong&gt;  to extract discrete speech representation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.03312&#34;&gt;2021.7 SoundStream: An End-to-End Neural Audio Codec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.13438&#34;&gt;2022.10 &lt;strong&gt;High Fidelity Neural Audio Compression&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;http://github.com/facebookresearch/encodec&#34;&gt;facebookresearch/encodec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.07405&#34;&gt;2023.10 FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec&lt;/a&gt; | &lt;a href=&#34;https://github.com/modelscope/FunCodec&#34;&gt;modelscope/FunCodec&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;speech &lt;strong&gt;codec model&lt;/strong&gt; + &lt;strong&gt;autoregressive model&lt;/strong&gt; to predict the speech tokens (acoustic tokens):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.02111&#34;&gt;2023.1 &lt;strong&gt;Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers&lt;/strong&gt;&lt;/a&gt; (Vall-E) | 以及后续 Vall-E 升级系列 (不包括MELLE): &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/vall-e-x/&#34;&gt;https://www.microsoft.com/en-us/research/project/vall-e-x/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.03540&#34;&gt;2023.2 Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;speech &lt;strong&gt;codec model&lt;/strong&gt; (speech semantics Codec) +  &lt;strong&gt;non-autoregressive masked model&lt;/strong&gt; to predict the speech tokens (acoustic tokens):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.00750&#34;&gt;2024.9 &lt;strong&gt;MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/Amphion/tree/main/models/tts/maskgct&#34;&gt;paper code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;codec model (speech acoustic Codec) or  &lt;strong&gt;vocoder&lt;/strong&gt; to synthesize waveforms from mel-spectrograms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.00814&#34;&gt;2023.6 &lt;strong&gt;Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/gemelo-ai/vocos&#34;&gt;paper code&lt;/a&gt; | 推理速度快：运行速度比 HiFi-GAN 快约 13 倍，比 BigVGAN 快近 70 倍。在没有 GPU 加速的情况下运行时，这种速度优势尤其明显。这主要是由于使用了短时傅里叶逆变换（ISTFT）算法而不是转置卷积。还评估了 Vocos 的一个变体，它利用 ResBlock 的扩张卷积而不是 ConvNeXt 块。在 GPU 上执行时，深度可分离卷积可提供额外的加速。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10389765&#34;&gt;2023.12 &lt;strong&gt;WaveNeXt: ConvNeXt-Based Fast Neural Vocoder Without ISTFT layer&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://ast-astrec.nict.go.jp/demo_samples/asru_2023_okamoto/index.html&#34;&gt;demo samples&lt;/a&gt; | paper code基于 &lt;a href=&#34;https://arxiv.org/abs/2110.07840&#34;&gt;ESPNet2-TTS&lt;/a&gt;  | 一种新型的基于ConvNeXt的快速神经声码器WaveNeXt，它通过替换Vocos中的逆短时傅里叶变换（iSTFT）层为可训练的线性层，直接预测语音波形样本，而不依赖于STFT频谱。这一改进不仅保持了Vocos的快速推理速度，还提高了语音合成的质量。文章还探讨了如何将WaveNeXt与基于JETS的端到端文本到语音（E2E TTS）框架集成，并研究了采样频率为48kHz的全带模型（Full-band Model：能够处理和生成覆盖整个音频频谱范围的模型，通常是指能够处理从最低频到最高频的完整音频信号的模型）。实验结果表明，WaveNeXt在分析-合成和E2E TTS条件下均优于Vocos，同时保持了快速推理的能力。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/4a1eeaff-528f-4706-b5fc-210caea2c13b&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-diffusion-models-特征扩散模型&#34;&gt;feature diffusion models 特征扩散模型&lt;/h3&gt;
&lt;p&gt;DDPM + CFM + NAR(non-autoregressive) model, 没有 codec&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Base module:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Denoising Diffusion Probabilistic Model(DDPM)： &lt;a href=&#34;https://arxiv.org/abs/2006.11239&#34;&gt;2020.6 &lt;strong&gt;Denoising Diffusion Probabilistic Models&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/hojonathanho/diffusion&#34;&gt;paper code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conditional Flow Matching (CFM)：  &lt;a href=&#34;https://arxiv.org/abs/2210.02747&#34;&gt;2022.10 &lt;strong&gt;Flow Matching for Generative Modeling&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/atong01/conditional-flow-matching&#34;&gt;CFM lib&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;the alignment modeling between input text and synthesized speech&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;phoneme-level duration model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.03100&#34;&gt;2024.5 NaturalSpeech 3&lt;/a&gt;  and &lt;a href=&#34;https://arxiv.org/abs/2306.15687&#34;&gt;2023.6 Voicebox&lt;/a&gt; use frame-wise phoneme alignment;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.03199&#34;&gt;2023.9 Matcha-TTS&lt;/a&gt; adopts monotonic alignment search(MAS) and relies on phoneme-level duration model;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.18009&#34;&gt;2024.6 E2 TTS&lt;/a&gt; 和&lt;a href=&#34;https://arxiv.org/abs/2406.02430&#34;&gt;2024.6 Seed-TTS&lt;/a&gt; 研究表明在文本和语音之间引入这种僵化和不灵活的对齐方式会阻碍模型生成更自然的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;E3 TTS 放弃音素级持续时间并对输入序列应用交叉注意力，但产生的音频质量有限；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DiTTo-TTS 使用扩散变换器 (DiT) ，并以来自预训练语言模型的编码文本为条件进行交叉注意。为了进一步增强对齐，它使用预训练的语言模型来微调神经音频编解码器，将语义信息注入生成的表示中；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;相比之下，基于 Voicebox的 E2 TTS采用了更简单的方法，删除了音素和持续时间预测器，直接使用填充token填充到梅尔频谱图长度的字符作为输入。这个简单的方案也实现了非常自然和真实的合成结果。然而，F5-TTS 发现 E2 TTS 中文本和语音对齐存在鲁棒性问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.02430&#34;&gt;2024.6 Seed-TTS&lt;/a&gt; 采用了类似的策略并取得了优异的结果，尽管没有详细说明模型细节。在这些未明确建模音素级持续时间的方法中，模型学习根据给定的总序列长度分配每个单词或音素的长度，从而改进韵律和节奏。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.06885&#34;&gt;2024.10 &lt;strong&gt;F5-TTS: A fairytaler that fakes fluent and faithful speech with flow matching&lt;/strong&gt;&lt;/a&gt; 保持了管道的简单性，无需音素对齐、持续时间预测器、文本编码器和语义注入编解码器模型，利用带有 &lt;a href=&#34;https://arxiv.org/abs/2301.00808&#34;&gt;ConvNeXt V2&lt;/a&gt;|&lt;a href=&#34;https://github.com/facebookresearch/ConvNeXt-V2&#34;&gt;paper code&lt;/a&gt; 的Diffusion Transformer(DiT)来更好地解决上下文学习期间的文本语音对齐问题。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;codec-language-and-feature-diffusion-hybrid-systems-混合系统&#34;&gt;codec language and feature diffusion hybrid systems 混合系统&lt;/h3&gt;
&lt;p&gt;text-to-codec language model  和 codec-to-feature diffusion model&lt;/p&gt;
&lt;p&gt;语言模型解决文本和语音之间的对齐以及话语持续时间预测，而编解码器到特征扩散模型则根据生成的编解码器和其他条件合成语音特征（梅尔谱）。通过利用两种生成模型的优势，混合系统实现了高度多样性、韵律一致性和语音质量。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.02430&#34;&gt;2024.6 Seed-TTS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.05407&#34;&gt;2024.7 Cosyvoice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.03283&#34;&gt;2024.9 Fireredtts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;language-model-based-zero-shot-tts-models--streaming-synthesis&#34;&gt;language model-based zero-shot TTS models  &lt;strong&gt;streaming&lt;/strong&gt; synthesis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.08093&#34;&gt;2024.2 &lt;strong&gt;BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data&lt;/strong&gt;&lt;/a&gt; | 小红书的FireRedTTS 来源于此 &lt;a href=&#34;https://github.com/FireRedTeam/FireRedTTS&#34;&gt;FireRedTTS paper code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.02897&#34;&gt;2024.6 LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.11210&#34;&gt;2024.9 Speak While You Think: Streaming Speech Synthesis During Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.00767&#34;&gt;2024.10 Zero-Shot Text-to-Speech from Continuous Text Streams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>论文解读：CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens</title>
      <link>https://weedge.github.io/post/multimoding/voices/cosyvoice/</link>
      <pubDate>Wed, 15 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/cosyvoice/</guid>
      <description>&lt;h2 id=&#34;cosyvoice&#34;&gt;CosyVoice&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.04051&#34;&gt;2024.7 FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs&lt;/a&gt; （主要介绍ASR SenseVoice 和 TTS CosyVoice,其中 SenseVoice 没有单独论文，相关CosyVoice 和单独论文是重复的, SenseVoice Large的工作可以用于 CosyVoice 在多语言上， Supervised speech tokenizer 模块的训练和推理）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.05407&#34;&gt;2024.7 &lt;strong&gt;CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.10117&#34;&gt;2024.12 CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models&lt;/a&gt; （流式合成）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/FunAudioLLM/CosyVoice&#34;&gt;paper code&lt;/a&gt;: 公开推理和权重，在openslr公开数据集英文数据集LibriSpeech 和中文数据集 MAGICDATA 对模型进行训练代码； 无supervised Speech Tokenizer (对SenseVoice ASR的改造微调) 和Speaker Embedding model(context-aware masking CAM++) 的训练过程代码。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;创新点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将监督语音token集成到TTS 模型，增强了零样本语音克隆中的内容一致性和说话者相似性。&lt;/li&gt;
&lt;li&gt;一种可扩展的零样本 TTS 合成系统，它将用于文本到token生成的 LLM 与用于token到语音合成的条件流匹配模型(conditional flow matching model(CFM))相结合，不依赖于音素持续时间预测(Duration predictor)，不需要使用补充音素器(phonemizers)和强制对齐器aligners (比如：Glow-TTS中 Monotonic Alignment Search(MAS))。&lt;/li&gt;
&lt;li&gt;为了进一步细化生成语音的质量，将 x-vector 合并到 LLM 中，将语音建模分为语义、说话者和韵律(semantic, speaker, and prosody)组件。 LLM 对语义(semantic)内容和韵律(prosody)进行建模，而条件流匹配模型(CFM)则捕获音色(timbre)和环境信息。我们使用&lt;strong&gt;Classifier-Free Guidance&lt;/strong&gt;(&lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;2022. &lt;strong&gt;Classifier-free diffusion guidance&lt;/strong&gt;&lt;/a&gt;)、余弦调度器(&lt;a href=&#34;https://d2l.ai/chapter_optimization/lr-scheduler.html#cosine-scheduler&#34;&gt;cosine scheduler&lt;/a&gt;)和屏蔽条件(masked conditions)等技术来优化流匹配过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/3e8c1132-e146-4b73-8d0c-f3972bf7c8bd&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;CosyVoice由四个组件组成，即文本编码器(text encoder)、语音分词器(speech tokenizer)、大语言模型(large language model)和条件流匹配模型(conditional flow matching model)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本编码器(text encoder)用于对齐文本和语音token的语义空间;&lt;/li&gt;
&lt;li&gt;语音标记器(speech tokenizer)用于提取语义token;&lt;/li&gt;
&lt;li&gt;LLM(GLM)学习文本编码和语音标记的整个序列，将 TTS 重新表述为以文本作为提示的自回归序列生成问题;&lt;/li&gt;
&lt;li&gt;利用条件流匹配模型(conditional flow matching model), 通过最优路径上的去噪处理,将语音标记转换为梅尔谱图(Mel spectrogram); 通过&lt;strong&gt;Classifier-Free Guidance&lt;/strong&gt;（Classifier-free diffusion guidance CFG）提高扩散概率模型的生成质量, 将CFG适应到条件流匹配模型中;&lt;/li&gt;
&lt;li&gt;获得人类耳朵可感知的声音信号，声码器(vocoder)使用 Hifi-GAN Generator 用于将生成的梅尔频谱图作为输入来合成波形(waveform)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;conditional flow matching model (OT-CFM) 来自 &lt;a href=&#34;https://arxiv.org/abs/2309.03199&#34;&gt;2023.9 &lt;strong&gt;Matcha-TTS: A fast TTS architecture with conditional flow matching&lt;/strong&gt;&lt;/a&gt;(CFM的改进版本OT-CFM)&lt;/li&gt;
&lt;li&gt;Classifier-free diffusion guidance (CFG) 来自 &lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;2022. &lt;strong&gt;Classifier-free diffusion guidance&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;vocoder 来自 &lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;2020. &lt;strong&gt;HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/strong&gt;&lt;/a&gt; Generator。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;附：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;推理和训练操作笔记：https://github.com/weedge/doraemon-nb/blob/main/CosyVoice.ipynb&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;achatbot 接入 CosyVoice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ai-bot-pro/achatbot/pull/21&#34;&gt;https://github.com/ai-bot-pro/achatbot/pull/21&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ai-bot-pro/achatbot/pull/23&#34;&gt;https://github.com/ai-bot-pro/achatbot/pull/23&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>论文解读 Matcha-TTS: A fast TTS architecture with conditional flow matching</title>
      <link>https://weedge.github.io/post/multimoding/voices/matcha-tts/</link>
      <pubDate>Tue, 14 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/matcha-tts/</guid>
      <description>&lt;h1 id=&#34;-matcha-tts&#34;&gt;🍵 matcha-tts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.03199&#34;&gt;2023.9 &lt;strong&gt;Matcha-TTS: A fast TTS architecture with conditional flow matching&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/shivammehta25/Matcha-TTS&#34;&gt;paper code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要：&lt;/h1&gt;
&lt;p&gt;一种用于快速 TTS 声学建模的新编码器-解码器架构，使用最佳传输条件流匹配 (OT-CFM) 进行训练。与使用分数匹配训练的模型相比，这产生了基于 ODE 的解码器，能够以更少的合成步骤实现高输出质量。仔细的设计选择还确保每个合成步骤都能快速运行。该方法是概率性的、非自回归的，并且无需外部对齐即可从头开始学习说话。与强大的预训练基线模型相比，Matcha-TTS 系统具有最小的内存占用，可以与长语音上最快的模型相媲美，并在听力测试中获得最高的平均意见得分。&lt;/p&gt;
&lt;p&gt;一种非自回归神经 TTS 的新方法，它使用条件流匹配(CFM from &lt;a href=&#34;https://arxiv.org/abs/2210.02747&#34;&gt;2022.10 &lt;strong&gt;Flow Matching for Generative Modeling&lt;/strong&gt;&lt;/a&gt;)（类似于校正流(Rectified Flow &lt;a href=&#34;https://arxiv.org/abs/2209.03003&#34;&gt;2022.9 Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow&lt;/a&gt;）来加速基于 ODE 的语音合成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is probabilistic  是概率性的&lt;/li&gt;
&lt;li&gt;Has compact memory footprint 具有紧凑的内存占用&lt;/li&gt;
&lt;li&gt;Sounds highly natural  听起来非常自然&lt;/li&gt;
&lt;li&gt;Is very fast to synthesise from 合成速度非常快&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简洁的结构，训练推理快，使用更少额内存空间，&lt;/p&gt;
&lt;p&gt;一种基于连续归一化流的概率性、非自回归、快速采样的 TTS 声学模型。主要有两个创新点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提出了一种改进的编码器-解码器 TTS 结构，该结构在解码器中结合使用 1D CNN 和 Transformer。这减少了内存消耗并且可以快速评估，从而提高综合速度。&lt;/li&gt;
&lt;li&gt;使用最优传输条件流匹配 optimal-transport conditional flow matching(OT-CFM) 来训练这些模型，这是一种学习从数据分布中采样的 ODE 的新方法。与传统的 连续时间归一化流 CNF（continuous-time normalising flows ） 和分数匹配概率流 ODE （probability flow ODE） 相比，OT-CFM 定义了从源到目标的更简单的路径，从而能够以比 DPM（Diffusion probabilistic model） 更少的步骤进行准确合成。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;实验结果表明，这两种创新都加速了合成，减少了速度和合成质量之间的权衡。尽管速度快且轻量级，Matcha-TTS能够在不需要外部对齐器的情况下学习说话和对齐。与强大的预训练基线模型相比，Matcha-TTS实现了快速合成，并获得了更好的自然度评分。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;附&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;使用LJ Speech数据集训练202 epochs, 在1xGPU 上训练了 79,372 step 的ckpt(论文中是2x3080GPU 50K step)：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/weege007/matchaTTS/tree/main&#34;&gt;https://huggingface.co/weege007/matchaTTS/tree/main&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;笔记地址： &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/matcha_tts.ipynb&#34;&gt;https://github.com/weedge/doraemon-nb/blob/main/matcha_tts.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/95bccbf2-b164-484f-a6c9-8ab7e235c018&#34; alt=&#34;loss&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>论文解读： VALL-E 系列</title>
      <link>https://weedge.github.io/post/multimoding/voices/vall-e-x/</link>
      <pubDate>Mon, 13 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/vall-e-x/</guid>
      <description>&lt;p&gt;前文讲到VITS，采用的端到端的NAR模型，这篇文章记录下微软提出的VALL-E系列，从 AR+NAR 到 AR 模型的转变，以及后面MELLE引入的VAE和Mel-Spectorgram，将neural codec text speech LM (AR+NAR Transformer Decoder) 转变为  autoregressive  mel-spectrogram text speech LM  (AR Transformer Decoder) ；由于LM生成的是mel-spectrogram 需要通过vocoder 转换成 waveform； 生成的内容采样模块：从top-p random sampling 变成 Latent Sampling潜在采样模块（思想源自VAE, 从预测的高斯分布中采样潜在嵌入，然后将其投影回频谱图空间）&lt;/p&gt;
&lt;h2 id=&#34;vall-e-系列&#34;&gt;VALL-E 系列&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/project/vall-e-x/&#34;&gt;https://www.microsoft.com/en-us/research/project/vall-e-x/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vall-E: &lt;a href=&#34;https://ar5iv.labs.arxiv.org/html/2301.02111&#34;&gt;https://ar5iv.labs.arxiv.org/html/2301.02111&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;具体来说，我们使用从现成的神经音频编解码器模型派生的离散代码来训练&lt;em&gt;&lt;strong&gt;神经编解码器语言模型（neural codec language model）&lt;/strong&gt;&lt;/em&gt;（称为VALL-E ），并将 TTS 视为条件语言建模任务，而不是像之前的工作那样将 TTS 视为连续信号回归。 在预训练阶段，我们将 TTS 训练数据扩展到 60K 小时的英语语音，比现有系统大数百倍。 VALL-E具有情境学习功能，可用于合成高质量的个性化语音，只需对看不见的说话者进行 3 秒的注册录音作为声音提示。实验结果表明， VALL-E在语音自然度和说话人相似度方面显着优于最先进的零样本 TTS 系统。此外，我们发现VALL-E可以在合成时保留说话者的情感和声音提示的声学环境。&lt;/p&gt;
&lt;p&gt;与之前的管道不同（例如，音素 → 梅尔谱图 → 波形）， VALL-E的管线是音素 → 离散码 → 波形。&lt;/p&gt;
&lt;p&gt;VALL-E根据音素和声学代码提示生成与目标内容和说话者的声音相对应的离散音频编解码器代码。 VALL-E直接支持各种语音合成应用，例如零样本 TTS、语音编辑以及与 GPT-3 等其他生成式 AI 模型相结合的内容创建。&lt;/p&gt;
&lt;p&gt;VALL-E系列：2023年的1月份开始 - 2024年的7月份&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VALL-E 使用从现成的神经音频编解码器模型派生的离散代码来训练神经编解码器语言模型，并将 TTS 视为条件语言建模任务，而不是像之前的工作那样将 TTS 视为连续信号回归。 VALL-E 具有情境学习功能，可用于合成高质量的个性化语音，只需录制未见过的讲话者的 3 秒注册录音作为提示。在语音自然度和说话人相似度方面，VALL-E 显着优于最先进的零样本 TTS 系统。此外，VALL-E可以在合成时保留说话者的情绪和声音提示的声学环境。&lt;/li&gt;
&lt;li&gt;VALL-E X 扩展其能力，适应多语言场景，促进跨语言零样本 TTS。&lt;/li&gt;
&lt;li&gt;VALL-E R 引入了音素单调对齐策略，增强了语音生成的鲁棒性。&lt;/li&gt;
&lt;li&gt;VALL-E 2 通过集成重复感知采样和分组代码建模技术， 实现了一个突破性的里程碑：在 LibriSpeech 和 VCTK 数据集上的零样本 TTS 性能与人类相当。这标志着此类成就的首次实例，为该领域树立了新标准。&lt;/li&gt;
&lt;li&gt;MELLE 是一种新颖的基于连续值标记的语言建模方法，用于文本到语音合成 (TTS)。 MELLE 直接从文本条件自回归生成连续的梅尔频谱图帧，绕过了矢量量化的需要，矢量量化最初是为音频压缩而设计的，与梅尔频谱图相比，牺牲了保真度。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>论文解读 VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</title>
      <link>https://weedge.github.io/post/multimoding/voices/vits/</link>
      <pubDate>Sat, 11 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/vits/</guid>
      <description>&lt;p&gt;前文讲到OpenVoicev2，补充下细节，然后梳理使用的基础模型VITS：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;📓&lt;/p&gt;
&lt;p&gt;melo-tts 生成原始音频：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenVoice 版本1不依赖melo-tts, 升级后的V2版本依赖melo-tts, 主要是生成原始音频质量加强了(由melo-tts生成);&lt;/li&gt;
&lt;li&gt;默认配置使用了TransformerCouplingBlock list作为flow 和 reverse flow, 而第一版的OpenVoice 模型使用的 ResidualCouplingBlock ;&lt;/li&gt;
&lt;li&gt;melo-tts的模型权重支持多语言，更具语言区分，比如 ZH: &lt;a href=&#34;https://huggingface.co/myshell-ai/MeloTTS-Chinese&#34;&gt;myshell-ai/MeloTTS-Chinese&lt;/a&gt;, EN_NEWEST: &lt;a href=&#34;https://huggingface.co/myshell-ai/MeloTTS-English-v3&#34;&gt;myshell-ai/MeloTTS-English-v3&lt;/a&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;音色转换生成目标音频：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过训练好的&lt;strong&gt;音色抽取器&lt;/strong&gt;抽取目标说话者的音色 (&lt;a href=&#34;https://huggingface.co/myshell-ai/OpenVoiceV2/tree/main/base_speakers/ses&#34;&gt;myshell-ai/OpenVoiceV2/converter&lt;/a&gt;)；&lt;/li&gt;
&lt;li&gt;生成的原始音频信息通过 训练抽取好的基础说话者的音色(&lt;a href=&#34;https://huggingface.co/myshell-ai/OpenVoiceV2/tree/main/converter&#34;&gt;myshell-ai/OpenVoiceV2/base_speakers/ses&lt;/a&gt;)，将原始音频中的音色去除 （flow）；&lt;/li&gt;
&lt;li&gt;将去除原始音色的音频 和 抽取好的目标说话者的音色 合并 （reverse flow）； 最终通过 vocoder(也是论文中的Decoder,使用的 HiFi-Gan模型)合成目标音频。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;额外注意的是，由melo-tts生成原始音频sample rate是 44100， 而通过音色提取器 提取 并且 生成目标音频sample rate是 22050&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;前提知识这里简单概括如下：&lt;/p&gt;
&lt;p&gt;AE(Autoencoder): 自编码器是&lt;a href=&#34;https://www.ibm.com/cn-zh/topics/self-supervised-learning&#34;&gt;自监督&lt;/a&gt;系统，其训练目标是通过降维来压缩（或&lt;em&gt;编码&lt;/em&gt;）输入数据，然后使用该压缩后的表示准确重建（或&lt;em&gt;解码&lt;/em&gt;）其原始输入。无泛化生成能力，但是可以执行特定任务：常用于数据压缩、图像去噪、异常检测和面部识别等任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VAE(Variational Autoencoder)&lt;/strong&gt; :与其他自编码器(Autoencoder(AE)的区别在于它们对潜在空间进行编码的独特方式，以及可以应用其概率编码的不同用例，即随机生成训练数据的变体。具有泛化生成能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CVAE(Conditional Variational Autoencoder)&lt;/strong&gt;: 条件变分自编码器 可以以特定输入为条件进行输出，而不仅仅是随机生成训练数据的变体。这是通过将监督学习（或半监督学习）的元素与常规自编码器的传统无监督训练目标相结合来实现的。具有指定特征的泛化能力。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;VAE 与 GAN的区别：&lt;/p&gt;
&lt;p&gt;VAE 经常与生成式对抗网络 (GAN) 进行比较，GAN 是另一种模型架构，用于生成类似于训练数据的样本，尤其是图像。&lt;/p&gt;
&lt;p&gt;与 VAE 类似，GAN 是结合两种神经网络的联合架构：一个生成器网络，负责输出与训练数据集中的图像相似的图像样本，另一个判别器网络，负责确定特定图像是训练数据中的“真实”图像还是来自生成器网络的“虚假”图像。&lt;/p&gt;
&lt;p&gt;这两个网络在零和博弈中进行对抗性训练：来自判别器的反馈用于改进生成器的输出，直到判别器不再能够区分真假样本。&lt;/p&gt;
&lt;p&gt;就图像合成而言，两者各有优劣：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GAN 可以生成更清晰的图像，但由于两种复合模型之间的对抗性权衡，在训练中并不稳定。&lt;/li&gt;
&lt;li&gt;VAE 更容易训练，但由于其根据训练数据的“平均”特征生成图像的性质，往往会生成比较模糊的图像。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VAE-GAN 两者结合
顾名思义，VAE-GAN 是变分自编码器 (VAE) 和生成式对抗网络 (GAN) 的混合体。通过用判别器网络替换 VAE 模型的重建损失项，来降低 VAE 生成图像的模糊性，提高生成质量。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;VITS 使用了 条件变分自编码器 (Conditional Variational Autoencoder (CVAE)) 和生成式对抗网络 (&lt;a href=&#34;https://en.wikipedia.org/wiki/Generative_adversarial_network&#34;&gt;Generative adversarial network&lt;/a&gt;(GAN)) 两个模型架构。 至于VAE和GAN的细节可以关注下baby-llm这个学习项目中的对应模块PR学习资料:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VAE: &lt;a href=&#34;https://github.com/ai-bot-pro/baby-llm/tree/main/modules/VAE&#34;&gt;https://github.com/ai-bot-pro/baby-llm/tree/main/modules/VAE&lt;/a&gt; | PR:  &lt;a href=&#34;https://github.com/ai-bot-pro/baby-llm/pull/13&#34;&gt;https://github.com/ai-bot-pro/baby-llm/pull/13&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN: &lt;a href=&#34;https://github.com/ai-bot-pro/baby-llm/tree/main/modules/GAN&#34;&gt;https://github.com/ai-bot-pro/baby-llm/tree/main/modules/GAN&lt;/a&gt; | PR: &lt;a href=&#34;https://github.com/ai-bot-pro/baby-llm/pull/12&#34;&gt;https://github.com/ai-bot-pro/baby-llm/pull/12&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这篇文章是讲解VITS，是现在工业上TTS常用的基础方案(NAR模型，成本相对AR模型低， 推理快，生成质量尽可能追平或超越SOTA AR模型)。作者来自韩国现代汽车公司的 AIR 实验室（人工智能研究实验室），论文结合了以前的研究成果：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.02155&#34;&gt;2018. &lt;strong&gt;FloWaveNet : A Generative Flow for Raw Audio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.11129&#34;&gt;2020. &lt;strong&gt;Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;2020. &lt;strong&gt;HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;后续作者还研究了加入扩散模型来生成语音，不需要使用分类器指导的目标说话者的任何转录。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.11755&#34;&gt;2022. Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Guided-TTS 将无条件扩散概率模型(unconditional Diffusion Model)与单独训练的音素分类器(phoneme classifier )相结合，用于分类器指导。无条件扩散模型学习在没有任何上下文的情况下从未转录的语音数据中生成语音。对于 TTS 合成，使用在大规模语音识别数据集上训练的音素分类器来指导扩散模型的生成过程。&lt;/p&gt;
&lt;h1 id=&#34;vits&#34;&gt;VITS&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.06103&#34;&gt;2021. &lt;strong&gt;Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/strong&gt;&lt;/a&gt;  | &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;paper coder&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;主要贡献：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出了一种并行的端到端 TTS 方法，它可以生成比当前两阶段模型更自然的音频；&lt;/li&gt;
&lt;li&gt;采用通过归一化流程和对抗性训练过程增强的变分推理，提高了生成模型的表达能力；&lt;/li&gt;
&lt;li&gt;一个随机持续时间预测器（stochastic duration predictor）来从输入文本中合成具有不同节奏的语音；&lt;/li&gt;
&lt;li&gt;通过对潜在变量的不确定性建模和随机持续时间预测器，表达了自然的一对多关系，其中文本输入可以以不同的音调（pitches）和节奏（rhythms）以多种方式说出。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过利用条件变分自编码器 CVAE，模型特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;学习直接从文本合成原始波形，而不需要额外的输入条件；&lt;/li&gt;
&lt;li&gt;使用动态编程方法 MAS 来搜索最佳对齐方式，而不是与计算损失相比,不需要任何外部对齐器；&lt;/li&gt;
&lt;li&gt;并行生成样本；&lt;/li&gt;
&lt;li&gt;高效的端到端训练方法, 并且生成质量优于最好的公开可用的两阶段模型。附两阶段的数据处理过程(在后续的研究论文中称之为级联方法(cascaded)，见&lt;a href=&#34;https://www.microsoft.com/en-us/research/project/vall-e-x/&#34;&gt;VALL-E&lt;/a&gt;系列论文研究)：
&lt;ul&gt;
&lt;li&gt;第一阶段是从预处理的文本中生成中间语音表示，例如梅尔谱图(mel-spectrograms)或语言特征(linguistic features)&lt;/li&gt;
&lt;li&gt;第二阶段是生成以中间表示为条件的原始波形。&lt;/li&gt;
&lt;li&gt;两阶段的相关模型大都是独立开发的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;结构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/3ddec975-a9fd-460c-91fa-894b8ebd8c8c&#34; alt=&#34;VITS&#34;&gt;&lt;/p&gt;
&lt;p&gt;PS： &lt;a href=&#34;https://github.com/ai-bot-pro/achatbot&#34;&gt;achatbot&lt;/a&gt; 集成了OpenVoiceV2 with meloTTS(meloTTS代码大部分来自VITS，Flow 采用 Transformer Encoder 结构来自 &lt;a href=&#34;https://arxiv.org/abs/2307.16430&#34;&gt;VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design&lt;/a&gt; | &lt;a href=&#34;https://github.com/daniilrobnikov/vits2&#34;&gt;paper code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PR地址： &lt;a href=&#34;https://github.com/ai-bot-pro/achatbot/pull/103&#34;&gt;https://github.com/ai-bot-pro/achatbot/pull/103&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>论文解读 OpenVoice: Versatile Instant Voice Cloning</title>
      <link>https://weedge.github.io/post/multimoding/voices/open_voice_extra_se_and_convert/</link>
      <pubDate>Sat, 11 May 2024 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/open_voice_extra_se_and_convert/</guid>
      <description>&lt;p&gt;使用meloTTS 本文生成的音频&lt;/p&gt;



&lt;figure &gt;
  &lt;audio controls class=&#34;player&#34; preload=&#34;&#34;&gt;
    &lt;source src=&#34;https://media.githubusercontent.com/media/weedge/paper-speaker/main/multimoding/voices/open_voice_inference/zh-tts.wav&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
  
  
&lt;/figure&gt;

&lt;p&gt;使用openVoice clone 自己的声音 阅读本文内容 


&lt;figure &gt;
  &lt;audio controls class=&#34;player&#34; preload=&#34;&#34;&gt;
    &lt;source src=&#34;https://media.githubusercontent.com/media/weedge/paper-speaker/main/multimoding/voices/open_voice_inference/clone-me-zh-tts.wav&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
  
  
&lt;/figure&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;文件直接上传在github中, 暂未走cdn, 缓存比较慢，可下载播放， 下载地址： &lt;a href=&#34;http://github.com/weedge/paper-speaker/tree/main/multimoding/voices/open_voice_inference&#34;&gt;http://github.com/weedge/paper-speaker/tree/main/multimoding/voices/open_voice_inference&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;openVoiceV2 tone color clone: base TTS + extra tone color + convert&lt;/p&gt;
&lt;p&gt;Base TTS: use meloTTS , 支持TTS模型训练，以及load Pre-Trained ckpt 进行TTS,  在 &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;VITS&lt;/a&gt;基础上支持多种语言；&lt;/p&gt;
&lt;p&gt;论文地址：&lt;a href=&#34;https://arxiv.org/abs/2312.01479&#34;&gt;OpenVoice: Versatile Instant Voice Cloning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;论文主作者：Zengyi Qin (同时是JetMoE的作者，站在巨人的肩膀上创新)&lt;/p&gt;
&lt;p&gt;公开的权重：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenVoice: &lt;a href=&#34;https://huggingface.co/myshell-ai/OpenVoice&#34;&gt;https://huggingface.co/myshell-ai/OpenVoice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenVoiceV2: &lt;a href=&#34;https://huggingface.co/myshell-ai/OpenVoiceV2&#34;&gt;https://huggingface.co/myshell-ai/OpenVoiceV2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;源码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/myshell-ai/OpenVoice&#34;&gt;https://github.com/myshell-ai/OpenVoice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/myshell-ai/MeloTTS&#34;&gt;https://github.com/myshell-ai/MeloTTS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;训练： MSML dataset 和 训练过程 未公开&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;附操作笔记&lt;/strong&gt;： &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/myshell_ai_OpenVoiceV2.ipynb&#34;&gt;https://github.com/weedge/doraemon-nb/blob/main/myshell_ai_OpenVoiceV2.ipynb&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>论文解读 DeLighT: Very Deep and Light-weight Transformers</title>
      <link>https://weedge.github.io/post/paper/transformer/delight/</link>
      <pubDate>Sun, 28 Apr 2024 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/paper/transformer/delight/</guid>
      <description>&lt;p&gt;在看apple 最近发布的OpenELM 模型，其论文中提到 block-wise scaling 模型结构优化方法，（论文见： &lt;a href=&#34;https://machinelearning.apple.com/research/openelm&#34;&gt;&lt;strong&gt;OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework&lt;/strong&gt;&lt;/a&gt;），这里记录下DeLighT论文中的 block-wise scaling，翻译整理下以便对照代码实现，了解背景和原理。DeLighT论文中的实验任务主要是在两个标准的序列建模任务上评估了DeLighT的性能：机器翻译（machine translation）任务 encoder-decoder architecture 和 语言建模（ language modeling）decoder architecture，论文中机器翻译任务未对En-Zh(英文译中文)进行实验，可以作为一个复现练习，根据源码实操一下论文中的实验；而语言建模可以作为openELM的来源延伸~ 结合cornet进行复现(也有mxl示例，mxl针对Apple Silicon 硬件进行的优化深度学习框架)。&lt;/p&gt;
&lt;p&gt;论文主作者：&lt;a href=&#34;https://sacmehta.github.io/&#34;&gt;Sachin Mehta &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;论文地址：https://arxiv.org/pdf/2008.00623&lt;/p&gt;
&lt;p&gt;论文代码： &lt;a href=&#34;https://github.com/sacmehta/delight&#34;&gt;https://github.com/sacmehta/delight&lt;/a&gt; （基于当时facebook的 &lt;a href=&#34;https://github.com/facebookresearch/fairseq&#34;&gt;fairseq&lt;/a&gt; seq2seq工具库开发）&lt;/p&gt;
&lt;p&gt;该论文研究是在作者以前的DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling 进行改进，模型结构引入更多的GLTs，来学习更宽的权重，并且减少了参数数量。&lt;/p&gt;
&lt;h2 id=&#34;摘要&#34;&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;我们介绍了一种深度且轻量级的Transformer，名为DeLighT，它在参数数量显著减少的情况下，提供了与标准基于Transformer的模型相似或更好的性能。DeLighT更有效地在每个Transformer块内部（通过DeLighT变换）以及跨块（通过块级缩放）分配参数，允许输入端使用较浅较窄的DeLighT块，输出端使用较宽较深的DeLighT块。总体而言，DeLighT网络比标准Transformer模型深2.5到4倍，但参数和运算量更少。在基准机器翻译和语言建模任务上的实验表明，DeLighT在平均参数数量减少2到3倍的情况下，达到或提高了基线Transformer的性能。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>解读论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</title>
      <link>https://weedge.github.io/post/paper/transformer/infini_attention/</link>
      <pubDate>Fri, 12 Apr 2024 10:26:12 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/paper/transformer/infini_attention/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://aiptcomics.com/ezoimgfmt/i0.wp.com/aiptcomics.com/wp-content/uploads/2024/04/transformers-7.jpg?w=1500&amp;amp;ssl=1&amp;amp;ezimgfmt=ngcb4/notWebP&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;图片来源： &lt;a href=&#34;https://aiptcomics.com/2024/04/10/transformers-7-2024-review/&#34;&gt;https://aiptcomics.com/2024/04/10/transformers-7-2024-review/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;： 本文介绍了一种有效的方法，将基于Transformer的大型语言模型（LLMs）扩展到无限长的输入，同时受到内存和计算的限制。我们提出的方法的关键组成部分是一种新的注意力技术，称为Infini-attention。Infini-attention将一种压缩内存集成到了传统的注意力机制中，并在单个Transformer块中构建了掩码局部注意力和长期线性注意力机制。我们通过在长上下文语言建模基准、1M序列长度的口令(keypass)上下文块检索和500K长度的书籍摘要任务中使用1B和8B LLMs，展示了我们方法的有效性。我们的方法引入了最小的有界内存参数，并实现了LLMs的快速流式推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：为解决大模型（LLMs）在处理超长输入序列时遇到的内存限制问题，本文作者提出了一种新型架构：Infini-Transformer，它可以在有限内存条件下，让基于Transformer的大语言模型（LLMs）高效处理无限长的输入序列。实验结果表明：Infini-Transformer在长上下文语言建模任务上超越了基线模型，内存最高可节约114倍。&lt;/p&gt;
&lt;p&gt;感觉有种外挂存储库(类似向量数据库)嵌入到模型结构中。比如： &lt;a href=&#34;https://arxiv.org/abs/2203.08913&#34;&gt;Memorizing Transformers&lt;/a&gt; + &lt;a href=&#34;https://github.com/lucidrains/memorizing-transformers-pytorch&#34;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在论文《Memorizing Transformers》中，作者提出了一种新的注意力机制，称为kNN-augmented attention layer，它结合了局部上下文的密集自注意力和对外部记忆的近似k-最近邻（kNN）搜索。这个机制的关键部分之一是使用了一个门控机制（gating mechanism）来结合局部注意力和外部记忆的注意力。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>翻译模型 inference 和 微调</title>
      <link>https://weedge.github.io/post/nlp/translate_model_inference_and_finetune/</link>
      <pubDate>Thu, 28 Mar 2024 21:51:52 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/nlp/translate_model_inference_and_finetune/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/nlp/translate_model_inference_and_finetune/1.png&#34; alt=&#34;image-20240328231303644&#34;&gt;&lt;/p&gt;
&lt;p&gt;想把 HuggingFaceTB/cosmopedia 英文数据中的prompt和text 翻译成 中文， 然后看了下python库 &lt;a href=&#34;https://github.com/nidhaloff/deep-translator&#34;&gt;deep_translator&lt;/a&gt;的实现， 翻译调用的是三方接口集成库， 于是使用这个库封装的谷歌翻译接口来翻译，但是三方平台接口多会有限流和接口调用频率限制，即使在代码中有容错处理， 比如常规的sleep下再调用，不影响整理处理流程，但是整体处理时间受接口限制，即使用批处理也如此，这个在大规模数据处理中使用三方接口时，经常会遇到的问题，用的三方服务，如果不升级接口服务，在技术上不太好解决； 于是选择另外一种方案，看是否有开源的翻译模型，底层模型结构一般也是Transform架构 Encoder-Decoder model ，也称sequence-to-sequence model； 比如 谷歌的T5模型， 但是推理速度受硬件条件影响，比较慢，而且原始模型不支持英文翻译成中文。&lt;/p&gt;
&lt;p&gt;然后看了下meta nllb 模型，专门用来处理翻译的模型，单个 AI 模型中支持 200 种语言，开源地址： &lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/nllb&#34;&gt;https://github.com/facebookresearch/fairseq/tree/nllb&lt;/a&gt; 模型相对现在的LLM参数量小一些，也在huggingface的Transforms库中集成 nllb-200-distilled-600M，直接可以load使用， 等等。。。 既然llm推理可以通过想llama.cpp通过加载ggml格式进行量化，在性能有少许折损的情况下降低推理成本，但是ggml gguf格式还不支持nllb模型权重文件(貌似llama.cpp只支持Transform Decoder模型结构)；那就直接用Transforms库来加载facebook/nllb-200-distilled-600M 模型来批量翻译试试看；后续还尝试使用 &lt;a href=&#34;https://huggingface.co/Helsinki-NLP/opus-mt-en-zh&#34;&gt;Helsinki-NLP/opus-mt-en-zh&lt;/a&gt; 模型，进行了简单对比。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>使用Gemma LLM构建RAG应用程序</title>
      <link>https://weedge.github.io/post/doraemon/gemma_faiss_langchain_rag/</link>
      <pubDate>Tue, 26 Mar 2024 20:16:30 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/doraemon/gemma_faiss_langchain_rag/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/rag/gemma_faiss_langchain_rag/0.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;随着大型语言模型的不断发展，构建 RAG（检索增强生成）应用程序的热潮与日俱增。谷歌推出了一个开源模型：Gemma。众所周知，RAG 代表了两种基本方法之间的融合: 基于检索的技术和生成模型。基于检索的技术涉及从广泛的知识库或语料库中获取相关信息以响应特定的查询。生成模型擅长利用训练数据中的见解从头开始创建新内容，从而精心制作原始文本或响应。&lt;/p&gt;
&lt;p&gt;目的：使用开源模型gemma来构建 RAG 管道并看看它的性能如何。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>通过chatGPT聊天解决rust线程安全问题</title>
      <link>https://weedge.github.io/post/rust/chatgpt_rust_check_thread_safety/</link>
      <pubDate>Sat, 16 Mar 2024 20:16:30 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/rust/chatgpt_rust_check_thread_safety/</guid>
      <description>&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=27445288&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;



&lt;p&gt;发现一个有趣玩法，针对rust 编译检查的问题（这个在编写代码逻辑的时候经常会遇到，逻辑是OK，但是通不过rust的安全检查），可以直接发给 chatGPT (其他通过代码库进行PT的模型，或者SFT的模型)， 会给出修改意见，并且可以根据它的提示继续追问怎么解决； 如果直接通过传统的搜索引擎比如google, 也很难找出好的解决方法，而且还要去筛选，去尝试这个方法，如果不是权威网站，可能还被坑。来来回回折腾，效率太低了。像最近的AI程序员 david： &lt;a href=&#34;https://www.cognition-labs.com/introducing-devin&#34;&gt;&lt;strong&gt;introducing-devin&lt;/strong&gt;&lt;/a&gt; ，其实类似思路，也是通过聊天来解决问题，只不过更加的专业，归根结底还是需要专业数据去PT/SFT底层的模型，上层应用通过pipeline对应框架去整体系统调优。(不知是否满足类似这种rust场景的解决方案pipeline，直接给它代码，帮忙修改)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Redis和GCP AI服务搭建RAG参考架构解决方案</title>
      <link>https://weedge.github.io/post/doraemon/redis_gcp_rag_stack/</link>
      <pubDate>Thu, 14 Mar 2024 20:16:30 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/doraemon/redis_gcp_rag_stack/</guid>
      <description>&lt;p&gt;本文主要是讲解一个快速搭建比如RAG pipeline相关应用参考方案，结合云厂商GCP AI服务，以及&lt;a href=&#34;https://redis.io/docs/about/about-stack/&#34;&gt;redis stack&lt;/a&gt; | &lt;a href=&#34;https://redis.io/docs/get-started/vector-database/&#34;&gt;vector index&lt;/a&gt;，借助 Google Cloud Platform 上易用的开发&lt;a href=&#34;https://cloud.google.com/sdk&#34;&gt;SDK&lt;/a&gt;,  以及使用&lt;a href=&#34;https://app.redislabs.com&#34;&gt;redislabs&lt;/a&gt; 提供的免费30M内存空间服务；GCP新用户前三个月好像是免费使用一些服务，而且提供 &lt;strong&gt;$300&lt;/strong&gt; 的赠金使用，对于前期学习和使用体验服务还是不错的选择，而且个人感觉学习文档很齐全，不会很零散。但是解决方案相对AWS要少些，毕竟AWS做的很深入，搭建解决方案很方便，集成开发工具比较齐全，特别是serverless lambda服务，可以看下以前写的文章『 &lt;a href=&#34;https://weedge.github.io/post/user-behavior-analytics-solution/&#34;&gt;用户行为分析方案设计&lt;/a&gt;』通过CDK构建解决方案stack(用于前期架构推演，不要YY，要动手，节约成本是干出来的)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/doraemon/redis_gcp_rag_stack/gcp-cost.png&#34; alt=&#34;image-20240314215720290&#34;&gt;&lt;/p&gt;
&lt;p&gt;以前注册的，忘记用了。。。&lt;/p&gt;
&lt;p&gt;笔记地址：&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/Google_BigQuery_Palm_Redis.ipynb&#34;&gt;https://github.com/weedge/doraemon-nb/blob/main/Google_BigQuery_Palm_Redis.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：这里使用redis作为向量索引数据库，也可以结合其他向量索引库来搭建相应方案。主要目的是熟悉GCP服务和redis cloud服务。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>论文：Retrieval-Augmented Generation for Large Language Models: A Survey [v4]</title>
      <link>https://weedge.github.io/post/paper/rag/rag-for-llms-a-survey/</link>
      <pubDate>Fri, 08 Mar 2024 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/paper/rag/rag-for-llms-a-survey/</guid>
      <description>&lt;p&gt;大型语言模型（LLMs）展示了显著的能力，但面临着幻觉、过时知识和不透明、不可追踪的推理过程等挑战。检索增强生成（RAG）已经成为一个有前途的解决方案，通过整合外部数据库的知识。这增强了模型的准确性和可信度，特别适用于知识密集型任务，并允许持续的知识更新和领域特定信息的整合。RAG通过将LLMs的内在知识与庞大、动态的外部数据库资源相结合，产生了协同效应。这篇综述论文详细考察了RAG范式的发展，包括朴素RAG、高级RAG和模块化RAG。它对RAG框架的三方基础进行了细致的了解，其中包括检索、生成和增强技术。该论文强调嵌入(embedding)在每个关键组成部分的最先进技术，并提对RAG系统进展的深入研究了解。此外，该论文介绍了评估RAG模型的指标和基准，以及最新的评估框架。最后，该论文讲了一些研究前景，包括未来挑战、多模态的扩展以及RAG基础设施及其生态系统的进展&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;论文地址:  &lt;a href=&#34;https://arxiv.org/pdf/2312.10997.pdf&#34;&gt;Retrieval-Augmented Generation for Large Language Models: A Survey&lt;/a&gt; |  &lt;a href=&#34;https://github.com/Tongji-KGLLM/RAG-Survey/blob/main/assets/RAG_Slide_ENG.pdf&#34;&gt;PPT&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;注： 主要是了解RAG的发展过程(召回率)，以及对相关子模块领域的现阶段了解，如果感兴趣，通过索引到论文引用处进一步了解。(提高看相应论文的准确率)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：内存分析</title>
      <link>https://weedge.github.io/post/cpu/memory_profiling/</link>
      <pubDate>Sun, 03 Mar 2024 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/cpu/memory_profiling/</guid>
      <description>&lt;h3 id=&#34;内存分析简介&#34;&gt;内存分析简介&lt;/h3&gt;
&lt;p&gt;在这个系列的&lt;a href=&#34;https://easyperf.net/blog/2024/02/12/Memory-Profiling-Part1&#34;&gt;原文博客文章&lt;/a&gt;中，你将学习如何收集有关程序与内存交互的高层次信息。这个过程通常被称为&lt;em&gt;内存分析&lt;/em&gt;。内存分析帮助你理解应用程序随时间变化的内存使用情况，并帮助你构建程序行为的正确心理模型。以下是它可以回答的一些问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;程序的总内存消耗是多少，以及它随时间如何变化？&lt;/li&gt;
&lt;li&gt;程序何时何地进行堆分配？&lt;/li&gt;
&lt;li&gt;哪些代码位置分配了最大量的内存？&lt;/li&gt;
&lt;li&gt;程序每秒访问多少内存？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当开发者谈论内存消耗时，他们通常指的是堆使用情况。实际上，堆是大多数应用程序中最大的内存消费者，因为它容纳了所有动态分配的对象。但堆并不是唯一的内存消费者。为了完整性，让我们提及其他内存消费者：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;栈：应用程序中帧栈使用的内存。应用程序中的每个线程都有自己的栈内存空间。通常，栈的大小只有几MB，如果超出限制，应用程序将崩溃。总的栈内存消耗与系统中运行的线程数量成正比。&lt;/li&gt;
&lt;li&gt;代码：用于存储应用程序及其库的代码（指令）的内存。在大多数情况下，它对内存消耗的贡献不大，但也有例外。例如，Clang C++编译器和Chrome浏览器拥有庞大的代码库，它们的二进制文件中有数十MB的代码段。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;接下来，我们将介绍&lt;em&gt;内存使用(memory usage)&lt;em&gt;和&lt;/em&gt;内存足迹(memory footprint)&lt;em&gt;或者翻译成&lt;/em&gt;内存占用&lt;/em&gt;这两个术语，并看看如何对它们进行分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; 主要是通过工具分析内存使用情况，尽量利用局部性原理：时间局部性和空间局部性，提高性能。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Performance Analysis and Tuning on Modern CPU 中文翻译</title>
      <link>https://weedge.github.io/post/book/performance-analysis-and-tuning-on-modern-cpu-cn/</link>
      <pubDate>Fri, 01 Mar 2024 01:16:30 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/book/performance-analysis-and-tuning-on-modern-cpu-cn/</guid>
      <description>&lt;h1 id=&#34;heading&#34;&gt;📚&lt;/h1&gt;
&lt;p&gt;这是一本名为Performance Analysis and Tuning on Modern CPU书籍的&lt;a href=&#34;https://github.com/dendibakh/perf-book&#34;&gt;源文件存储库&lt;/a&gt;的中文翻译，原版由 Denis Bakhvalov 等人编写。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原版电子书：https://book.easyperf.net/perf_book&lt;/li&gt;
&lt;li&gt;中文翻译(第一版)：https://book.douban.com/subject/36243215/&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;原作者第二版正在进行中！&lt;/strong&gt; 计划的更改在谷歌&lt;a href=&#34;https://docs.google.com/document/d/1tr2qRDe72VSBYypIANYjJLM_zCdPB6S9m4LmXsQb0vQ/edit?usp=sharing&#34;&gt;文档&lt;/a&gt;中进行了概述。计划中的新目录在 &lt;a href=&#34;https://github.com/dendibakh/perf-book/blob/main/new_toc.md&#34;&gt;new_toc.md&lt;/a&gt; 中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;虽然已经有翻译的书籍;但是想follow更新,借助 『chatGPT』/『gemini/moonshot(kimi)』 翻译成中文，(加速学习节奏，掌握，并举一反三)&lt;/li&gt;
&lt;li&gt;英文源书是开源的，翻译成中文工作也持续更新，也是开源的，可以作为学习资料, 在线阅读可编辑，希望一起参与改进。&lt;/li&gt;
&lt;li&gt;对每章节的内容通过 『chatGPT』/『gemini/moonshot(kimi)』 进行归纳总结，结巩固知识点，并对课后练习进行回答,并验证答案。&lt;/li&gt;
&lt;li&gt;最后整体勘误，定搞。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;[!TIP]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;授之以鱼不如授之以渔, 使用AI赋能。&lt;/li&gt;
&lt;li&gt;性能优化分析数据可以借助『chatGPT』分析。&lt;/li&gt;
&lt;li&gt;『chatGPT』和『moonshot(kimi)』 翻译效果差不多(相同的prompt)，但是当问文中的规划练习和代码练习时，『moonshot(kimi)』不能理解问题，不过长文本上传根据章节翻译和归纳总结不错，毕竟不用翻墙就可以使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;在线阅读地址&lt;/strong&gt;: &lt;a href=&#34;https://weedge.github.io/perf-book-cn/zh/&#34;&gt;https://weedge.github.io/perf-book-cn/zh/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中文版PDF(推荐)&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/weedge/perf-book-cn/main/perf-book-cn.pdf&#34;&gt;https://raw.githubusercontent.com/weedge/perf-book-cn/main/perf-book-cn.pdf&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>逝去的奶奶</title>
      <link>https://weedge.github.io/post/nainai/</link>
      <pubDate>Fri, 02 Feb 2024 00:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/nainai/</guid>
      <description>&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=32574246&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;



&lt;blockquote&gt;
&lt;p&gt;天上的每一颗星 都是爱过我们的人&lt;/p&gt;
&lt;p&gt;听说，地上少个人，天上多颗星，每一颗闪烁的星星都在跟地上的亲人说话。&lt;/p&gt;
&lt;p&gt;但愿，今夜有星。&lt;/p&gt;
&lt;p&gt;但愿，星星会闪。&lt;/p&gt;
&lt;p&gt;&amp;ndash; 人生大事&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;简单踏实就好，奶奶经常给说的话，一直记着。【step by step, 懂得珍惜】&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>构建一个简单的数据库[golang版]</title>
      <link>https://weedge.github.io/post/db_tutorial_go/</link>
      <pubDate>Wed, 10 Jan 2024 10:58:28 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/db_tutorial_go/</guid>
      <description>&lt;p&gt;上篇文章使用chatGPT翻译了&lt;a href=&#34;https://cstack.github.io/db_tutorial/&#34;&gt;db_tutorial&lt;/a&gt; 文章，文中使用的是c语言开发； 这篇文章使用chatGPT根据db_tutorial中的c源码，使用golang进行重写, 测试的ruby代码使用python进行重写；同理其他语言也适用。&lt;/p&gt;
&lt;p&gt;注：利用已有知识结构，通过chatGPT来生成另一种表达(现实中这种转换经常出现，比如一个基础知识点，嚼碎了，揉烂了，底层相通，表达方式不同，变了个花样玩，而且还能通过认知差来盈利，也许精细利己主义会利益最大化吧)，使用AGI工具进行效率编码的一种小小实践。在实践过程中，chatGPT生成的代码不可能都能正常运行，需要调试下(特别是指针操作)。&lt;/p&gt;
&lt;p&gt;整体实现代码：https://github.com/weedge/baby-db/tree/main/golang&lt;/p&gt;
&lt;p&gt;主要的btree数据结构为leafNode 和 internalNode，叶子节点表数据存放在value中，id存放在key中，序列化和遍历操作需要额外偏移操作；这里仅实现简单的insert和select操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cstack.github.io/db_tutorial/assets/images/leaf-node-format.png&#34; alt=&#34;leafNode&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cstack.github.io/db_tutorial/assets/images/internal-node-format.png&#34; alt=&#34;internalNode&#34;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[译]构建一个简单的数据库</title>
      <link>https://weedge.github.io/post/db_tutorial_zh/</link>
      <pubDate>Tue, 09 Jan 2024 11:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/db_tutorial_zh/</guid>
      <description>&lt;p&gt;用 C 从头开始编写 SQLite 克隆；&lt;/p&gt;
&lt;p&gt;注：用chatGPT翻译+人工稍微整理下，耗时一个多小时整理完成，使用这个简单的db from scratch试下效果, 代码简单；现在高中甚至初中生有在学这个。&lt;/p&gt;
&lt;p&gt;原文地址： &lt;a href=&#34;https://cstack.github.io/db_tutorial/&#34;&gt;https://cstack.github.io/db_tutorial/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;数据库如何工作&#34;&gt;数据库如何工作？&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;数据以什么格式保存？（在内存和磁盘上）&lt;/li&gt;
&lt;li&gt;它什么时候从内存移动到磁盘？&lt;/li&gt;
&lt;li&gt;为什么一张表只能有一个主键？&lt;/li&gt;
&lt;li&gt;回滚事务如何进行？&lt;/li&gt;
&lt;li&gt;索引是如何格式化的？&lt;/li&gt;
&lt;li&gt;全表扫描何时以及如何发生？&lt;/li&gt;
&lt;li&gt;准备好的语句以什么格式保存？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简而言之，数据库是如何&lt;strong&gt;工作的&lt;/strong&gt;？&lt;/p&gt;
&lt;p&gt;为了理解，我正在用 C 从头开始构建&lt;a href=&#34;https://www.sqlite.org/arch.html&#34;&gt;sqlite&lt;/a&gt;的克隆，并且我将记录我的过程。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“我无法创造的东西，我就不明白。&lt;em&gt;What I cannot create, I do not understand&lt;/em&gt;” ——&lt;a href=&#34;https://en.m.wikiquote.org/wiki/Richard_Feynman&#34;&gt;理查德·费曼&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://cstack.github.io/db_tutorial/assets/images/arch2.gif&#34; alt=&#34;sqlite 架构（https://www.sqlite.org/arch.html）&#34;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LLM 知识点 All u need</title>
      <link>https://weedge.github.io/post/llm/llm-knowledge-point-all-u-need/</link>
      <pubDate>Mon, 01 Jan 2024 20:26:12 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/llm-knowledge-point-all-u-need/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/LLM.png&#34; alt=&#34;LLM知识点&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图给出了学习LLM所需要的知识点。&lt;/p&gt;
&lt;p&gt;该文主要是梳理LLM基础结构知识点，模型结构大多相同，以llama2模型结构为切入点，梳理相关知识点，以便构建整体知识体系，可方便快速阅读其他论文的改进点；结合&lt;a href=&#34;https://weedge.github.io/post/llm/llm-knowledge-point-all-u-need/#%E5%8F%82%E8%80%83%E5%AD%A6%E4%B9%A0&#34;&gt;参考学习&lt;/a&gt;中给出的链接补充基础知识。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：掌握 LLM 技术：推理优化</title>
      <link>https://weedge.github.io/post/llm/mastering-llm-techniques-inference-optimization/</link>
      <pubDate>Sat, 30 Dec 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/mastering-llm-techniques-inference-optimization/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/mastering-llm-techniques-inference-optimization/0.png&#34; alt=&#34;llm-optimize-deploy&#34;&gt;&lt;/p&gt;
&lt;p&gt;将transformer层叠以创建大型模型会在各种语言任务中带来更高的准确性、少样本学习能力，甚至接近人类的新兴能力。这些基础模型在训练过程中成本高昂，而在推理过程中（一个经常发生的成本）可能需要大量内存和计算资源。如今最受欢迎的&lt;a href=&#34;https://www.nvidia.com/en-us/glossary/data-science/large-language-models/&#34;&gt;大型语言模型（LLMs）&lt;/a&gt;可以达到数百亿到数千亿个参数的规模，并且根据使用情况，可能需要处理长输入（或上下文），这也会增加成本。&lt;/p&gt;
&lt;p&gt;本文讨论了LLM推理中最紧迫的挑战，以及一些实用的解决方案。读者应该对&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;transformer架构&lt;/a&gt;和注意力机制有基本的理解。理解LLM推理的复杂性至关重要，我们将在接下来的部分进行介绍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：上篇译文有对 transformer 有相关的介绍，以及相关编码笔记入门；或者深入学习&lt;a href=&#34;https://web.stanford.edu/class/cs25/prev_years/2023_winter/index.html&#34;&gt;CS25: Transformers United V2&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&#34;&gt;video&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
