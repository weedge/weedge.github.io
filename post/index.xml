<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 时间飘过</title>
    <link>https://weedge.github.io/post/</link>
    <description>Recent content in Posts on 时间飘过</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 12 Apr 2024 10:26:12 +0800</lastBuildDate><atom:link href="https://weedge.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>论文：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</title>
      <link>https://weedge.github.io/post/paper/transformer/infini_attention/</link>
      <pubDate>Fri, 12 Apr 2024 10:26:12 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/paper/transformer/infini_attention/</guid>
      <description>&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;： 本文介绍了一种有效的方法，将基于Transformer的大型语言模型（LLMs）扩展到无限长的输入，同时受到内存和计算的限制。我们提出的方法的关键组成部分是一种新的注意力技术，称为Infini-attention。Infini-attention将一种压缩内存集成到了传统的注意力机制中，并在单个Transformer块中构建了掩码局部注意力和长期线性注意力机制。我们通过在长上下文语言建模基准、1M序列长度的密码上下文块检索和500K长度的书籍摘要任务中使用1B和8B LLMs，展示了我们方法的有效性。我们的方法引入了最小的有界内存参数，并实现了LLMs的快速流式推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：为解决大模型（LLMs）在处理超长输入序列时遇到的内存限制问题，本文作者提出了一种新型架构：Infini-Transformer，它可以在有限内存条件下，让基于Transformer的大语言模型（LLMs）高效处理无限长的输入序列。实验结果表明：Infini-Transformer在长上下文语言建模任务上超越了基线模型，内存最高可节约114倍。&lt;/p&gt;
&lt;p&gt;感觉有种外挂存储库(类似向量数据库)嵌入到模型结构中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;：通过翻译通读论文，了解Transformer相关优化模型结构论文思路，比如文中对比的 &lt;a href=&#34;https://arxiv.org/abs/1901.02860&#34;&gt;Transformer-XL&lt;/a&gt; ，&lt;a href=&#34;https://arxiv.org/abs/1911.05507&#34;&gt;Compressive Transformer&lt;/a&gt;，&lt;a href=&#34;https://arxiv.org/abs/2207.06881&#34;&gt;Recurrent Memory Transformer(RMT)&lt;/a&gt; ， &lt;a href=&#34;https://arxiv.org/abs/2203.08913&#34;&gt;Memorizing Transformers&lt;/a&gt;； 论文中提到的优化技术：Compressive Memory &amp;amp; Linear attention 还可以整合到现到Spare MoE 相关的模型结构中，比如&lt;a href=&#34;https://arxiv.org/abs/2101.03961&#34;&gt;Switch Transformers&lt;/a&gt; , &lt;a href=&#34;https://arxiv.org/abs/2306.04640&#34;&gt;ModuleFormer&lt;/a&gt;中提到的MoE中，诶~是不是做做实验，困惑度(perplexity)/loss 效果不错的话是不是可以发个论文嘞~；Google大法下了个优化蛋，后面跟着一批组合优化蛋。法力无边，ღ( ´･ᴗ･` )比心~&lt;/p&gt;
&lt;p&gt;原论文地址： &lt;a href=&#34;https://arxiv.org/pdf/2404.07143.pdf&#34;&gt;https://arxiv.org/pdf/2404.07143.pdf&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>翻译模型 inference 和 微调</title>
      <link>https://weedge.github.io/post/nlp/translate_model_inference_and_finetune/</link>
      <pubDate>Thu, 28 Mar 2024 21:51:52 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/nlp/translate_model_inference_and_finetune/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/nlp/translate_model_inference_and_finetune/1.png&#34; alt=&#34;image-20240328231303644&#34;&gt;&lt;/p&gt;
&lt;p&gt;想把 HuggingFaceTB/cosmopedia 英文数据中的prompt和text 翻译成 中文， 然后看了下python库 &lt;a href=&#34;https://github.com/nidhaloff/deep-translator&#34;&gt;deep_translator&lt;/a&gt;的实现， 翻译调用的是三方接口集成库， 于是使用这个库封装的谷歌翻译接口来翻译，但是三方平台接口多会有限流和接口调用频率限制，即使在代码中有容错处理， 比如常规的sleep下再调用，不影响整理处理流程，但是整体处理时间受接口限制，即使用批处理也如此，这个在大规模数据处理中使用三方接口时，经常会遇到的问题，用的三方服务，如果不升级接口服务，在技术上不太好解决； 于是选择另外一种方案，看是否有开源的翻译模型，底层模型结构一般也是Transform架构 Encoder-Decoder model ，也称sequence-to-sequence model； 比如 谷歌的T5模型， 但是推理速度受硬件条件影响，比较慢，而且原始模型不支持英文翻译成中文。&lt;/p&gt;
&lt;p&gt;然后看了下meta nllb 模型，专门用来处理翻译的模型，单个 AI 模型中支持 200 种语言，开源地址： &lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/nllb&#34;&gt;https://github.com/facebookresearch/fairseq/tree/nllb&lt;/a&gt; 模型相对现在的LLM参数量小一些，也在huggingface的Transforms库中集成 nllb-200-distilled-600M，直接可以load使用， 等等。。。 既然llm推理可以通过想llama.cpp通过加载ggml格式进行量化，在性能有少许折损的情况下降低推理成本，但是ggml gguf格式还不支持nllb模型权重文件(貌似llama.cpp只支持Transform Decoder模型结构)；那就直接用Transforms库来加载facebook/nllb-200-distilled-600M 模型来批量翻译试试看；后续还尝试使用 &lt;a href=&#34;https://huggingface.co/Helsinki-NLP/opus-mt-en-zh&#34;&gt;Helsinki-NLP/opus-mt-en-zh&lt;/a&gt; 模型，进行了简单对比。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>使用Gemma LLM构建RAG应用程序</title>
      <link>https://weedge.github.io/post/doraemon/gemma_faiss_langchain_rag/</link>
      <pubDate>Tue, 26 Mar 2024 20:16:30 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/doraemon/gemma_faiss_langchain_rag/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/rag/gemma_faiss_langchain_rag/0.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;随着大型语言模型的不断发展，构建 RAG（检索增强生成）应用程序的热潮与日俱增。谷歌推出了一个开源模型：Gemma。众所周知，RAG 代表了两种基本方法之间的融合: 基于检索的技术和生成模型。基于检索的技术涉及从广泛的知识库或语料库中获取相关信息以响应特定的查询。生成模型擅长利用训练数据中的见解从头开始创建新内容，从而精心制作原始文本或响应。&lt;/p&gt;
&lt;p&gt;目的：使用开源模型gemma来构建 RAG 管道并看看它的性能如何。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>通过chatGPT聊天解决rust线程安全问题</title>
      <link>https://weedge.github.io/post/rust/chatgpt_rust_check_thread_safety/</link>
      <pubDate>Sat, 16 Mar 2024 20:16:30 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/rust/chatgpt_rust_check_thread_safety/</guid>
      <description>&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=27445288&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;



&lt;p&gt;发现一个有趣玩法，针对rust 编译检查的问题（这个在编写代码逻辑的时候经常会遇到，逻辑是OK，但是通不过rust的安全检查），可以直接发给 chatGPT (其他通过代码库进行PT的模型，或者SFT的模型)， 会给出修改意见，并且可以根据它的提示继续追问怎么解决； 如果直接通过传统的搜索引擎比如google, 也很难找出好的解决方法，而且还要去筛选，去尝试这个方法，如果不是权威网站，可能还被坑。来来回回折腾，效率太低了。像最近的AI程序员 david： &lt;a href=&#34;https://www.cognition-labs.com/introducing-devin&#34;&gt;&lt;strong&gt;introducing-devin&lt;/strong&gt;&lt;/a&gt; ，其实类似思路，也是通过聊天来解决问题，只不过更加的专业，归根结底还是需要专业数据去PT/SFT底层的模型，上层应用通过pipeline对应框架去整体系统调优。(不知是否满足类似这种rust场景的解决方案pipeline，直接给它代码，帮忙修改)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Redis和GCP AI服务搭建RAG参考架构解决方案</title>
      <link>https://weedge.github.io/post/doraemon/redis_gcp_rag_stack/</link>
      <pubDate>Thu, 14 Mar 2024 20:16:30 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/doraemon/redis_gcp_rag_stack/</guid>
      <description>&lt;p&gt;本文主要是讲解一个快速搭建比如RAG pipeline相关应用参考方案，结合云厂商GCP AI服务，以及&lt;a href=&#34;https://redis.io/docs/about/about-stack/&#34;&gt;redis stack&lt;/a&gt; | &lt;a href=&#34;https://redis.io/docs/get-started/vector-database/&#34;&gt;vector index&lt;/a&gt;，借助 Google Cloud Platform 上易用的开发&lt;a href=&#34;https://cloud.google.com/sdk&#34;&gt;SDK&lt;/a&gt;,  以及使用&lt;a href=&#34;https://app.redislabs.com&#34;&gt;redislabs&lt;/a&gt; 提供的免费30M内存空间服务；GCP新用户前三个月好像是免费使用一些服务，而且提供 &lt;strong&gt;$300&lt;/strong&gt; 的赠金使用，对于前期学习和使用体验服务还是不错的选择，而且个人感觉学习文档很齐全，不会很零散。但是解决方案相对AWS要少些，毕竟AWS做的很深入，搭建解决方案很方便，集成开发工具比较齐全，特别是serverless lambda服务，可以看下以前写的文章『 &lt;a href=&#34;https://weedge.github.io/post/user-behavior-analytics-solution/&#34;&gt;用户行为分析方案设计&lt;/a&gt;』通过CDK构建解决方案stack(用于前期架构推演，不要YY，要动手，节约成本是干出来的)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/doraemon/redis_gcp_rag_stack/gcp-cost.png&#34; alt=&#34;image-20240314215720290&#34;&gt;&lt;/p&gt;
&lt;p&gt;以前注册的，忘记用了。。。&lt;/p&gt;
&lt;p&gt;笔记地址：&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/Google_BigQuery_Palm_Redis.ipynb&#34;&gt;https://github.com/weedge/doraemon-nb/blob/main/Google_BigQuery_Palm_Redis.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：这里使用redis作为向量索引数据库，也可以结合其他向量索引库来搭建相应方案。主要目的是熟悉GCP服务和redis cloud服务。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>论文：Retrieval-Augmented Generation for Large Language Models: A Survey [v4]</title>
      <link>https://weedge.github.io/post/paper/rag/rag-for-llms-a-survey/</link>
      <pubDate>Fri, 08 Mar 2024 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/paper/rag/rag-for-llms-a-survey/</guid>
      <description>&lt;p&gt;大型语言模型（LLMs）展示了显著的能力，但面临着幻觉、过时知识和不透明、不可追踪的推理过程等挑战。检索增强生成（RAG）已经成为一个有前途的解决方案，通过整合外部数据库的知识。这增强了模型的准确性和可信度，特别适用于知识密集型任务，并允许持续的知识更新和领域特定信息的整合。RAG通过将LLMs的内在知识与庞大、动态的外部数据库资源相结合，产生了协同效应。这篇综述论文详细考察了RAG范式的发展，包括朴素RAG、高级RAG和模块化RAG。它对RAG框架的三方基础进行了细致的了解，其中包括检索、生成和增强技术。该论文强调嵌入(embedding)在每个关键组成部分的最先进技术，并提对RAG系统进展的深入研究了解。此外，该论文介绍了评估RAG模型的指标和基准，以及最新的评估框架。最后，该论文讲了一些研究前景，包括未来挑战、多模态的扩展以及RAG基础设施及其生态系统的进展&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;论文地址:  &lt;a href=&#34;https://arxiv.org/pdf/2312.10997.pdf&#34;&gt;Retrieval-Augmented Generation for Large Language Models: A Survey&lt;/a&gt; |  &lt;a href=&#34;https://github.com/Tongji-KGLLM/RAG-Survey/blob/main/assets/RAG_Slide_ENG.pdf&#34;&gt;PPT&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;注： 主要是了解RAG的发展过程(召回率)，以及对相关子模块领域的现阶段了解，如果感兴趣，通过索引到论文引用处进一步了解。(提高看相应论文的准确率)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：内存分析</title>
      <link>https://weedge.github.io/post/cpu/memory_profiling/</link>
      <pubDate>Sun, 03 Mar 2024 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/cpu/memory_profiling/</guid>
      <description>&lt;h3 id=&#34;内存分析简介&#34;&gt;内存分析简介&lt;/h3&gt;
&lt;p&gt;在这个系列的&lt;a href=&#34;https://easyperf.net/blog/2024/02/12/Memory-Profiling-Part1&#34;&gt;原文博客文章&lt;/a&gt;中，你将学习如何收集有关程序与内存交互的高层次信息。这个过程通常被称为&lt;em&gt;内存分析&lt;/em&gt;。内存分析帮助你理解应用程序随时间变化的内存使用情况，并帮助你构建程序行为的正确心理模型。以下是它可以回答的一些问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;程序的总内存消耗是多少，以及它随时间如何变化？&lt;/li&gt;
&lt;li&gt;程序何时何地进行堆分配？&lt;/li&gt;
&lt;li&gt;哪些代码位置分配了最大量的内存？&lt;/li&gt;
&lt;li&gt;程序每秒访问多少内存？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当开发者谈论内存消耗时，他们通常指的是堆使用情况。实际上，堆是大多数应用程序中最大的内存消费者，因为它容纳了所有动态分配的对象。但堆并不是唯一的内存消费者。为了完整性，让我们提及其他内存消费者：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;栈：应用程序中帧栈使用的内存。应用程序中的每个线程都有自己的栈内存空间。通常，栈的大小只有几MB，如果超出限制，应用程序将崩溃。总的栈内存消耗与系统中运行的线程数量成正比。&lt;/li&gt;
&lt;li&gt;代码：用于存储应用程序及其库的代码（指令）的内存。在大多数情况下，它对内存消耗的贡献不大，但也有例外。例如，Clang C++编译器和Chrome浏览器拥有庞大的代码库，它们的二进制文件中有数十MB的代码段。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;接下来，我们将介绍&lt;em&gt;内存使用(memory usage)&lt;em&gt;和&lt;/em&gt;内存足迹(memory footprint)&lt;em&gt;或者翻译成&lt;/em&gt;内存占用&lt;/em&gt;这两个术语，并看看如何对它们进行分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; 主要是通过工具分析内存使用情况，尽量利用局部性原理：时间局部性和空间局部性，提高性能。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Performance Analysis and Tuning on Modern CPU 中文翻译</title>
      <link>https://weedge.github.io/post/book/performance-analysis-and-tuning-on-modern-cpu-cn/</link>
      <pubDate>Fri, 01 Mar 2024 01:16:30 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/book/performance-analysis-and-tuning-on-modern-cpu-cn/</guid>
      <description>&lt;h1 id=&#34;heading&#34;&gt;📚&lt;/h1&gt;
&lt;p&gt;这是一本名为Performance Analysis and Tuning on Modern CPU书籍的&lt;a href=&#34;https://github.com/dendibakh/perf-book&#34;&gt;源文件存储库&lt;/a&gt;的中文翻译，原版由 Denis Bakhvalov 等人编写。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原版电子书：https://book.easyperf.net/perf_book&lt;/li&gt;
&lt;li&gt;中文翻译(第一版)：https://book.douban.com/subject/36243215/&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;原作者第二版正在进行中！&lt;/strong&gt; 计划的更改在谷歌&lt;a href=&#34;https://docs.google.com/document/d/1tr2qRDe72VSBYypIANYjJLM_zCdPB6S9m4LmXsQb0vQ/edit?usp=sharing&#34;&gt;文档&lt;/a&gt;中进行了概述。计划中的新目录在 &lt;a href=&#34;https://github.com/dendibakh/perf-book/blob/main/new_toc.md&#34;&gt;new_toc.md&lt;/a&gt; 中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;虽然已经有翻译的书籍;但是想follow更新,借助 『chatGPT』/『gemini/moonshot(kimi)』 翻译成中文，(加速学习节奏，掌握，并举一反三)&lt;/li&gt;
&lt;li&gt;英文源书是开源的，翻译成中文工作也持续更新，也是开源的，可以作为学习资料, 在线阅读可编辑，希望一起参与改进。&lt;/li&gt;
&lt;li&gt;对每章节的内容通过 『chatGPT』/『gemini/moonshot(kimi)』 进行归纳总结，结巩固知识点，并对课后练习进行回答,并验证答案。&lt;/li&gt;
&lt;li&gt;最后整体勘误，定搞。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;[!TIP]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;授之以鱼不如授之以渔, 使用AI赋能。&lt;/li&gt;
&lt;li&gt;性能优化分析数据可以借助『chatGPT』分析。&lt;/li&gt;
&lt;li&gt;『chatGPT』和『moonshot(kimi)』 翻译效果差不多(相同的prompt)，但是当问文中的规划练习和代码练习时，『moonshot(kimi)』不能理解问题，不过长文本上传根据章节翻译和归纳总结不错，毕竟不用翻墙就可以使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;在线阅读地址&lt;/strong&gt;: &lt;a href=&#34;https://weedge.github.io/perf-book-cn/zh/&#34;&gt;https://weedge.github.io/perf-book-cn/zh/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中文版PDF(推荐)&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/weedge/perf-book-cn/main/perf-book-cn.pdf&#34;&gt;https://raw.githubusercontent.com/weedge/perf-book-cn/main/perf-book-cn.pdf&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>逝去的奶奶</title>
      <link>https://weedge.github.io/post/nainai/</link>
      <pubDate>Fri, 02 Feb 2024 00:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/nainai/</guid>
      <description>&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=32574246&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;



&lt;blockquote&gt;
&lt;p&gt;天上的每一颗星 都是爱过我们的人&lt;/p&gt;
&lt;p&gt;听说，地上少个人，天上多颗星，每一颗闪烁的星星都在跟地上的亲人说话。&lt;/p&gt;
&lt;p&gt;但愿，今夜有星。&lt;/p&gt;
&lt;p&gt;但愿，星星会闪。&lt;/p&gt;
&lt;p&gt;&amp;ndash; 人生大事&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;简单踏实就好，奶奶经常给说的话，一直记着。【step by step, 懂得珍惜】&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>构建一个简单的数据库[golang版]</title>
      <link>https://weedge.github.io/post/db_tutorial_go/</link>
      <pubDate>Wed, 10 Jan 2024 10:58:28 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/db_tutorial_go/</guid>
      <description>&lt;p&gt;上篇文章使用chatGPT翻译了&lt;a href=&#34;https://cstack.github.io/db_tutorial/&#34;&gt;db_tutorial&lt;/a&gt; 文章，文中使用的是c语言开发； 这篇文章使用chatGPT根据db_tutorial中的c源码，使用golang进行重写, 测试的ruby代码使用python进行重写；同理其他语言也适用。&lt;/p&gt;
&lt;p&gt;注：利用已有知识结构，通过chatGPT来生成另一种表达(现实中这种转换经常出现，比如一个基础知识点，嚼碎了，揉烂了，底层相通，表达方式不同，变了个花样玩，而且还能通过认知差来盈利，也许精细利己主义会利益最大化吧)，使用AGI工具进行效率编码的一种小小实践。在实践过程中，chatGPT生成的代码不可能都能正常运行，需要调试下(特别是指针操作)。&lt;/p&gt;
&lt;p&gt;整体实现代码：https://github.com/weedge/baby-db/tree/main/golang&lt;/p&gt;
&lt;p&gt;主要的btree数据结构为leafNode 和 internalNode，叶子节点表数据存放在value中，id存放在key中，序列化和遍历操作需要额外偏移操作；这里仅实现简单的insert和select操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cstack.github.io/db_tutorial/assets/images/leaf-node-format.png&#34; alt=&#34;leafNode&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cstack.github.io/db_tutorial/assets/images/internal-node-format.png&#34; alt=&#34;internalNode&#34;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[译]构建一个简单的数据库</title>
      <link>https://weedge.github.io/post/db_tutorial_zh/</link>
      <pubDate>Tue, 09 Jan 2024 11:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/db_tutorial_zh/</guid>
      <description>&lt;p&gt;用 C 从头开始编写 SQLite 克隆；&lt;/p&gt;
&lt;p&gt;注：用chatGPT翻译+人工稍微整理下，耗时一个多小时整理完成，使用这个简单的db from scratch试下效果, 代码简单；现在高中甚至初中生有在学这个。&lt;/p&gt;
&lt;p&gt;原文地址： &lt;a href=&#34;https://cstack.github.io/db_tutorial/&#34;&gt;https://cstack.github.io/db_tutorial/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;数据库如何工作&#34;&gt;数据库如何工作？&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;数据以什么格式保存？（在内存和磁盘上）&lt;/li&gt;
&lt;li&gt;它什么时候从内存移动到磁盘？&lt;/li&gt;
&lt;li&gt;为什么一张表只能有一个主键？&lt;/li&gt;
&lt;li&gt;回滚事务如何进行？&lt;/li&gt;
&lt;li&gt;索引是如何格式化的？&lt;/li&gt;
&lt;li&gt;全表扫描何时以及如何发生？&lt;/li&gt;
&lt;li&gt;准备好的语句以什么格式保存？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简而言之，数据库是如何&lt;strong&gt;工作的&lt;/strong&gt;？&lt;/p&gt;
&lt;p&gt;为了理解，我正在用 C 从头开始构建&lt;a href=&#34;https://www.sqlite.org/arch.html&#34;&gt;sqlite&lt;/a&gt;的克隆，并且我将记录我的过程。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“我无法创造的东西，我就不明白。&lt;em&gt;What I cannot create, I do not understand&lt;/em&gt;” ——&lt;a href=&#34;https://en.m.wikiquote.org/wiki/Richard_Feynman&#34;&gt;理查德·费曼&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://cstack.github.io/db_tutorial/assets/images/arch2.gif&#34; alt=&#34;sqlite 架构（https://www.sqlite.org/arch.html）&#34;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LLM 知识点 All u need</title>
      <link>https://weedge.github.io/post/llm/llm-knowledge-point-all-u-need/</link>
      <pubDate>Mon, 01 Jan 2024 20:26:12 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/llm-knowledge-point-all-u-need/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/LLM.png&#34; alt=&#34;LLM知识点&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图给出了学习LLM所需要的知识点。&lt;/p&gt;
&lt;p&gt;该文主要是梳理LLM基础结构知识点，模型结构大多相同，以llama2模型结构为切入点，梳理相关知识点，以便构建整体知识体系，可方便快速阅读其他论文的改进点；结合&lt;a href=&#34;https://weedge.github.io/post/llm/llm-knowledge-point-all-u-need/#%E5%8F%82%E8%80%83%E5%AD%A6%E4%B9%A0&#34;&gt;参考学习&lt;/a&gt;中给出的链接补充基础知识。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：掌握 LLM 技术：推理优化</title>
      <link>https://weedge.github.io/post/llm/mastering-llm-techniques-inference-optimization/</link>
      <pubDate>Sat, 30 Dec 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/mastering-llm-techniques-inference-optimization/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/mastering-llm-techniques-inference-optimization/0.png&#34; alt=&#34;llm-optimize-deploy&#34;&gt;&lt;/p&gt;
&lt;p&gt;将transformer层叠以创建大型模型会在各种语言任务中带来更高的准确性、少样本学习能力，甚至接近人类的新兴能力。这些基础模型在训练过程中成本高昂，而在推理过程中（一个经常发生的成本）可能需要大量内存和计算资源。如今最受欢迎的&lt;a href=&#34;https://www.nvidia.com/en-us/glossary/data-science/large-language-models/&#34;&gt;大型语言模型（LLMs）&lt;/a&gt;可以达到数百亿到数千亿个参数的规模，并且根据使用情况，可能需要处理长输入（或上下文），这也会增加成本。&lt;/p&gt;
&lt;p&gt;本文讨论了LLM推理中最紧迫的挑战，以及一些实用的解决方案。读者应该对&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;transformer架构&lt;/a&gt;和注意力机制有基本的理解。理解LLM推理的复杂性至关重要，我们将在接下来的部分进行介绍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：上篇译文有对 transformer 有相关的介绍，以及相关编码笔记入门；或者深入学习&lt;a href=&#34;https://web.stanford.edu/class/cs25/prev_years/2023_winter/index.html&#34;&gt;CS25: Transformers United V2&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&#34;&gt;video&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：大型语言模型入门介绍</title>
      <link>https://weedge.github.io/post/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/</link>
      <pubDate>Mon, 04 Dec 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/transformers.svg&#34; alt=&#34;原始的 Transformer 模型结构&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;本文旨在让没有计算机科学背景的人深入了解 ChatGPT 和类似的 AI 系统（GPT-3、GPT-4、Bing Chat、Bard 等）的工作原理。ChatGPT 是一个聊天机器人——一种构建的对话式人工智能——但建立在&lt;em&gt;大型语言模型&lt;/em&gt;之上。我们将把它们全部分解。在此过程中，我们将讨论它们背后的核心概念。本文不需要任何技术或数学背景。我们将大量使用隐喻来说明这些概念。我们将讨论为什么核心概念以它们的方式工作，以及我们可以期望或不期望像 ChatGPT 这样的大型语言模型做什么。&lt;/p&gt;
&lt;p&gt;这就是我们要做的事情。我们将温和地介绍一些与大型语言模型和 ChatGPT 相关的术语，不使用任何行话。如果我必须使用行话，我会不使用行话来分解它。我们将从“什么是人工智能”开始，然后逐步提高。我会尽可能地使用一些反复出现的隐喻。我将讨论这些技术的影响，即我们应该期望它们做什么或不应该期望它们做什么。let&amp;rsquo;s go~!&lt;/p&gt;
&lt;p&gt;注：主要是结合论文「&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;」理解Transformer。&lt;a href=&#34;https://www.youtube.com/watch?v=nzqlFIcCSWQ&#34;&gt;Transformer论文逐段精读&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;附：&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/transformer.ipynb&#34;&gt;Transformer学习笔记&lt;/a&gt; | &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/AnnotatedTransformer.ipynb&#34;&gt;Annotated Transformer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zjkBMFhNj_g&#34;&gt;&lt;strong&gt;Intro to Large Language Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：使用大型语言模型进行开发所需了解的知识</title>
      <link>https://weedge.github.io/post/llm/all-you-need-to-know-to-develop-using-large-language-models/</link>
      <pubDate>Sun, 03 Dec 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/all-you-need-to-know-to-develop-using-large-language-models/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/0.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;导读&#34;&gt;导读&lt;/h2&gt;
&lt;p&gt;本文的目的是简单地解释开始开发基于 LLM 的应用程序所需的关键技术。它面向对机器学习概念有基本了解并希望深入研究的软件开发人员、数据科学家和&lt;strong&gt;人工智能爱好者&lt;/strong&gt;。本文还提供了许多有用的链接以供进一步研究。这会很有趣！&lt;/p&gt;
&lt;p&gt;注：本文可以作为一个索引目录(进一步阅读资料深入学习)，从整体上了解下，毕竟现在LLM发展很快，可以发散或者focus某个领域；大部分LLM相关开源实现，可以手动demo下过程，至于炼丹了解过程即可，主要在场景上结合工程去利用好大力神丸在生产环境落地；还有就是应用场景，国内app应该可以复刻，如果模型和数据有了，缺个落地idea的话~&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：更快的字符串转整数</title>
      <link>https://weedge.github.io/post/simd/faster_integer_parsing/</link>
      <pubDate>Thu, 30 Nov 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/simd/faster_integer_parsing/</guid>
      <description>&lt;p&gt;​                        &lt;img src=&#34;https://github.com/weedge/mypic/raw/master/simd/faster_integer_parsing/0.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;导读&#34;&gt;导读&lt;/h2&gt;
&lt;p&gt;字符串转换成整数，或者浮点类型数据，是在编程中经常遇到的问题，各种语言的标准库中会有实现，本文通过一个常见问题场景，来研究优化如何使用cpu 硬件SIMD指令集，并结合编译器在 log(n) 时间内完成此类parse操作；由于最终的优化需要结合对应cpu arch的指令集，这里硬件平台cpu为Intel x86，整数类型以uint64_t为例，最大2^64-1 20个字符表示。目的：结合场景优化思路(以小见大)，熟悉下Intel cpu simd相关指令的使用。常见场景： &lt;a href=&#34;https://github.com/simdjson/simdjson&#34;&gt;simdjson&lt;/a&gt;  (PS: 不因过早优化，在对应场景下整体稳定性和优化成本/收益上折中)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：掌握 RAPIDS libcudf 中的字符串转换</title>
      <link>https://weedge.github.io/post/gpu/mastering-string-transformations-in-rapids-libcudf/</link>
      <pubDate>Tue, 07 Nov 2023 15:00:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/mastering-string-transformations-in-rapids-libcudf/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/mastering-string-transformations-in-rapids-libcudf/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;字符串数据的高效处理对于许多数据科学应用至关重要。为了从字符串数据中提取有价值的信息，&lt;a href=&#34;https://github.com/rapidsai/cudf&#34;&gt;RAPIDS libcudf&lt;/a&gt;提供了强大的工具来加速字符串数据转换。libcudf 是一个 C++ GPU DataFrame 库，用于加载、连接、聚合和过滤数据。&lt;/p&gt;
&lt;p&gt;在数据科学中，字符串数据代表语音、文本、基因序列、日志记录和许多其他类型的信息。在使用字符串数据进行机器学习和特征工程时，必须经常对数据进行规范化和转换，然后才能将其应用于特定用例。libcudf 提供通用 API 和设备端实用程序，以支持各种自定义字符串操作。&lt;/p&gt;
&lt;p&gt;这篇文章演示了如何使用 libcudf 通用 API 巧妙地转换字符串列。您将获得有关如何使用自定义内核和 libcudf 设备端实用程序解锁峰值性能的新知识。这篇文章还向您介绍了如何最好地管理 GPU 内存和高效构建 libcudf 列以加速字符串转换的示例。&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;注&lt;/strong&gt;：从文件中获取数据到buffer中，都需要通过字符串处理操作，特别是split操作，如果是数值，需要atoi，atof操作进行数据分析，向量化操作等， 和数据处理打交道的 super 马里奥 应该学会这个工具，这里直接使用底层操作库libcudf；集成的其他语言有java(JNI)和python(cython), 主要是方便和现有 大数据生态打通(会有一些内存方面的性能损耗)，大多是离线处理场景，特别是LLM的预训练场景)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：加速向量搜索：RAPIDS RAFT IVF-Flat 近似算法</title>
      <link>https://weedge.github.io/post/gpu/3.accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/</link>
      <pubDate>Fri, 03 Nov 2023 15:00:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/3.accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;执行详尽的精确 k 最近邻 (kNN) 搜索，也称为&lt;em&gt;暴力搜索(brute-force search)&lt;/em&gt;，成本高昂，并且它不能很好地扩展到更大的数据集。在向量搜索期间，暴力搜索需要计算每个查询向量和数据库向量之间的距离。对于常用的欧几里德和余弦距离，计算任务等同于大型矩阵乘法。&lt;/p&gt;
&lt;p&gt;虽然 GPU 在执行矩阵乘法方面效率很高，但随着数据量的增加，计算成本变得令人望而却步。然而，许多应用程序不需要精确的结果，而是可以为了更快的搜索而牺牲一些准确性。当不需要精确的结果时，近似最近邻 (ANN) 方法通常可以减少搜索期间必须执行的距离计算的数量。&lt;/p&gt;
&lt;p&gt;本文主要介绍了 IVF-Flat，这是 NVIDIA &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/&#34;&gt;RAPIDS RAFT&lt;/a&gt; 中的一种方法。IVF-Flat 方法使用原始（即Flat）向量的倒排索引 (IVF)。此算法提供了简单的调整手段，以减少整体搜索空间并在准确性和速度之间进行权衡。&lt;/p&gt;
&lt;p&gt;为了帮助了解如何使用 IVF-Flat，我们讨论了该算法的工作原理，并演示了&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#ivf-flat&#34;&gt;Python&lt;/a&gt;和&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_ivf_flat/&#34;&gt;C++ APIs&lt;/a&gt;我们介绍了索引构建的设置参数，并提供了如何配置 GPU 加速的 IVF-Flat搜索的技巧。这些步骤也可以在示例中遵循&lt;a href=&#34;https://github.com/rapidsai/raft/blob/a1002f8c8f4debc52fbab7191297a2f54ff42856/notebooks/ivf_flat_example.ipynb&#34;&gt;Python notebook&lt;/a&gt;和&lt;a href=&#34;https://github.com/rapidsai/raft/blob/a1002f8c8f4debc52fbab7191297a2f54ff42856/cpp/template/src/ivf_flat_example.cu&#34;&gt;C++ project&lt;/a&gt;.最后，我们演示了 GPU 加速的向量搜索比 CPU 搜索快一个数量级。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：加速向量搜索：微调 GPU 索引算法</title>
      <link>https://weedge.github.io/post/gpu/2.accelerating-vector-search-fine-tuning-gpu-index-algorithms/</link>
      <pubDate>Fri, 03 Nov 2023 14:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/2.accelerating-vector-search-fine-tuning-gpu-index-algorithms/</guid>
      <description>&lt;p&gt;这个 &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/accelerating-vector-search-using-gpu-accelerated-indexes-with-rapids-raft/&#34;&gt;系列的第一篇文章&lt;/a&gt; 介绍了向量搜索索引，解释了它们在实现广泛的重要应用中所起的作用，并使用了 &lt;a href=&#34;https://github.com/rapidsai/raft&#34;&gt;RAFT&lt;/a&gt; 库。&lt;/p&gt;
&lt;p&gt;在这篇文章中，我们深入探讨第 1 部分中提到的每种 GPU 加速索引方法，并简要解释了算法的工作原理，以及总结重要的微调参数。&lt;/p&gt;
&lt;p&gt;然后，我们通过一个简单的端到端示例，用预训练的大型语言模型演示 RAFT 在问答问题上的 Python API，并在涉及同时传递给搜索算法的不同查询向量数量的几个不同场景下，将 RAFT 的算法与 HNSW 的性能进行比较。&lt;/p&gt;
&lt;p&gt;内容如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可与 GPU 一起使用的向量搜索索引算法概述&lt;/li&gt;
&lt;li&gt;一个端到端的例子演示了使用 Python 在 GPU 上运行向量搜索是多么容易&lt;/li&gt;
&lt;li&gt;GPU 上的向量搜索与 CPU 上当前最先进的 HNSW 方法的性能比较&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>译：加速向量搜索：利用 GPU 索引的 RAPIDS RAFT</title>
      <link>https://weedge.github.io/post/gpu/1.accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/</link>
      <pubDate>Fri, 03 Nov 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/1.accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/</guid>
      <description>&lt;p&gt;在 2023 年的人工智能领域，向量搜索成为最热门的话题之一，因为它在&lt;a href=&#34;https://www.nvidia.cn/glossary/data-science/large-language-models/&#34;&gt;大语言模型&lt;/a&gt;（LLM）和&lt;a href=&#34;https://www.nvidia.cn/glossary/data-science/generative-ai/&#34;&gt;生成式人工智能&lt;/a&gt;中发挥了重要作用。语义向量搜索实现了一系列重要任务，如检测欺诈交易、向用户推荐产品、使用上下文信息增强全文搜索以及查找潜在安全风险的参与者。&lt;/p&gt;
&lt;p&gt;数据量持续飙升，传统的逐一比较的方法在计算上变得不可行。向量搜索方法使用近似查找，这种查找更具可扩展性，可以更有效地处理大量数据。正如我们在这篇文章中所展示的，在 GPU 上加速向量搜索不仅提供了更快的搜索时间，而且索引构建时间也可以更快。&lt;/p&gt;
&lt;p&gt;本文内容如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;向量搜索简介及流行应用综述&lt;/li&gt;
&lt;li&gt;在 GPU 上加速向量搜索的 RAFT 库综述&lt;/li&gt;
&lt;li&gt;GPU 加速向量搜索索引与 CPU 上最新技术的性能比较&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本系列的第二篇文章深入探讨了每一个 GPU 加速指数，并简要解释了算法的工作原理以及微调其行为的重要参数摘要。想要了解更多信息，请访问 &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/&#34;&gt;加速向量搜索：微调 GPU 索引算法&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
