<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mel-spectrogram on 时间飘过</title>
    <link>https://weedge.github.io/tags/mel-spectrogram/</link>
    <description>Recent content in mel-spectrogram on 时间飘过</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 14 Jan 2025 10:26:23 +0800</lastBuildDate><atom:link href="https://weedge.github.io/tags/mel-spectrogram/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>论文解读 Matcha-TTS: A fast TTS architecture with conditional flow matching</title>
      <link>https://weedge.github.io/post/multimoding/voices/matcha-tts/</link>
      <pubDate>Tue, 14 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/matcha-tts/</guid>
      <description>&lt;h1 id=&#34;-matcha-tts&#34;&gt;🍵 matcha-tts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.03199&#34;&gt;2024.1 &lt;strong&gt;Matcha-TTS: A fast TTS architecture with conditional flow matching&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/shivammehta25/Matcha-TTS&#34;&gt;paper code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要：&lt;/h1&gt;
&lt;p&gt;一种用于快速 TTS 声学建模的新编码器-解码器架构，使用最佳传输条件流匹配 (OT-CFM) 进行训练。与使用分数匹配训练的模型相比，这产生了基于 ODE 的解码器，能够以更少的合成步骤实现高输出质量。仔细的设计选择还确保每个合成步骤都能快速运行。该方法是概率性的、非自回归的，并且无需外部对齐即可从头开始学习说话。与强大的预训练基线模型相比，Matcha-TTS 系统具有最小的内存占用，可以与长语音上最快的模型相媲美，并在听力测试中获得最高的平均意见得分。&lt;/p&gt;
&lt;p&gt;一种非自回归神经 TTS 的新方法，它使用条件流匹配(CFM from &lt;a href=&#34;https://arxiv.org/abs/2210.02747&#34;&gt;2022.10 &lt;strong&gt;Flow Matching for Generative Modeling&lt;/strong&gt;&lt;/a&gt;)（类似于校正流(Rectified Flow &lt;a href=&#34;https://arxiv.org/abs/2209.03003&#34;&gt;2022.9 Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow&lt;/a&gt;）来加速基于 ODE 的语音合成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is probabilistic  是概率性的&lt;/li&gt;
&lt;li&gt;Has compact memory footprint 具有紧凑的内存占用&lt;/li&gt;
&lt;li&gt;Sounds highly natural  听起来非常自然&lt;/li&gt;
&lt;li&gt;Is very fast to synthesise from 合成速度非常快&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简洁的结构，训练推理快，使用更少额内存空间，&lt;/p&gt;
&lt;p&gt;一种基于连续归一化流的概率性、非自回归、快速采样的 TTS 声学模型。主要有两个创新点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提出了一种改进的编码器-解码器 TTS 结构，该结构在解码器中结合使用 1D CNN 和 Transformer。这减少了内存消耗并且可以快速评估，从而提高综合速度。&lt;/li&gt;
&lt;li&gt;使用最优传输条件流匹配 optimal-transport conditional flow matching(OT-CFM) 来训练这些模型，这是一种学习从数据分布中采样的 ODE 的新方法。与传统的 连续时间归一化流 CNF（continuous-time normalising flows ） 和分数匹配概率流 ODE （probability flow ODE） 相比，OT-CFM 定义了从源到目标的更简单的路径，从而能够以比 DPM（Diffusion probabilistic model） 更少的步骤进行准确合成。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;实验结果表明，这两种创新都加速了合成，减少了速度和合成质量之间的权衡。尽管速度快且轻量级，Matcha-TTS能够在不需要外部对齐器的情况下学习说话和对齐。与强大的预训练基线模型相比，Matcha-TTS实现了快速合成，并获得了更好的自然度评分。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;附&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用LJ Speech数据集训练202 epochs 的ckpt: &lt;a href=&#34;https://huggingface.co/weege007/matchaTTS/tree/main&#34;&gt;https://huggingface.co/weege007/matchaTTS/tree/main&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;笔记地址：https://github.com/weedge/doraemon-nb/blob/main/matcha_tts.ipynb&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>论文解读： VALL-E 系列</title>
      <link>https://weedge.github.io/post/multimoding/voices/vall-e-x/</link>
      <pubDate>Mon, 13 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/vall-e-x/</guid>
      <description>&lt;p&gt;前文讲到VITS，采用的端到端的NAR模型，这篇文章记录下微软提出的VALL-E系列，从 AR+NAR 到 AR 模型的转变，以及后面MELLE引入的VAE和Mel-Spectorgram，将neural codec text speech LM (AR+NAR Transformer Decoder) 转变为  autoregressive  mel-spectrogram text speech LM  (AR Transformer Decoder) ；由于LM生成的是mel-spectrogram 需要通过vocoder 转换成 waveform； 生成的内容采样模块：从top-p random sampling 变成 Latent Sampling潜在采样模块（思想源自VAE, 从预测的高斯分布中采样潜在嵌入，然后将其投影回频谱图空间）&lt;/p&gt;
&lt;h2 id=&#34;vall-e-系列&#34;&gt;VALL-E 系列&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/project/vall-e-x/&#34;&gt;https://www.microsoft.com/en-us/research/project/vall-e-x/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vall-E: &lt;a href=&#34;https://ar5iv.labs.arxiv.org/html/2301.02111&#34;&gt;https://ar5iv.labs.arxiv.org/html/2301.02111&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;具体来说，我们使用从现成的神经音频编解码器模型派生的离散代码来训练&lt;em&gt;&lt;strong&gt;神经编解码器语言模型（neural codec language model）&lt;/strong&gt;&lt;/em&gt;（称为VALL-E ），并将 TTS 视为条件语言建模任务，而不是像之前的工作那样将 TTS 视为连续信号回归。 在预训练阶段，我们将 TTS 训练数据扩展到 60K 小时的英语语音，比现有系统大数百倍。 VALL-E具有情境学习功能，可用于合成高质量的个性化语音，只需对看不见的说话者进行 3 秒的注册录音作为声音提示。实验结果表明， VALL-E在语音自然度和说话人相似度方面显着优于最先进的零样本 TTS 系统。此外，我们发现VALL-E可以在合成时保留说话者的情感和声音提示的声学环境。&lt;/p&gt;
&lt;p&gt;与之前的管道不同（例如，音素 → 梅尔谱图 → 波形）， VALL-E的管线是音素 → 离散码 → 波形。&lt;/p&gt;
&lt;p&gt;VALL-E根据音素和声学代码提示生成与目标内容和说话者的声音相对应的离散音频编解码器代码。 VALL-E直接支持各种语音合成应用，例如零样本 TTS、语音编辑以及与 GPT-3 等其他生成式 AI 模型相结合的内容创建。&lt;/p&gt;
&lt;p&gt;VALL-E系列：2023年的1月份开始 - 2024年的7月份&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VALL-E 使用从现成的神经音频编解码器模型派生的离散代码来训练神经编解码器语言模型，并将 TTS 视为条件语言建模任务，而不是像之前的工作那样将 TTS 视为连续信号回归。 VALL-E 具有情境学习功能，可用于合成高质量的个性化语音，只需录制未见过的讲话者的 3 秒注册录音作为提示。在语音自然度和说话人相似度方面，VALL-E 显着优于最先进的零样本 TTS 系统。此外，VALL-E可以在合成时保留说话者的情绪和声音提示的声学环境。&lt;/li&gt;
&lt;li&gt;VALL-E X 扩展其能力，适应多语言场景，促进跨语言零样本 TTS。&lt;/li&gt;
&lt;li&gt;VALL-E R 引入了音素单调对齐策略，增强了语音生成的鲁棒性。&lt;/li&gt;
&lt;li&gt;VALL-E 2 通过集成重复感知采样和分组代码建模技术， 实现了一个突破性的里程碑：在 LibriSpeech 和 VCTK 数据集上的零样本 TTS 性能与人类相当。这标志着此类成就的首次实例，为该领域树立了新标准。&lt;/li&gt;
&lt;li&gt;MELLE 是一种新颖的基于连续值标记的语言建模方法，用于文本到语音合成 (TTS)。 MELLE 直接从文本条件自回归生成连续的梅尔频谱图帧，绕过了矢量量化的需要，矢量量化最初是为音频压缩而设计的，与梅尔频谱图相比，牺牲了保真度。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>论文解读 VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</title>
      <link>https://weedge.github.io/post/multimoding/voices/vits/</link>
      <pubDate>Sat, 11 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/vits/</guid>
      <description>&lt;p&gt;前文讲到OpenVoicev2，补充下细节，然后梳理使用的基础模型VITS：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;📓&lt;/p&gt;
&lt;p&gt;melo-tts 生成原始音频：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenVoice 版本1不依赖melo-tts, 升级后的V2版本依赖melo-tts, 主要是生成原始音频质量加强了(由melo-tts生成);&lt;/li&gt;
&lt;li&gt;默认配置使用了TransformerCouplingBlock list作为flow 和 reverse flow, 而第一版的OpenVoice 模型使用的 ResidualCouplingBlock ;&lt;/li&gt;
&lt;li&gt;melo-tts的模型权重支持多语言，更具语言区分，比如 ZH: &lt;a href=&#34;https://huggingface.co/myshell-ai/MeloTTS-Chinese&#34;&gt;myshell-ai/MeloTTS-Chinese&lt;/a&gt;, EN_NEWEST: &lt;a href=&#34;https://huggingface.co/myshell-ai/MeloTTS-English-v3&#34;&gt;myshell-ai/MeloTTS-English-v3&lt;/a&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;音色转换生成目标音频：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过训练好的&lt;strong&gt;音色抽取器&lt;/strong&gt;抽取目标说话者的音色 (&lt;a href=&#34;https://huggingface.co/myshell-ai/OpenVoiceV2/tree/main/base_speakers/ses&#34;&gt;myshell-ai/OpenVoiceV2/converter&lt;/a&gt;)；&lt;/li&gt;
&lt;li&gt;生成的原始音频信息通过 训练抽取好的基础说话者的音色(&lt;a href=&#34;https://huggingface.co/myshell-ai/OpenVoiceV2/tree/main/converter&#34;&gt;myshell-ai/OpenVoiceV2/base_speakers/ses&lt;/a&gt;)，将原始音频中的音色去除 （flow）；&lt;/li&gt;
&lt;li&gt;将去除原始音色的音频 和 抽取好的目标说话者的音色 合并 （reverse flow）； 最终通过 vocoder(也是论文中的Decoder,使用的 HiFi-Gan模型)合成目标音频。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;额外注意的是，由melo-tts生成原始音频sample rate是 44100， 而通过音色提取器 提取 并且 生成目标音频sample rate是 22050&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;前提知识这里简单概括如下：&lt;/p&gt;
&lt;p&gt;AE(Autoencoder): 自编码器是&lt;a href=&#34;https://www.ibm.com/cn-zh/topics/self-supervised-learning&#34;&gt;自监督&lt;/a&gt;系统，其训练目标是通过降维来压缩（或&lt;em&gt;编码&lt;/em&gt;）输入数据，然后使用该压缩后的表示准确重建（或&lt;em&gt;解码&lt;/em&gt;）其原始输入。无泛化生成能力，但是可以执行特定任务：常用于数据压缩、图像去噪、异常检测和面部识别等任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VAE(Variational Autoencoder)&lt;/strong&gt; :与其他自编码器(Autoencoder(AE)的区别在于它们对潜在空间进行编码的独特方式，以及可以应用其概率编码的不同用例，即随机生成训练数据的变体。具有泛化生成能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CVAE(Conditional Variational Autoencoder)&lt;/strong&gt;: 条件变分自编码器 可以以特定输入为条件进行输出，而不仅仅是随机生成训练数据的变体。这是通过将监督学习（或半监督学习）的元素与常规自编码器的传统无监督训练目标相结合来实现的。具有指定特征的泛化能力。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;VAE 与 GAN的区别：&lt;/p&gt;
&lt;p&gt;VAE 经常与生成式对抗网络 (GAN) 进行比较，GAN 是另一种模型架构，用于生成类似于训练数据的样本，尤其是图像。&lt;/p&gt;
&lt;p&gt;与 VAE 类似，GAN 是结合两种神经网络的联合架构：一个生成器网络，负责输出与训练数据集中的图像相似的图像样本，另一个判别器网络，负责确定特定图像是训练数据中的“真实”图像还是来自生成器网络的“虚假”图像。&lt;/p&gt;
&lt;p&gt;这两个网络在零和博弈中进行对抗性训练：来自判别器的反馈用于改进生成器的输出，直到判别器不再能够区分真假样本。&lt;/p&gt;
&lt;p&gt;就图像合成而言，两者各有优劣：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GAN 可以生成更清晰的图像，但由于两种复合模型之间的对抗性权衡，在训练中并不稳定。&lt;/li&gt;
&lt;li&gt;VAE 更容易训练，但由于其根据训练数据的“平均”特征生成图像的性质，往往会生成比较模糊的图像。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VAE-GAN 两者结合
顾名思义，VAE-GAN 是变分自编码器 (VAE) 和生成式对抗网络 (GAN) 的混合体。通过用判别器网络替换 VAE 模型的重建损失项，来降低 VAE 生成图像的模糊性，提高生成质量。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;VITS 使用了 条件变分自编码器 (Conditional Variational Autoencoder (CVAE)) 和生成式对抗网络 (&lt;a href=&#34;https://en.wikipedia.org/wiki/Generative_adversarial_network&#34;&gt;Generative adversarial network&lt;/a&gt;(GAN)) 两个模型架构。 至于VAE和GAN的细节可以关注下baby-llm这个学习项目中的对应模块PR学习资料:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VAE: &lt;a href=&#34;https://github.com/ai-bot-pro/baby-llm/tree/main/modules/VAE&#34;&gt;https://github.com/ai-bot-pro/baby-llm/tree/main/modules/VAE&lt;/a&gt; | PR:  &lt;a href=&#34;https://github.com/ai-bot-pro/baby-llm/pull/13&#34;&gt;https://github.com/ai-bot-pro/baby-llm/pull/13&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN: &lt;a href=&#34;https://github.com/ai-bot-pro/baby-llm/tree/main/modules/GAN&#34;&gt;https://github.com/ai-bot-pro/baby-llm/tree/main/modules/GAN&lt;/a&gt; | PR: &lt;a href=&#34;https://github.com/ai-bot-pro/baby-llm/pull/12&#34;&gt;https://github.com/ai-bot-pro/baby-llm/pull/12&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这篇文章是讲解VITS，是现在工业上TTS常用的基础方案(NAR模型，成本相对AR模型低， 推理快，生成质量尽可能追平或超越SOTA AR模型)。作者来自韩国现代汽车公司的 AIR 实验室（人工智能研究实验室），论文结合了以前的研究成果：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.02155&#34;&gt;2018. &lt;strong&gt;FloWaveNet : A Generative Flow for Raw Audio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.11129&#34;&gt;2020. &lt;strong&gt;Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;2020. &lt;strong&gt;HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;后续作者还研究了加入扩散模型来生成语音，不需要使用分类器指导的目标说话者的任何转录。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.11755&#34;&gt;2022. Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Guided-TTS 将无条件扩散概率模型(unconditional Diffusion Model)与单独训练的音素分类器(phoneme classifier )相结合，用于分类器指导。无条件扩散模型学习在没有任何上下文的情况下从未转录的语音数据中生成语音。对于 TTS 合成，使用在大规模语音识别数据集上训练的音素分类器来指导扩散模型的生成过程。&lt;/p&gt;
&lt;h1 id=&#34;vits&#34;&gt;VITS&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.06103&#34;&gt;2021. &lt;strong&gt;Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/strong&gt;&lt;/a&gt;  | &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;paper coder&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;主要贡献：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出了一种并行的端到端 TTS 方法，它可以生成比当前两阶段模型更自然的音频；&lt;/li&gt;
&lt;li&gt;采用通过归一化流程和对抗性训练过程增强的变分推理，提高了生成模型的表达能力；&lt;/li&gt;
&lt;li&gt;一个随机持续时间预测器（stochastic duration predictor）来从输入文本中合成具有不同节奏的语音；&lt;/li&gt;
&lt;li&gt;通过对潜在变量的不确定性建模和随机持续时间预测器，表达了自然的一对多关系，其中文本输入可以以不同的音调（pitches）和节奏（rhythms）以多种方式说出。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过利用条件变分自编码器 CVAE，模型特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;学习直接从文本合成原始波形，而不需要额外的输入条件；&lt;/li&gt;
&lt;li&gt;使用动态编程方法 MAS 来搜索最佳对齐方式，而不是与计算损失相比,不需要任何外部对齐器；&lt;/li&gt;
&lt;li&gt;并行生成样本；&lt;/li&gt;
&lt;li&gt;高效的端到端训练方法, 并且生成质量优于最好的公开可用的两阶段模型。附两阶段的数据处理过程(在后续的研究论文中称之为级联方法(cascaded)，见&lt;a href=&#34;https://www.microsoft.com/en-us/research/project/vall-e-x/&#34;&gt;VALL-E&lt;/a&gt;系列论文研究)：
&lt;ul&gt;
&lt;li&gt;第一阶段是从预处理的文本中生成中间语音表示，例如梅尔谱图(mel-spectrograms)或语言特征(linguistic features)&lt;/li&gt;
&lt;li&gt;第二阶段是生成以中间表示为条件的原始波形。&lt;/li&gt;
&lt;li&gt;两阶段的相关模型大都是独立开发的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;结构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/3ddec975-a9fd-460c-91fa-894b8ebd8c8c&#34; alt=&#34;VITS&#34;&gt;&lt;/p&gt;
&lt;p&gt;PS： &lt;a href=&#34;https://github.com/ai-bot-pro/achatbot&#34;&gt;achatbot&lt;/a&gt; 集成了OpenVoiceV2 with meloTTS(meloTTS代码大部分来自VITS，Flow 采用 Transformer Encoder 结构来自 &lt;a href=&#34;https://arxiv.org/abs/2307.16430&#34;&gt;VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design&lt;/a&gt; | &lt;a href=&#34;https://github.com/daniilrobnikov/vits2&#34;&gt;paper code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PR地址： &lt;a href=&#34;https://github.com/ai-bot-pro/achatbot/pull/103&#34;&gt;https://github.com/ai-bot-pro/achatbot/pull/103&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>论文解读 OpenVoice: Versatile Instant Voice Cloning</title>
      <link>https://weedge.github.io/post/multimoding/voices/open_voice_extra_se_and_convert/</link>
      <pubDate>Sat, 11 May 2024 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/open_voice_extra_se_and_convert/</guid>
      <description>&lt;p&gt;使用meloTTS 本文生成的音频&lt;/p&gt;



&lt;figure &gt;
  &lt;audio controls class=&#34;player&#34; preload=&#34;&#34;&gt;
    &lt;source src=&#34;https://media.githubusercontent.com/media/weedge/paper-speaker/main/multimoding/voices/open_voice_inference/zh-tts.wav&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
  
  
&lt;/figure&gt;

&lt;p&gt;使用openVoice clone 自己的声音 阅读本文内容 


&lt;figure &gt;
  &lt;audio controls class=&#34;player&#34; preload=&#34;&#34;&gt;
    &lt;source src=&#34;https://media.githubusercontent.com/media/weedge/paper-speaker/main/multimoding/voices/open_voice_inference/clone-me-zh-tts.wav&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
  
  
&lt;/figure&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;文件直接上传在github中, 暂未走cdn, 缓存比较慢，可下载播放， 下载地址： &lt;a href=&#34;http://github.com/weedge/paper-speaker/tree/main/multimoding/voices/open_voice_inference&#34;&gt;http://github.com/weedge/paper-speaker/tree/main/multimoding/voices/open_voice_inference&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;openVoiceV2 tone color clone: base TTS + extra tone color + convert&lt;/p&gt;
&lt;p&gt;Base TTS: use meloTTS , 支持TTS模型训练，以及load Pre-Trained ckpt 进行TTS,  在 &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;VITS&lt;/a&gt;基础上支持多种语言；&lt;/p&gt;
&lt;p&gt;论文地址：&lt;a href=&#34;https://arxiv.org/abs/2312.01479&#34;&gt;OpenVoice: Versatile Instant Voice Cloning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;论文主作者：Zengyi Qin (同时是JetMoE的作者，站在巨人的肩膀上创新)&lt;/p&gt;
&lt;p&gt;公开的权重：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenVoice: &lt;a href=&#34;https://huggingface.co/myshell-ai/OpenVoice&#34;&gt;https://huggingface.co/myshell-ai/OpenVoice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenVoiceV2: &lt;a href=&#34;https://huggingface.co/myshell-ai/OpenVoiceV2&#34;&gt;https://huggingface.co/myshell-ai/OpenVoiceV2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;源码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/myshell-ai/OpenVoice&#34;&gt;https://github.com/myshell-ai/OpenVoice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/myshell-ai/MeloTTS&#34;&gt;https://github.com/myshell-ai/MeloTTS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;训练： MSML dataset 和 训练过程 未公开&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;附操作笔记&lt;/strong&gt;： &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/myshell_ai_OpenVoiceV2.ipynb&#34;&gt;https://github.com/weedge/doraemon-nb/blob/main/myshell_ai_OpenVoiceV2.ipynb&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
