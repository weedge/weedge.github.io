<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>non-G2P on </title>
    <link>https://weedge.github.io/tags/non-g2p/</link>
    <description>Recent content in non-G2P on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 19 Jan 2025 10:26:23 +0800</lastBuildDate><atom:link href="https://weedge.github.io/tags/non-g2p/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>è®ºæ–‡è§£è¯»ï¼šFish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis</title>
      <link>https://weedge.github.io/post/multimoding/voices/fishspeech/</link>
      <pubDate>Sun, 19 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/fishspeech/</guid>
      <description>&lt;h2 id=&#34;ç›¸å…³è®ºæ–‡&#34;&gt;ç›¸å…³è®ºæ–‡&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;base&lt;/strong&gt;: åŸºç¡€æ™®é€‚ç ”ç©¶&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hourglass transformers: &lt;a href=&#34;https://arxiv.org/abs/2110.13711&#34;&gt;2021. Hierarchical Transformers Are More Efficient Language Models&lt;/a&gt; | &lt;a href=&#34;https://github.com/lucidrains/simple-hierarchical-transformer&#34;&gt;lucidrains/simple-hierarchical-transformer&lt;/a&gt; vanilla layers and shortened layers use GPT  AR  GLM ğŸ¤&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.07185&#34;&gt;2023.5 &lt;strong&gt;MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers&lt;/strong&gt;&lt;/a&gt; (åˆ†åˆ«åœ¨æ–‡æœ¬ï¼Œå›¾ç‰‡ï¼Œè¯­éŸ³å»ºæ¨¡)| &lt;a href=&#34;https://github.com/lucidrains/MEGABYTE-pytorch&#34;&gt;lucidrains/MEGABYTE-pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PS: æƒ³æ³•å’Œè‡ªå·±æ•´ç†çš„&lt;a href=&#34;https://github.com/ai-bot-pro/baby-llm/tree/main/simpleLM&#34;&gt;simple LM&lt;/a&gt;ç›¸ä¼¼~&lt;/p&gt;
&lt;p&gt;æ‰©å±•é˜…è¯»ï¼š&lt;a href=&#34;https://arxiv.org/abs/2412.09871&#34;&gt;2024.12 &lt;strong&gt;Byte Latent Transformer: Patches Scale Better Than Tokens&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/facebookresearch/blt&#34;&gt;paper code&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;audio speech&lt;/strong&gt;: åœºæ™¯ç ”ç©¶&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;â­ï¸&lt;a href=&#34;https://arxiv.org/abs/2310.00704&#34;&gt;2023.10 UniAudio: &lt;strong&gt;An Audio Foundation Model Toward Universal Audio Generation&lt;/strong&gt;&lt;/a&gt; (çµæ„Ÿæ¥è‡ª MEGABYTEï¼Œå°†å…¶åº”ç”¨äºè¯­éŸ³æ¨¡å‹)| &lt;a href=&#34;https://github.com/yangdongchao/UniAudio&#34;&gt;paper code&lt;/a&gt;  (ä»£ç å¯æ‰©å±•ä»»åŠ¡è¿›è¡Œè®­ç»ƒ, å·²æ‰©å±•äº†éŸ³ä¹æ•°æ®)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/d0bc3097-3daa-47db-8bbb-f24f5aec800d&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;â­ï¸&lt;a href=&#34;https://arxiv.org/abs/2411.01156&#34;&gt;2024. &lt;strong&gt;Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis&lt;/strong&gt;&lt;/a&gt; (è®ºæ–‡ä¸­ä»‹ç»çš„Daul-AR, GFSQ, Firefly-GAN(FF-GAN) å¯¹EVA-GANæ”¹ç‰ˆï¼Œç»†èŠ‚éœ€è¦ç»“åˆä»£ç ç†è§£) | &lt;a href=&#34;https://github.com/fishaudio/fish-speech&#34;&gt;paper code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;é™„&lt;/strong&gt;ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;achatbot æ¥å…¥fishspeech tts colab ç¬”è®°ï¼š &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/achatbot_fishspeech_tts.ipynb&#34;&gt;https://github.com/weedge/doraemon-nb/blob/main/achatbot_fishspeech_tts.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;æ“ä½œç¬”è®°ï¼š &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/fish_speech_tts.ipynb&#34;&gt;https://github.com/weedge/doraemon-nb/blob/main/fish_speech_tts.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
