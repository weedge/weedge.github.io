<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on </title>
    <link>https://weedge.github.io/tags/llm/</link>
    <description>Recent content in LLM on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 07 Mar 2025 10:26:23 +0800</lastBuildDate><atom:link href="https://weedge.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>æ¨¡å‹ç³»ç»Ÿå·¥ç¨‹ï¼šæ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒå¹¶è¡Œç­–ç•¥</title>
      <link>https://weedge.github.io/post/llm/trainingparallelstrategy/</link>
      <pubDate>Fri, 07 Mar 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/trainingparallelstrategy/</guid>
      <description>&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=28692286&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;



&lt;h2 id=&#34;å¼•è¨€&#34;&gt;å¼•è¨€&lt;/h2&gt;
&lt;p&gt;ä»¥å‰åœ¨è®­ç»ƒæ¨¡å‹çš„æ—¶å€™å¤§éƒ¨åˆ†ä½¿ç”¨çš„æ˜¯å•æœºå•å¡è¿›è¡Œè®­ç»ƒæµ‹è¯•ï¼ŒçœŸæ­£ä½¿ç”¨å•æœºå¤šå¡å’Œå¤šæœºå¤šå¡çš„æ—¶å€™ï¼Œå¾ˆå°‘å»å®æ“åˆ°åˆ†å¸ƒå¼çš„è®­ç»ƒæ¨ç†ï¼Œæœ¬èº«è¿™å—å¯¹åº”ç¡¬ä»¶çš„æˆæœ¬é«˜ï¼Œå¯¹äºä¸ªäººæ˜¯å¾ˆå°‘å¯ä»¥å»æŠŠç©ä¸Šçš„ï¼Œæ›´ä½•å†µç°åœ¨è®­ç»ƒå¤§æ¨¡å‹å¤§éƒ¨åˆ†ä½¿ç”¨çš„PTå¥½çš„æ¨¡å‹ï¼Œè¿›è¡Œå¾®è°ƒå’Œè’¸é¦ï¼Œæˆ–è€…é‡åŒ–éƒ¨ç½²ï¼Œéšç€deepseekç³»åˆ—æ¨¡å‹çš„å¼€æºï¼Œå¯¹æ¨¡å‹çš„è®­ç»ƒå¾®è°ƒå’Œéƒ¨ç½²çš„éœ€æ±‚å¢å¤šï¼Œé‡Œé¢æ¶‰åŠåˆ°çš„åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼Œæ€æ ·å¯¹æ¨¡å‹å’Œæ•°æ®è¿›è¡Œæ‹†åˆ†è¿›è¡Œå¤šå¡små¹¶è¡Œå¤„ç†, è¿™é‡Œç®€å•ä»‹ç»ä¸‹ç›¸å…³çš„åˆ†å¸ƒå¼è®­ç»ƒæ¨ç†å¹¶è¡Œç­–ç•¥ï¼Œä»¥åŠå¯¹åº”çš„ä»£ç ï¼Œä»£ç ä¸»è¦ä½¿ç”¨pytorchè¿›è¡Œå®ç°,é‡‡ç”¨å•èŠ‚ç‚¹çš„2ä¸ªgpuã€‚(å¦‚æœä½ æœ‰é’±çš„è¯ï¼Œå¯ä»¥å¤ç°ä¸‹ZeROæˆ–è€…Megatronä¸­çš„å®ç°ï¼‰&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ’¡ä½¿ç”¨ PyTorch è¿›è¡Œæ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹ï¼š&lt;/p&gt;
&lt;p&gt;PyTorch å°†å€¼ç»„ç»‡æˆTensor ï¼Œ Tensoræ˜¯å…·æœ‰ä¸°å¯Œæ•°æ®æ“ä½œæ“ä½œçš„é€šç”¨ n ç»´æ•°ç»„ã€‚Module å®šä¹‰ä»è¾“å…¥å€¼åˆ°è¾“å‡ºå€¼çš„è½¬æ¢ï¼Œå…¶åœ¨æ­£å‘ä¼ é€’æœŸé—´çš„è¡Œä¸ºç”±å…¶forwardæˆå‘˜å‡½æ•°æŒ‡å®šã€‚Module å¯ä»¥åŒ…å«Tensorä½œä¸ºå‚æ•°ã€‚ä¾‹å¦‚ï¼Œçº¿æ€§æ¨¡å—åŒ…å«æƒé‡å‚æ•°å’Œåå·®å‚æ•°ï¼Œå…¶æ­£å‘å‡½æ•°é€šè¿‡å°†è¾“å…¥ä¸æƒé‡ç›¸ä¹˜å¹¶æ·»åŠ åå·®æ¥ç”Ÿæˆè¾“å‡ºã€‚åº”ç”¨ç¨‹åºé€šè¿‡åœ¨è‡ªå®šä¹‰æ­£å‘å‡½æ•°ä¸­å°†æœ¬æœºModule ï¼ˆ&lt;em&gt;ä¾‹å¦‚&lt;/em&gt;çº¿æ€§ã€å·ç§¯ç­‰ï¼‰å’ŒFunction ï¼ˆä¾‹å¦‚reluã€pool ç­‰ï¼‰æ‹¼æ¥åœ¨ä¸€èµ·æ¥ç»„æˆè‡ªå·±çš„Module ã€‚å…¸å‹çš„è®­ç»ƒè¿­ä»£åŒ…å«ä½¿ç”¨è¾“å…¥å’Œæ ‡ç­¾ç”ŸæˆæŸå¤±çš„å‰å‘ä¼ é€’ã€ç”¨äºè®¡ç®—å‚æ•°æ¢¯åº¦çš„åå‘ä¼ é€’ä»¥åŠä½¿ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°çš„ä¼˜åŒ–å™¨æ­¥éª¤ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œåœ¨æ­£å‘ä¼ é€’æœŸé—´ï¼ŒPyTorch ä¼šæ„å»ºä¸€ä¸ªè‡ªåŠ¨æ±‚å¯¼å›¾æ¥è®°å½•æ‰§è¡Œçš„æ“ä½œã€‚ç„¶åï¼Œåœ¨åå‘ä¼ æ’­ä¸­ï¼Œå®ƒä½¿ç”¨è‡ªåŠ¨æ¢¯åº¦å›¾è¿›è¡Œåå‘ä¼ æ’­ä»¥ç”Ÿæˆæ¢¯åº¦ã€‚æœ€åï¼Œä¼˜åŒ–å™¨åº”ç”¨æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ã€‚è®­ç»ƒè¿‡ç¨‹é‡å¤è¿™ä¸‰ä¸ªæ­¥éª¤ï¼Œç›´åˆ°æ¨¡å‹æ”¶æ•›ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;å¹¶è¡Œç­–ç•¥ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ç§ï¼š&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/9616a34d-c9f3-4db9-b2a1-a79342a95856&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Data Parallelism (DP) åŒ…æ‹¬ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distributed Data Parallelismï¼ˆDDPï¼‰&lt;/li&gt;
&lt;li&gt;Fully-Shared Data Parallelism (FSDP)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model Parallelismï¼ˆMPï¼‰åŒ…æ‹¬ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tensor Parallelismï¼ˆTP, æœ‰äº›è®ºæ–‡ä¸­å°†TPæè¿°æˆMP,æ¯”å¦‚Megatron-LM, ZeROï¼‰&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pipeline Parallelism (PP)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Expert Parallelism (EP)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Activation Partitioning åŒ…æ‹¬ï¼š(SPå’ŒCPä¸¤è€…é€šå¸¸å’ŒTPä¸€èµ·ä½¿ç”¨)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sequence Parallelism (SP)ï¼š é’ˆå¯¹åºåˆ—åˆ‡åˆ†ï¼Œåœ¨æ¨¡å—çš„è¾“å…¥å’Œè¾“å‡ºä¾§å¯¹åºåˆ—è¿›è¡Œåˆ‡åˆ†ï¼Œå¸¸ä½¿ç”¨åœ¨LayerNorm,Dropoutï¼ˆé€šè¿‡all_gather,reduce-scatter é€šä¿¡è®¡ç®—ï¼Œå‡å°‘è®­ç»ƒæ—¶æ¿€æ´»å€¼ gpuå†…å­˜å ç”¨ï¼Œä»¥å‰æ˜¯æ¯ä¸ªå¡ä¸Šå•ç‹¬çš„å‰¯æœ¬LayerNorm,Dropoutï¼Œå†—ä½™gpuå†…å­˜ï¼‰&lt;/li&gt;
&lt;li&gt;Context Parallelism (CP): ä¸»è¦é’ˆå¯¹Transformeræ¨¡å‹é•¿åºåˆ—çš„è®­ç»ƒã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DPå’ŒMPçš„æ¯”è¾ƒï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;DP çš„æ‰©å±•æ•ˆç‡æ¯” MP æ›´å¥½ï¼Œå› ä¸º MP é™ä½äº†è®¡ç®—çš„ç²’åº¦ï¼ŒåŒæ—¶ä¹Ÿå¢åŠ äº†é€šä¿¡å¼€é”€ã€‚è¶…è¿‡æŸä¸ªç‚¹åï¼Œè¾ƒä½çš„è®¡ç®—ç²’åº¦ä¼šé™ä½æ¯ä¸ª GPU çš„æ•ˆç‡ï¼Œè€Œå¢åŠ çš„é€šä¿¡å¼€é”€ä¼šé˜»ç¢è·¨ GPU çš„å¯æ‰©å±•æ€§ï¼Œå°¤å…¶æ˜¯è·¨è¶ŠèŠ‚ç‚¹è¾¹ç•Œæ—¶ã€‚ç›¸åï¼ŒDP æ—¢å…·æœ‰æ›´é«˜çš„è®¡ç®—ç²’åº¦ï¼Œåˆå…·æœ‰æ›´ä½çš„é€šä¿¡é‡ï¼Œä»è€Œå¯ä»¥å®ç°æ›´é«˜çš„æ•ˆç‡ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DP å†…å­˜(GPU HBM)æ•ˆç‡ä½ï¼Œå› ä¸ºæ¨¡å‹çŠ¶æ€åœ¨æ‰€æœ‰æ•°æ®å¹¶è¡Œè¿›ç¨‹ä¸­å†—ä½™å­˜å‚¨ã€‚ç›¸åï¼ŒMP å¯¹æ¨¡å‹çŠ¶æ€è¿›è¡Œåˆ†åŒºï¼Œä»¥æé«˜å†…å­˜æ•ˆç‡ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DPå’ŒMPéƒ½ä¿å­˜äº†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦çš„æ‰€æœ‰æ¨¡å‹çŠ¶æ€ï¼Œä½†å¹¶ä¸æ˜¯æ‰€æœ‰çŠ¶æ€éƒ½æ˜¯ä¸€ç›´éœ€è¦çš„ï¼Œæ¯”å¦‚æ¯ä¸€å±‚å¯¹åº”çš„å‚æ•°åªåœ¨è¯¥å±‚çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­æ—¶æ‰éœ€è¦ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;æ¨¡å‹è®­ç»ƒçš„å¹¶è¡Œç­–ç•¥åŒæ ·é€‚ç”¨äºæ¨ç†æµ‹çš„å¹¶è¡Œç­–ç•¥ï¼Œä½†æ˜¯è°ƒåº¦å’Œæ‰§è¡Œçš„æ–¹å¼ç¨æœ‰ä¸åŒã€‚è¿™é‡Œä¸»è¦æ˜¯ç»“åˆpytorch å®ç°ç®€å•çš„å¤šå¡æ¨¡å‹å¹¶è¡Œç­–ç•¥è®­ç»ƒã€‚&lt;/p&gt;
&lt;p&gt;ç›¸å…³çš„åˆ†å¸ƒå¼è®­ç»ƒæ¨ç†å¹¶è¡Œç­–ç•¥ï¼Œä»¥åŠå¯¹åº”çš„ä»£ç è¿è¡Œæ“ä½œdemoï¼Œå¯ä»¥æŸ¥çœ‹è¿™ä¸ªPR:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ai-bot-pro/achatbot/pull/127&#34;&gt;https://github.com/ai-bot-pro/achatbot/pull/127&lt;/a&gt;  (å¦‚æœ‰ä¸å¯¹ï¼Œæ¬¢è¿æŒ‡å‡º~)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>è®ºæ–‡è§£è¯» DeLighT: Very Deep and Light-weight Transformers</title>
      <link>https://weedge.github.io/post/paper/transformer/delight/</link>
      <pubDate>Sun, 28 Apr 2024 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/paper/transformer/delight/</guid>
      <description>&lt;p&gt;åœ¨çœ‹apple æœ€è¿‘å‘å¸ƒçš„OpenELM æ¨¡å‹ï¼Œå…¶è®ºæ–‡ä¸­æåˆ° block-wise scaling æ¨¡å‹ç»“æ„ä¼˜åŒ–æ–¹æ³•ï¼Œï¼ˆè®ºæ–‡è§ï¼š &lt;a href=&#34;https://machinelearning.apple.com/research/openelm&#34;&gt;&lt;strong&gt;OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework&lt;/strong&gt;&lt;/a&gt;ï¼‰ï¼Œè¿™é‡Œè®°å½•ä¸‹DeLighTè®ºæ–‡ä¸­çš„ block-wise scalingï¼Œç¿»è¯‘æ•´ç†ä¸‹ä»¥ä¾¿å¯¹ç…§ä»£ç å®ç°ï¼Œäº†è§£èƒŒæ™¯å’ŒåŸç†ã€‚DeLighTè®ºæ–‡ä¸­çš„å®éªŒä»»åŠ¡ä¸»è¦æ˜¯åœ¨ä¸¤ä¸ªæ ‡å‡†çš„åºåˆ—å»ºæ¨¡ä»»åŠ¡ä¸Šè¯„ä¼°äº†DeLighTçš„æ€§èƒ½ï¼šæœºå™¨ç¿»è¯‘ï¼ˆmachine translationï¼‰ä»»åŠ¡ encoder-decoder architecture å’Œ è¯­è¨€å»ºæ¨¡ï¼ˆ language modelingï¼‰decoder architectureï¼Œè®ºæ–‡ä¸­æœºå™¨ç¿»è¯‘ä»»åŠ¡æœªå¯¹En-Zh(è‹±æ–‡è¯‘ä¸­æ–‡)è¿›è¡Œå®éªŒï¼Œå¯ä»¥ä½œä¸ºä¸€ä¸ªå¤ç°ç»ƒä¹ ï¼Œæ ¹æ®æºç å®æ“ä¸€ä¸‹è®ºæ–‡ä¸­çš„å®éªŒï¼›è€Œè¯­è¨€å»ºæ¨¡å¯ä»¥ä½œä¸ºopenELMçš„æ¥æºå»¶ä¼¸~ ç»“åˆcornetè¿›è¡Œå¤ç°(ä¹Ÿæœ‰mxlç¤ºä¾‹ï¼Œmxlé’ˆå¯¹Apple Silicon ç¡¬ä»¶è¿›è¡Œçš„ä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¡†æ¶)ã€‚&lt;/p&gt;
&lt;p&gt;è®ºæ–‡ä¸»ä½œè€…ï¼š&lt;a href=&#34;https://sacmehta.github.io/&#34;&gt;Sachin Mehta &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/pdf/2008.00623&lt;/p&gt;
&lt;p&gt;è®ºæ–‡ä»£ç ï¼š &lt;a href=&#34;https://github.com/sacmehta/delight&#34;&gt;https://github.com/sacmehta/delight&lt;/a&gt; ï¼ˆåŸºäºå½“æ—¶facebookçš„ &lt;a href=&#34;https://github.com/facebookresearch/fairseq&#34;&gt;fairseq&lt;/a&gt; seq2seqå·¥å…·åº“å¼€å‘ï¼‰&lt;/p&gt;
&lt;p&gt;è¯¥è®ºæ–‡ç ”ç©¶æ˜¯åœ¨ä½œè€…ä»¥å‰çš„DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling è¿›è¡Œæ”¹è¿›ï¼Œæ¨¡å‹ç»“æ„å¼•å…¥æ›´å¤šçš„GLTsï¼Œæ¥å­¦ä¹ æ›´å®½çš„æƒé‡ï¼Œå¹¶ä¸”å‡å°‘äº†å‚æ•°æ•°é‡ã€‚&lt;/p&gt;
&lt;h2 id=&#34;æ‘˜è¦&#34;&gt;&lt;strong&gt;æ‘˜è¦&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ·±åº¦ä¸”è½»é‡çº§çš„Transformerï¼Œåä¸ºDeLighTï¼Œå®ƒåœ¨å‚æ•°æ•°é‡æ˜¾è‘—å‡å°‘çš„æƒ…å†µä¸‹ï¼Œæä¾›äº†ä¸æ ‡å‡†åŸºäºTransformerçš„æ¨¡å‹ç›¸ä¼¼æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚DeLighTæ›´æœ‰æ•ˆåœ°åœ¨æ¯ä¸ªTransformerå—å†…éƒ¨ï¼ˆé€šè¿‡DeLighTå˜æ¢ï¼‰ä»¥åŠè·¨å—ï¼ˆé€šè¿‡å—çº§ç¼©æ”¾ï¼‰åˆ†é…å‚æ•°ï¼Œå…è®¸è¾“å…¥ç«¯ä½¿ç”¨è¾ƒæµ…è¾ƒçª„çš„DeLighTå—ï¼Œè¾“å‡ºç«¯ä½¿ç”¨è¾ƒå®½è¾ƒæ·±çš„DeLighTå—ã€‚æ€»ä½“è€Œè¨€ï¼ŒDeLighTç½‘ç»œæ¯”æ ‡å‡†Transformeræ¨¡å‹æ·±2.5åˆ°4å€ï¼Œä½†å‚æ•°å’Œè¿ç®—é‡æ›´å°‘ã€‚åœ¨åŸºå‡†æœºå™¨ç¿»è¯‘å’Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeLighTåœ¨å¹³å‡å‚æ•°æ•°é‡å‡å°‘2åˆ°3å€çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°æˆ–æé«˜äº†åŸºçº¿Transformerçš„æ€§èƒ½ã€‚&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>è§£è¯»è®ºæ–‡ï¼šLeave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</title>
      <link>https://weedge.github.io/post/paper/transformer/infini_attention/</link>
      <pubDate>Fri, 12 Apr 2024 10:26:12 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/paper/transformer/infini_attention/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://aiptcomics.com/ezoimgfmt/i0.wp.com/aiptcomics.com/wp-content/uploads/2024/04/transformers-7.jpg?w=1500&amp;amp;ssl=1&amp;amp;ezimgfmt=ngcb4/notWebP&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;å›¾ç‰‡æ¥æºï¼š &lt;a href=&#34;https://aiptcomics.com/2024/04/10/transformers-7-2024-review/&#34;&gt;https://aiptcomics.com/2024/04/10/transformers-7-2024-review/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;æ‘˜è¦&lt;/strong&gt;ï¼š æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå°†åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ‰©å±•åˆ°æ— é™é•¿çš„è¾“å…¥ï¼ŒåŒæ—¶å—åˆ°å†…å­˜å’Œè®¡ç®—çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„å…³é”®ç»„æˆéƒ¨åˆ†æ˜¯ä¸€ç§æ–°çš„æ³¨æ„åŠ›æŠ€æœ¯ï¼Œç§°ä¸ºInfini-attentionã€‚Infini-attentionå°†ä¸€ç§å‹ç¼©å†…å­˜é›†æˆåˆ°äº†ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå¹¶åœ¨å•ä¸ªTransformerå—ä¸­æ„å»ºäº†æ©ç å±€éƒ¨æ³¨æ„åŠ›å’Œé•¿æœŸçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬é€šè¿‡åœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡åŸºå‡†ã€1Måºåˆ—é•¿åº¦çš„å£ä»¤(keypass)ä¸Šä¸‹æ–‡å—æ£€ç´¢å’Œ500Ké•¿åº¦çš„ä¹¦ç±æ‘˜è¦ä»»åŠ¡ä¸­ä½¿ç”¨1Bå’Œ8B LLMsï¼Œå±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†æœ€å°çš„æœ‰ç•Œå†…å­˜å‚æ•°ï¼Œå¹¶å®ç°äº†LLMsçš„å¿«é€Ÿæµå¼æ¨ç†ã€‚&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;æ³¨&lt;/strong&gt;ï¼šä¸ºè§£å†³å¤§æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†è¶…é•¿è¾“å…¥åºåˆ—æ—¶é‡åˆ°çš„å†…å­˜é™åˆ¶é—®é¢˜ï¼Œæœ¬æ–‡ä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„ï¼šInfini-Transformerï¼Œå®ƒå¯ä»¥åœ¨æœ‰é™å†…å­˜æ¡ä»¶ä¸‹ï¼Œè®©åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é«˜æ•ˆå¤„ç†æ— é™é•¿çš„è¾“å…¥åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼šInfini-Transformeråœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šè¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå†…å­˜æœ€é«˜å¯èŠ‚çº¦114å€ã€‚&lt;/p&gt;
&lt;p&gt;æ„Ÿè§‰æœ‰ç§å¤–æŒ‚å­˜å‚¨åº“(ç±»ä¼¼å‘é‡æ•°æ®åº“)åµŒå…¥åˆ°æ¨¡å‹ç»“æ„ä¸­ã€‚æ¯”å¦‚ï¼š &lt;a href=&#34;https://arxiv.org/abs/2203.08913&#34;&gt;Memorizing Transformers&lt;/a&gt; + &lt;a href=&#34;https://github.com/lucidrains/memorizing-transformers-pytorch&#34;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;åœ¨è®ºæ–‡ã€ŠMemorizing Transformersã€‹ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºkNN-augmented attention layerï¼Œå®ƒç»“åˆäº†å±€éƒ¨ä¸Šä¸‹æ–‡çš„å¯†é›†è‡ªæ³¨æ„åŠ›å’Œå¯¹å¤–éƒ¨è®°å¿†çš„è¿‘ä¼¼k-æœ€è¿‘é‚»ï¼ˆkNNï¼‰æœç´¢ã€‚è¿™ä¸ªæœºåˆ¶çš„å…³é”®éƒ¨åˆ†ä¹‹ä¸€æ˜¯ä½¿ç”¨äº†ä¸€ä¸ªé—¨æ§æœºåˆ¶ï¼ˆgating mechanismï¼‰æ¥ç»“åˆå±€éƒ¨æ³¨æ„åŠ›å’Œå¤–éƒ¨è®°å¿†çš„æ³¨æ„åŠ›ã€‚&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>è®ºæ–‡ï¼šRetrieval-Augmented Generation for Large Language Models: A Survey [v4]</title>
      <link>https://weedge.github.io/post/paper/rag/rag-for-llms-a-survey/</link>
      <pubDate>Fri, 08 Mar 2024 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/paper/rag/rag-for-llms-a-survey/</guid>
      <description>&lt;p&gt;å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç¤ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†é¢ä¸´ç€å¹»è§‰ã€è¿‡æ—¶çŸ¥è¯†å’Œä¸é€æ˜ã€ä¸å¯è¿½è¸ªçš„æ¨ç†è¿‡ç¨‹ç­‰æŒ‘æˆ˜ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²ç»æˆä¸ºä¸€ä¸ªæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ•´åˆå¤–éƒ¨æ•°æ®åº“çš„çŸ¥è¯†ã€‚è¿™å¢å¼ºäº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ï¼Œç‰¹åˆ«é€‚ç”¨äºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼Œå¹¶å…è®¸æŒç»­çš„çŸ¥è¯†æ›´æ–°å’Œé¢†åŸŸç‰¹å®šä¿¡æ¯çš„æ•´åˆã€‚RAGé€šè¿‡å°†LLMsçš„å†…åœ¨çŸ¥è¯†ä¸åºå¤§ã€åŠ¨æ€çš„å¤–éƒ¨æ•°æ®åº“èµ„æºç›¸ç»“åˆï¼Œäº§ç”Ÿäº†ååŒæ•ˆåº”ã€‚è¿™ç¯‡ç»¼è¿°è®ºæ–‡è¯¦ç»†è€ƒå¯Ÿäº†RAGèŒƒå¼çš„å‘å±•ï¼ŒåŒ…æ‹¬æœ´ç´ RAGã€é«˜çº§RAGå’Œæ¨¡å—åŒ–RAGã€‚å®ƒå¯¹RAGæ¡†æ¶çš„ä¸‰æ–¹åŸºç¡€è¿›è¡Œäº†ç»†è‡´çš„äº†è§£ï¼Œå…¶ä¸­åŒ…æ‹¬æ£€ç´¢ã€ç”Ÿæˆå’Œå¢å¼ºæŠ€æœ¯ã€‚è¯¥è®ºæ–‡å¼ºè°ƒåµŒå…¥(embedding)åœ¨æ¯ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶æå¯¹RAGç³»ç»Ÿè¿›å±•çš„æ·±å…¥ç ”ç©¶äº†è§£ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡ä»‹ç»äº†è¯„ä¼°RAGæ¨¡å‹çš„æŒ‡æ ‡å’ŒåŸºå‡†ï¼Œä»¥åŠæœ€æ–°çš„è¯„ä¼°æ¡†æ¶ã€‚æœ€åï¼Œè¯¥è®ºæ–‡è®²äº†ä¸€äº›ç ”ç©¶å‰æ™¯ï¼ŒåŒ…æ‹¬æœªæ¥æŒ‘æˆ˜ã€å¤šæ¨¡æ€çš„æ‰©å±•ä»¥åŠRAGåŸºç¡€è®¾æ–½åŠå…¶ç”Ÿæ€ç³»ç»Ÿçš„è¿›å±•&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;ã€‚&lt;/p&gt;
&lt;p&gt;è®ºæ–‡åœ°å€:  &lt;a href=&#34;https://arxiv.org/pdf/2312.10997.pdf&#34;&gt;Retrieval-Augmented Generation for Large Language Models: A Survey&lt;/a&gt; |  &lt;a href=&#34;https://github.com/Tongji-KGLLM/RAG-Survey/blob/main/assets/RAG_Slide_ENG.pdf&#34;&gt;PPT&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;æ³¨ï¼š ä¸»è¦æ˜¯äº†è§£RAGçš„å‘å±•è¿‡ç¨‹(å¬å›ç‡)ï¼Œä»¥åŠå¯¹ç›¸å…³å­æ¨¡å—é¢†åŸŸçš„ç°é˜¶æ®µäº†è§£ï¼Œå¦‚æœæ„Ÿå…´è¶£ï¼Œé€šè¿‡ç´¢å¼•åˆ°è®ºæ–‡å¼•ç”¨å¤„è¿›ä¸€æ­¥äº†è§£ã€‚(æé«˜çœ‹ç›¸åº”è®ºæ–‡çš„å‡†ç¡®ç‡)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LLM çŸ¥è¯†ç‚¹ All u need</title>
      <link>https://weedge.github.io/post/llm/llm-knowledge-point-all-u-need/</link>
      <pubDate>Mon, 01 Jan 2024 20:26:12 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/llm-knowledge-point-all-u-need/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/LLM.png&#34; alt=&#34;LLMçŸ¥è¯†ç‚¹&#34;&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šå›¾ç»™å‡ºäº†å­¦ä¹ LLMæ‰€éœ€è¦çš„çŸ¥è¯†ç‚¹ã€‚&lt;/p&gt;
&lt;p&gt;è¯¥æ–‡ä¸»è¦æ˜¯æ¢³ç†LLMåŸºç¡€ç»“æ„çŸ¥è¯†ç‚¹ï¼Œæ¨¡å‹ç»“æ„å¤§å¤šç›¸åŒï¼Œä»¥llama2æ¨¡å‹ç»“æ„ä¸ºåˆ‡å…¥ç‚¹ï¼Œæ¢³ç†ç›¸å…³çŸ¥è¯†ç‚¹ï¼Œä»¥ä¾¿æ„å»ºæ•´ä½“çŸ¥è¯†ä½“ç³»ï¼Œå¯æ–¹ä¾¿å¿«é€Ÿé˜…è¯»å…¶ä»–è®ºæ–‡çš„æ”¹è¿›ç‚¹ï¼›ç»“åˆ&lt;a href=&#34;https://weedge.github.io/post/llm/llm-knowledge-point-all-u-need/#%E5%8F%82%E8%80%83%E5%AD%A6%E4%B9%A0&#34;&gt;å‚è€ƒå­¦ä¹ &lt;/a&gt;ä¸­ç»™å‡ºçš„é“¾æ¥è¡¥å……åŸºç¡€çŸ¥è¯†ã€‚&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>è¯‘ï¼šæŒæ¡ LLM æŠ€æœ¯ï¼šæ¨ç†ä¼˜åŒ–</title>
      <link>https://weedge.github.io/post/llm/mastering-llm-techniques-inference-optimization/</link>
      <pubDate>Sat, 30 Dec 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/mastering-llm-techniques-inference-optimization/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/mastering-llm-techniques-inference-optimization/0.png&#34; alt=&#34;llm-optimize-deploy&#34;&gt;&lt;/p&gt;
&lt;p&gt;å°†transformerå±‚å ä»¥åˆ›å»ºå¤§å‹æ¨¡å‹ä¼šåœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸­å¸¦æ¥æ›´é«˜çš„å‡†ç¡®æ€§ã€å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œç”šè‡³æ¥è¿‘äººç±»çš„æ–°å…´èƒ½åŠ›ã€‚è¿™äº›åŸºç¡€æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆæœ¬é«˜æ˜‚ï¼Œè€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ˆä¸€ä¸ªç»å¸¸å‘ç”Ÿçš„æˆæœ¬ï¼‰å¯èƒ½éœ€è¦å¤§é‡å†…å­˜å’Œè®¡ç®—èµ„æºã€‚å¦‚ä»Šæœ€å—æ¬¢è¿çš„&lt;a href=&#34;https://www.nvidia.com/en-us/glossary/data-science/large-language-models/&#34;&gt;å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰&lt;/a&gt;å¯ä»¥è¾¾åˆ°æ•°ç™¾äº¿åˆ°æ•°åƒäº¿ä¸ªå‚æ•°çš„è§„æ¨¡ï¼Œå¹¶ä¸”æ ¹æ®ä½¿ç”¨æƒ…å†µï¼Œå¯èƒ½éœ€è¦å¤„ç†é•¿è¾“å…¥ï¼ˆæˆ–ä¸Šä¸‹æ–‡ï¼‰ï¼Œè¿™ä¹Ÿä¼šå¢åŠ æˆæœ¬ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡è®¨è®ºäº†LLMæ¨ç†ä¸­æœ€ç´§è¿«çš„æŒ‘æˆ˜ï¼Œä»¥åŠä¸€äº›å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚è¯»è€…åº”è¯¥å¯¹&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;transformeræ¶æ„&lt;/a&gt;å’Œæ³¨æ„åŠ›æœºåˆ¶æœ‰åŸºæœ¬çš„ç†è§£ã€‚ç†è§£LLMæ¨ç†çš„å¤æ‚æ€§è‡³å…³é‡è¦ï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†è¿›è¡Œä»‹ç»ã€‚&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;æ³¨&lt;/strong&gt;ï¼šä¸Šç¯‡è¯‘æ–‡æœ‰å¯¹ transformer æœ‰ç›¸å…³çš„ä»‹ç»ï¼Œä»¥åŠç›¸å…³ç¼–ç ç¬”è®°å…¥é—¨ï¼›æˆ–è€…æ·±å…¥å­¦ä¹ &lt;a href=&#34;https://web.stanford.edu/class/cs25/prev_years/2023_winter/index.html&#34;&gt;CS25: Transformers United V2&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&#34;&gt;video&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>è¯‘ï¼šå¤§å‹è¯­è¨€æ¨¡å‹å…¥é—¨ä»‹ç»</title>
      <link>https://weedge.github.io/post/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/</link>
      <pubDate>Mon, 04 Dec 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/transformers.svg&#34; alt=&#34;åŸå§‹çš„ Transformer æ¨¡å‹ç»“æ„&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ç®€ä»‹&#34;&gt;ç®€ä»‹&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡æ—¨åœ¨è®©æ²¡æœ‰è®¡ç®—æœºç§‘å­¦èƒŒæ™¯çš„äººæ·±å…¥äº†è§£ ChatGPT å’Œç±»ä¼¼çš„ AI ç³»ç»Ÿï¼ˆGPT-3ã€GPT-4ã€Bing Chatã€Bard ç­‰ï¼‰çš„å·¥ä½œåŸç†ã€‚ChatGPT æ˜¯ä¸€ä¸ªèŠå¤©æœºå™¨äººâ€”â€”ä¸€ç§æ„å»ºçš„å¯¹è¯å¼äººå·¥æ™ºèƒ½â€”â€”ä½†å»ºç«‹åœ¨&lt;em&gt;å¤§å‹è¯­è¨€æ¨¡å‹&lt;/em&gt;ä¹‹ä¸Šã€‚æˆ‘ä»¬å°†æŠŠå®ƒä»¬å…¨éƒ¨åˆ†è§£ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå®ƒä»¬èƒŒåçš„æ ¸å¿ƒæ¦‚å¿µã€‚æœ¬æ–‡ä¸éœ€è¦ä»»ä½•æŠ€æœ¯æˆ–æ•°å­¦èƒŒæ™¯ã€‚æˆ‘ä»¬å°†å¤§é‡ä½¿ç”¨éšå–»æ¥è¯´æ˜è¿™äº›æ¦‚å¿µã€‚æˆ‘ä»¬å°†è®¨è®ºä¸ºä»€ä¹ˆæ ¸å¿ƒæ¦‚å¿µä»¥å®ƒä»¬çš„æ–¹å¼å·¥ä½œï¼Œä»¥åŠæˆ‘ä»¬å¯ä»¥æœŸæœ›æˆ–ä¸æœŸæœ›åƒ ChatGPT è¿™æ ·çš„å¤§å‹è¯­è¨€æ¨¡å‹åšä»€ä¹ˆã€‚&lt;/p&gt;
&lt;p&gt;è¿™å°±æ˜¯æˆ‘ä»¬è¦åšçš„äº‹æƒ…ã€‚æˆ‘ä»¬å°†æ¸©å’Œåœ°ä»‹ç»ä¸€äº›ä¸å¤§å‹è¯­è¨€æ¨¡å‹å’Œ ChatGPT ç›¸å…³çš„æœ¯è¯­ï¼Œä¸ä½¿ç”¨ä»»ä½•è¡Œè¯ã€‚å¦‚æœæˆ‘å¿…é¡»ä½¿ç”¨è¡Œè¯ï¼Œæˆ‘ä¼šä¸ä½¿ç”¨è¡Œè¯æ¥åˆ†è§£å®ƒã€‚æˆ‘ä»¬å°†ä»â€œä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½â€å¼€å§‹ï¼Œç„¶åé€æ­¥æé«˜ã€‚æˆ‘ä¼šå°½å¯èƒ½åœ°ä½¿ç”¨ä¸€äº›åå¤å‡ºç°çš„éšå–»ã€‚æˆ‘å°†è®¨è®ºè¿™äº›æŠ€æœ¯çš„å½±å“ï¼Œå³æˆ‘ä»¬åº”è¯¥æœŸæœ›å®ƒä»¬åšä»€ä¹ˆæˆ–ä¸åº”è¯¥æœŸæœ›å®ƒä»¬åšä»€ä¹ˆã€‚let&amp;rsquo;s go~!&lt;/p&gt;
&lt;p&gt;æ³¨ï¼šä¸»è¦æ˜¯ç»“åˆè®ºæ–‡ã€Œ&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;ã€ç†è§£Transformerã€‚&lt;a href=&#34;https://www.youtube.com/watch?v=nzqlFIcCSWQ&#34;&gt;Transformerè®ºæ–‡é€æ®µç²¾è¯»&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;é™„ï¼š&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/transformer.ipynb&#34;&gt;Transformerå­¦ä¹ ç¬”è®°&lt;/a&gt; | &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/AnnotatedTransformer.ipynb&#34;&gt;Annotated Transformer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zjkBMFhNj_g&#34;&gt;&lt;strong&gt;Intro to Large Language Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>è¯‘ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¼€å‘æ‰€éœ€äº†è§£çš„çŸ¥è¯†</title>
      <link>https://weedge.github.io/post/llm/all-you-need-to-know-to-develop-using-large-language-models/</link>
      <pubDate>Sun, 03 Dec 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/all-you-need-to-know-to-develop-using-large-language-models/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/0.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;å¯¼è¯»&#34;&gt;å¯¼è¯»&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡çš„ç›®çš„æ˜¯ç®€å•åœ°è§£é‡Šå¼€å§‹å¼€å‘åŸºäº LLM çš„åº”ç”¨ç¨‹åºæ‰€éœ€çš„å…³é”®æŠ€æœ¯ã€‚å®ƒé¢å‘å¯¹æœºå™¨å­¦ä¹ æ¦‚å¿µæœ‰åŸºæœ¬äº†è§£å¹¶å¸Œæœ›æ·±å…¥ç ”ç©¶çš„è½¯ä»¶å¼€å‘äººå‘˜ã€æ•°æ®ç§‘å­¦å®¶å’Œ&lt;strong&gt;äººå·¥æ™ºèƒ½çˆ±å¥½è€…&lt;/strong&gt;ã€‚æœ¬æ–‡è¿˜æä¾›äº†è®¸å¤šæœ‰ç”¨çš„é“¾æ¥ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚è¿™ä¼šå¾ˆæœ‰è¶£ï¼&lt;/p&gt;
&lt;p&gt;æ³¨ï¼šæœ¬æ–‡å¯ä»¥ä½œä¸ºä¸€ä¸ªç´¢å¼•ç›®å½•(è¿›ä¸€æ­¥é˜…è¯»èµ„æ–™æ·±å…¥å­¦ä¹ )ï¼Œä»æ•´ä½“ä¸Šäº†è§£ä¸‹ï¼Œæ¯•ç«Ÿç°åœ¨LLMå‘å±•å¾ˆå¿«ï¼Œå¯ä»¥å‘æ•£æˆ–è€…focusæŸä¸ªé¢†åŸŸï¼›å¤§éƒ¨åˆ†LLMç›¸å…³å¼€æºå®ç°ï¼Œå¯ä»¥æ‰‹åŠ¨demoä¸‹è¿‡ç¨‹ï¼Œè‡³äºç‚¼ä¸¹äº†è§£è¿‡ç¨‹å³å¯ï¼Œä¸»è¦åœ¨åœºæ™¯ä¸Šç»“åˆå·¥ç¨‹å»åˆ©ç”¨å¥½å¤§åŠ›ç¥ä¸¸åœ¨ç”Ÿäº§ç¯å¢ƒè½åœ°ï¼›è¿˜æœ‰å°±æ˜¯åº”ç”¨åœºæ™¯ï¼Œå›½å†…appåº”è¯¥å¯ä»¥å¤åˆ»ï¼Œå¦‚æœæ¨¡å‹å’Œæ•°æ®æœ‰äº†ï¼Œç¼ºä¸ªè½åœ°ideaçš„è¯~&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
