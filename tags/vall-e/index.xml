<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>VALL-E on </title>
    <link>https://weedge.github.io/tags/vall-e/</link>
    <description>Recent content in VALL-E on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 13 Jan 2025 10:26:23 +0800</lastBuildDate><atom:link href="https://weedge.github.io/tags/vall-e/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>论文解读： VALL-E 系列</title>
      <link>https://weedge.github.io/post/multimoding/voices/vall-e-x/</link>
      <pubDate>Mon, 13 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/vall-e-x/</guid>
      <description>&lt;p&gt;前文讲到VITS，采用的端到端的NAR模型，这篇文章记录下微软提出的VALL-E系列，从 AR+NAR 到 AR 模型的转变，以及后面MELLE引入的VAE和Mel-Spectorgram，将neural codec text speech LM (AR+NAR Transformer Decoder) 转变为  autoregressive  mel-spectrogram text speech LM  (AR Transformer Decoder) ；由于LM生成的是mel-spectrogram 需要通过vocoder 转换成 waveform； 生成的内容采样模块：从top-p random sampling 变成 Latent Sampling潜在采样模块（思想源自VAE, 从预测的高斯分布中采样潜在嵌入，然后将其投影回频谱图空间）&lt;/p&gt;
&lt;h2 id=&#34;vall-e-系列&#34;&gt;VALL-E 系列&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/project/vall-e-x/&#34;&gt;https://www.microsoft.com/en-us/research/project/vall-e-x/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vall-E: &lt;a href=&#34;https://ar5iv.labs.arxiv.org/html/2301.02111&#34;&gt;https://ar5iv.labs.arxiv.org/html/2301.02111&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;具体来说，我们使用从现成的神经音频编解码器模型派生的离散代码来训练&lt;em&gt;&lt;strong&gt;神经编解码器语言模型（neural codec language model）&lt;/strong&gt;&lt;/em&gt;（称为VALL-E ），并将 TTS 视为条件语言建模任务，而不是像之前的工作那样将 TTS 视为连续信号回归。 在预训练阶段，我们将 TTS 训练数据扩展到 60K 小时的英语语音，比现有系统大数百倍。 VALL-E具有情境学习功能，可用于合成高质量的个性化语音，只需对看不见的说话者进行 3 秒的注册录音作为声音提示。实验结果表明， VALL-E在语音自然度和说话人相似度方面显着优于最先进的零样本 TTS 系统。此外，我们发现VALL-E可以在合成时保留说话者的情感和声音提示的声学环境。&lt;/p&gt;
&lt;p&gt;与之前的管道不同（例如，音素 → 梅尔谱图 → 波形）， VALL-E的管线是音素 → 离散码 → 波形。&lt;/p&gt;
&lt;p&gt;VALL-E根据音素和声学代码提示生成与目标内容和说话者的声音相对应的离散音频编解码器代码。 VALL-E直接支持各种语音合成应用，例如零样本 TTS、语音编辑以及与 GPT-3 等其他生成式 AI 模型相结合的内容创建。&lt;/p&gt;
&lt;p&gt;VALL-E系列：2023年的1月份开始 - 2024年的7月份&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VALL-E 使用从现成的神经音频编解码器模型派生的离散代码来训练神经编解码器语言模型，并将 TTS 视为条件语言建模任务，而不是像之前的工作那样将 TTS 视为连续信号回归。 VALL-E 具有情境学习功能，可用于合成高质量的个性化语音，只需录制未见过的讲话者的 3 秒注册录音作为提示。在语音自然度和说话人相似度方面，VALL-E 显着优于最先进的零样本 TTS 系统。此外，VALL-E可以在合成时保留说话者的情绪和声音提示的声学环境。&lt;/li&gt;
&lt;li&gt;VALL-E X 扩展其能力，适应多语言场景，促进跨语言零样本 TTS。&lt;/li&gt;
&lt;li&gt;VALL-E R 引入了音素单调对齐策略，增强了语音生成的鲁棒性。&lt;/li&gt;
&lt;li&gt;VALL-E 2 通过集成重复感知采样和分组代码建模技术， 实现了一个突破性的里程碑：在 LibriSpeech 和 VCTK 数据集上的零样本 TTS 性能与人类相当。这标志着此类成就的首次实例，为该领域树立了新标准。&lt;/li&gt;
&lt;li&gt;MELLE 是一种新颖的基于连续值标记的语言建模方法，用于文本到语音合成 (TTS)。 MELLE 直接从文本条件自回归生成连续的梅尔频谱图帧，绕过了矢量量化的需要，矢量量化最初是为音频压缩而设计的，与梅尔频谱图相比，牺牲了保真度。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
