<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>streaming on æ—¶é—´é£˜è¿‡</title>
    <link>https://weedge.github.io/tags/streaming/</link>
    <description>Recent content in streaming on æ—¶é—´é£˜è¿‡</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 19 Jan 2025 10:26:23 +0800</lastBuildDate><atom:link href="https://weedge.github.io/tags/streaming/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>è®ºæ–‡è§£è¯»ï¼šFish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis</title>
      <link>https://weedge.github.io/post/multimoding/voices/fishspeech/</link>
      <pubDate>Sun, 19 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/fishspeech/</guid>
      <description>&lt;h2 id=&#34;ç›¸å…³è®ºæ–‡&#34;&gt;ç›¸å…³è®ºæ–‡&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;base&lt;/strong&gt;: åŸºç¡€æ™®é€‚ç ”ç©¶&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hourglass transformers: &lt;a href=&#34;https://arxiv.org/abs/2110.13711&#34;&gt;2021. Hierarchical Transformers Are More Efficient Language Models&lt;/a&gt; | &lt;a href=&#34;https://github.com/lucidrains/simple-hierarchical-transformer&#34;&gt;lucidrains/simple-hierarchical-transformer&lt;/a&gt; vanilla layers and shortened layers use GPT  AR  GLM ğŸ¤&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.07185&#34;&gt;2023.5 &lt;strong&gt;MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers&lt;/strong&gt;&lt;/a&gt; (åˆ†åˆ«åœ¨æ–‡æœ¬ï¼Œå›¾ç‰‡ï¼Œè¯­éŸ³å»ºæ¨¡)| &lt;a href=&#34;https://github.com/lucidrains/MEGABYTE-pytorch&#34;&gt;lucidrains/MEGABYTE-pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PS: æƒ³æ³•å’Œè‡ªå·±æ•´ç†çš„&lt;a href=&#34;https://github.com/ai-bot-pro/baby-llm/tree/main/simpleLM&#34;&gt;simple LM&lt;/a&gt;ç›¸ä¼¼~&lt;/p&gt;
&lt;p&gt;æ‰©å±•é˜…è¯»ï¼š&lt;a href=&#34;https://arxiv.org/abs/2412.09871&#34;&gt;2024.12 &lt;strong&gt;Byte Latent Transformer: Patches Scale Better Than Tokens&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/facebookresearch/blt&#34;&gt;paper code&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;audio speech&lt;/strong&gt;: åœºæ™¯ç ”ç©¶&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;â­ï¸&lt;a href=&#34;https://arxiv.org/abs/2310.00704&#34;&gt;2023.10 UniAudio: &lt;strong&gt;An Audio Foundation Model Toward Universal Audio Generation&lt;/strong&gt;&lt;/a&gt; (çµæ„Ÿæ¥è‡ª MEGABYTEï¼Œå°†å…¶åº”ç”¨äºè¯­éŸ³æ¨¡å‹)| &lt;a href=&#34;https://github.com/yangdongchao/UniAudio&#34;&gt;paper code&lt;/a&gt;  (ä»£ç å¯æ‰©å±•ä»»åŠ¡è¿›è¡Œè®­ç»ƒ, å·²æ‰©å±•äº†éŸ³ä¹æ•°æ®)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/d0bc3097-3daa-47db-8bbb-f24f5aec800d&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;â­ï¸&lt;a href=&#34;https://arxiv.org/abs/2411.01156&#34;&gt;2024. &lt;strong&gt;Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis&lt;/strong&gt;&lt;/a&gt; (è®ºæ–‡ä¸­ä»‹ç»çš„Daul-AR, GFSQ, Firefly-GAN(FF-GAN) å¯¹EVA-GANæ”¹ç‰ˆï¼Œç»†èŠ‚éœ€è¦ç»“åˆä»£ç ç†è§£) | &lt;a href=&#34;https://github.com/fishaudio/fish-speech&#34;&gt;paper code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>è®ºæ–‡è§£è¯»ï¼šCosyVoice2: Scalable Streaming Speech Synthesis with Large Language Models</title>
      <link>https://weedge.github.io/post/multimoding/voices/cosyvoice2/</link>
      <pubDate>Fri, 17 Jan 2025 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/multimoding/voices/cosyvoice2/</guid>
      <description>&lt;h2 id=&#34;cosyvoice2-è®ºæ–‡&#34;&gt;CosyVoice2 è®ºæ–‡&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.10117&#34;&gt;2024.12  &lt;strong&gt;CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models&lt;/strong&gt;&lt;/a&gt;ï¼ˆæµå¼åˆæˆï¼‰&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/FunAudioLLM/CosyVoice&#34;&gt;paper code&lt;/a&gt;: å…¬å¼€æ¨ç†å’Œæƒé‡ï¼Œè®­ç»ƒè¿‡ç¨‹éœ€è¦åœ¨CosyVoiceçš„åŸºç¡€ä¸Šä¿®æ”¹ä¸‹ã€‚&lt;/li&gt;
&lt;li&gt;achatbot TTS é›†æˆ CosyVoice2ï¼š &lt;a href=&#34;https://github.com/ai-bot-pro/achatbot/pull/107&#34;&gt;https://github.com/ai-bot-pro/achatbot/pull/107&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colab achatbot_CosyVoice æ“ä½œç¬”è®°ï¼š &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/achatbot_CosyVoice.ipynb&#34;&gt;https://github.com/weedge/doraemon-nb/blob/main/achatbot_CosyVoice.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colab CosyVoice æ“ä½œç¬”è®°ï¼š &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/CosyVoice.ipynb&#34;&gt;https://github.com/weedge/doraemon-nb/blob/main/CosyVoice.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;æ‰©å±•é˜…è¯»&#34;&gt;æ‰©å±•é˜…è¯»&lt;/h2&gt;
&lt;h3 id=&#34;zero-shot-tts-models-é›¶æ ·æœ¬-tts-æ¨¡å‹&#34;&gt;zero-shot TTS models é›¶æ ·æœ¬ TTS æ¨¡å‹&lt;/h3&gt;
&lt;h4 id=&#34;codec-language-models-ç¼–è§£ç å™¨è¯­è¨€æ¨¡å‹&#34;&gt;codec language models ç¼–è§£ç å™¨è¯­è¨€æ¨¡å‹&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;speech &lt;strong&gt;codec model&lt;/strong&gt;  to extract discrete speech representation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.03312&#34;&gt;2021.7 SoundStream: An End-to-End Neural Audio Codec&lt;/a&gt; | &lt;a href=&#34;https://github.com/wesbz/SoundStream&#34;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.13438&#34;&gt;2022.10 &lt;strong&gt;High Fidelity Neural Audio Compression&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;http://github.com/facebookresearch/encodec&#34;&gt;facebookresearch/encodec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.02765&#34;&gt;2023.5 HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec&lt;/a&gt; | &lt;a href=&#34;https://github.com/yangdongchao/AcademiCodec&#34;&gt;paper code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.07405&#34;&gt;2023.10 FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec&lt;/a&gt; | &lt;a href=&#34;https://github.com/modelscope/FunCodec&#34;&gt;modelscope/FunCodec&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;speech &lt;strong&gt;codec model&lt;/strong&gt; + &lt;strong&gt;autoregressive model&lt;/strong&gt; to predict the speech tokens (acoustic tokens):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.02111&#34;&gt;2023.1 &lt;strong&gt;Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers&lt;/strong&gt;&lt;/a&gt; (Vall-E) | ä»¥åŠåç»­ Vall-E å‡çº§ç³»åˆ— (ä¸åŒ…æ‹¬MELLE): &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/vall-e-x/&#34;&gt;https://www.microsoft.com/en-us/research/project/vall-e-x/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.03540&#34;&gt;2023.2 Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;speech &lt;strong&gt;codec model&lt;/strong&gt; (speech semantics Codec) +  &lt;strong&gt;non-autoregressive masked model&lt;/strong&gt; to predict the speech tokens (acoustic tokens):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.00750&#34;&gt;2024.9 &lt;strong&gt;MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/Amphion/tree/main/models/tts/maskgct&#34;&gt;paper code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;codec model (speech acoustic Codec) or  &lt;strong&gt;vocoder&lt;/strong&gt; to synthesize waveforms from mel-spectrograms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.00814&#34;&gt;2023.6 &lt;strong&gt;Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/gemelo-ai/vocos&#34;&gt;paper code&lt;/a&gt; | æ¨ç†é€Ÿåº¦å¿«ï¼šè¿è¡Œé€Ÿåº¦æ¯” HiFi-GAN å¿«çº¦ 13 å€ï¼Œæ¯” BigVGAN å¿«è¿‘ 70 å€ã€‚åœ¨æ²¡æœ‰ GPU åŠ é€Ÿçš„æƒ…å†µä¸‹è¿è¡Œæ—¶ï¼Œè¿™ç§é€Ÿåº¦ä¼˜åŠ¿å°¤å…¶æ˜æ˜¾ã€‚è¿™ä¸»è¦æ˜¯ç”±äºä½¿ç”¨äº†çŸ­æ—¶å‚…é‡Œå¶é€†å˜æ¢ï¼ˆISTFTï¼‰ç®—æ³•è€Œä¸æ˜¯è½¬ç½®å·ç§¯ã€‚è¿˜è¯„ä¼°äº† Vocos çš„ä¸€ä¸ªå˜ä½“ï¼Œå®ƒåˆ©ç”¨ ResBlock çš„æ‰©å¼ å·ç§¯è€Œä¸æ˜¯ ConvNeXt å—ã€‚åœ¨ GPU ä¸Šæ‰§è¡Œæ—¶ï¼Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯å¯æä¾›é¢å¤–çš„åŠ é€Ÿã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10389765&#34;&gt;2023.12 &lt;strong&gt;WaveNeXt: ConvNeXt-Based Fast Neural Vocoder Without ISTFT layer&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://ast-astrec.nict.go.jp/demo_samples/asru_2023_okamoto/index.html&#34;&gt;demo samples&lt;/a&gt; | paper codeåŸºäº &lt;a href=&#34;https://arxiv.org/abs/2110.07840&#34;&gt;ESPNet2-TTS&lt;/a&gt;  | ä¸€ç§æ–°å‹çš„åŸºäºConvNeXtçš„å¿«é€Ÿç¥ç»å£°ç å™¨WaveNeXtï¼Œå®ƒé€šè¿‡æ›¿æ¢Vocosä¸­çš„é€†çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆiSTFTï¼‰å±‚ä¸ºå¯è®­ç»ƒçš„çº¿æ€§å±‚ï¼Œç›´æ¥é¢„æµ‹è¯­éŸ³æ³¢å½¢æ ·æœ¬ï¼Œè€Œä¸ä¾èµ–äºSTFTé¢‘è°±ã€‚è¿™ä¸€æ”¹è¿›ä¸ä»…ä¿æŒäº†Vocosçš„å¿«é€Ÿæ¨ç†é€Ÿåº¦ï¼Œè¿˜æé«˜äº†è¯­éŸ³åˆæˆçš„è´¨é‡ã€‚æ–‡ç« è¿˜æ¢è®¨äº†å¦‚ä½•å°†WaveNeXtä¸åŸºäºJETSçš„ç«¯åˆ°ç«¯æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆE2E TTSï¼‰æ¡†æ¶é›†æˆï¼Œå¹¶ç ”ç©¶äº†é‡‡æ ·é¢‘ç‡ä¸º48kHzçš„å…¨å¸¦æ¨¡å‹ï¼ˆFull-band Modelï¼šèƒ½å¤Ÿå¤„ç†å’Œç”Ÿæˆè¦†ç›–æ•´ä¸ªéŸ³é¢‘é¢‘è°±èŒƒå›´çš„æ¨¡å‹ï¼Œé€šå¸¸æ˜¯æŒ‡èƒ½å¤Ÿå¤„ç†ä»æœ€ä½é¢‘åˆ°æœ€é«˜é¢‘çš„å®Œæ•´éŸ³é¢‘ä¿¡å·çš„æ¨¡å‹ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWaveNeXtåœ¨åˆ†æ-åˆæˆå’ŒE2E TTSæ¡ä»¶ä¸‹å‡ä¼˜äºVocosï¼ŒåŒæ—¶ä¿æŒäº†å¿«é€Ÿæ¨ç†çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/4a1eeaff-528f-4706-b5fc-210caea2c13b&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;feature-diffusion-models-ç‰¹å¾æ‰©æ•£æ¨¡å‹&#34;&gt;feature diffusion models ç‰¹å¾æ‰©æ•£æ¨¡å‹&lt;/h4&gt;
&lt;p&gt;DDPM + CFM + NAR(non-autoregressive) model, æ²¡æœ‰ codec&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Base module:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Denoising Diffusion Probabilistic Model(DDPM)ï¼š &lt;a href=&#34;https://arxiv.org/abs/2006.11239&#34;&gt;2020.6 &lt;strong&gt;Denoising Diffusion Probabilistic Models&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/hojonathanho/diffusion&#34;&gt;paper code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conditional Flow Matching (CFM)ï¼š  &lt;a href=&#34;https://arxiv.org/abs/2210.02747&#34;&gt;2022.10 &lt;strong&gt;Flow Matching for Generative Modeling&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/atong01/conditional-flow-matching&#34;&gt;CFM lib&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;the alignment modeling between input text and synthesized speech&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;phoneme-level duration model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.03100&#34;&gt;2024.5 NaturalSpeech 3&lt;/a&gt;  and &lt;a href=&#34;https://arxiv.org/abs/2306.15687&#34;&gt;2023.6 Voicebox&lt;/a&gt; use frame-wise phoneme alignment;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.03199&#34;&gt;2023.9 Matcha-TTS&lt;/a&gt; adopts monotonic alignment search(MAS) and relies on phoneme-level duration model;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.18009&#34;&gt;2024.6 E2 TTS&lt;/a&gt; å’Œ&lt;a href=&#34;https://arxiv.org/abs/2406.02430&#34;&gt;2024.6 Seed-TTS&lt;/a&gt; ç ”ç©¶è¡¨æ˜åœ¨æ–‡æœ¬å’Œè¯­éŸ³ä¹‹é—´å¼•å…¥è¿™ç§åƒµåŒ–å’Œä¸çµæ´»çš„å¯¹é½æ–¹å¼ä¼šé˜»ç¢æ¨¡å‹ç”Ÿæˆæ›´è‡ªç„¶çš„ç»“æœã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;E3 TTS æ”¾å¼ƒéŸ³ç´ çº§æŒç»­æ—¶é—´å¹¶å¯¹è¾“å…¥åºåˆ—åº”ç”¨äº¤å‰æ³¨æ„åŠ›ï¼Œä½†äº§ç”Ÿçš„éŸ³é¢‘è´¨é‡æœ‰é™ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DiTTo-TTS ä½¿ç”¨æ‰©æ•£å˜æ¢å™¨ (DiT) ï¼Œå¹¶ä»¥æ¥è‡ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç¼–ç æ–‡æœ¬ä¸ºæ¡ä»¶è¿›è¡Œäº¤å‰æ³¨æ„ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå¯¹é½ï¼Œå®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥å¾®è°ƒç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ï¼Œå°†è¯­ä¹‰ä¿¡æ¯æ³¨å…¥ç”Ÿæˆçš„è¡¨ç¤ºä¸­ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäº Voiceboxçš„ E2 TTSé‡‡ç”¨äº†æ›´ç®€å•çš„æ–¹æ³•ï¼Œåˆ é™¤äº†éŸ³ç´ å’ŒæŒç»­æ—¶é—´é¢„æµ‹å™¨ï¼Œç›´æ¥ä½¿ç”¨å¡«å……tokenå¡«å……åˆ°æ¢…å°”é¢‘è°±å›¾é•¿åº¦çš„å­—ç¬¦ä½œä¸ºè¾“å…¥ã€‚è¿™ä¸ªç®€å•çš„æ–¹æ¡ˆä¹Ÿå®ç°äº†éå¸¸è‡ªç„¶å’ŒçœŸå®çš„åˆæˆç»“æœã€‚ç„¶è€Œï¼ŒF5-TTS å‘ç° E2 TTS ä¸­æ–‡æœ¬å’Œè¯­éŸ³å¯¹é½å­˜åœ¨é²æ£’æ€§é—®é¢˜ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.02430&#34;&gt;2024.6 Seed-TTS&lt;/a&gt; é‡‡ç”¨äº†ç±»ä¼¼çš„ç­–ç•¥å¹¶å–å¾—äº†ä¼˜å¼‚çš„ç»“æœï¼Œå°½ç®¡æ²¡æœ‰è¯¦ç»†è¯´æ˜æ¨¡å‹ç»†èŠ‚ã€‚åœ¨è¿™äº›æœªæ˜ç¡®å»ºæ¨¡éŸ³ç´ çº§æŒç»­æ—¶é—´çš„æ–¹æ³•ä¸­ï¼Œæ¨¡å‹å­¦ä¹ æ ¹æ®ç»™å®šçš„æ€»åºåˆ—é•¿åº¦åˆ†é…æ¯ä¸ªå•è¯æˆ–éŸ³ç´ çš„é•¿åº¦ï¼Œä»è€Œæ”¹è¿›éŸµå¾‹å’ŒèŠ‚å¥ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.06885&#34;&gt;2024.10 &lt;strong&gt;F5-TTS: A fairytaler that fakes fluent and faithful speech with flow matching&lt;/strong&gt;&lt;/a&gt; ä¿æŒäº†ç®¡é“çš„ç®€å•æ€§ï¼Œæ— éœ€éŸ³ç´ å¯¹é½ã€æŒç»­æ—¶é—´é¢„æµ‹å™¨ã€æ–‡æœ¬ç¼–ç å™¨å’Œè¯­ä¹‰æ³¨å…¥ç¼–è§£ç å™¨æ¨¡å‹ï¼Œåˆ©ç”¨å¸¦æœ‰ &lt;a href=&#34;https://arxiv.org/abs/2301.00808&#34;&gt;ConvNeXt V2&lt;/a&gt;|&lt;a href=&#34;https://github.com/facebookresearch/ConvNeXt-V2&#34;&gt;paper code&lt;/a&gt; çš„Diffusion Transformer(DiT)æ¥æ›´å¥½åœ°è§£å†³ä¸Šä¸‹æ–‡å­¦ä¹ æœŸé—´çš„æ–‡æœ¬è¯­éŸ³å¯¹é½é—®é¢˜ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;codec-language-and-feature-diffusion-hybrid-systems-æ··åˆç³»ç»Ÿ&#34;&gt;codec language and feature diffusion hybrid systems æ··åˆç³»ç»Ÿ&lt;/h4&gt;
&lt;p&gt;text-to-codec language model  å’Œ codec-to-feature diffusion model&lt;/p&gt;
&lt;p&gt;è¯­è¨€æ¨¡å‹è§£å†³æ–‡æœ¬å’Œè¯­éŸ³ä¹‹é—´çš„å¯¹é½ä»¥åŠè¯è¯­æŒç»­æ—¶é—´é¢„æµ‹ï¼Œè€Œç¼–è§£ç å™¨åˆ°ç‰¹å¾æ‰©æ•£æ¨¡å‹åˆ™æ ¹æ®ç”Ÿæˆçš„ç¼–è§£ç å™¨å’Œå…¶ä»–æ¡ä»¶åˆæˆè¯­éŸ³ç‰¹å¾ï¼ˆæ¢…å°”è°±ï¼‰ã€‚é€šè¿‡åˆ©ç”¨ä¸¤ç§ç”Ÿæˆæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæ··åˆç³»ç»Ÿå®ç°äº†é«˜åº¦å¤šæ ·æ€§ã€éŸµå¾‹ä¸€è‡´æ€§å’Œè¯­éŸ³è´¨é‡ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.02430&#34;&gt;2024.6 Seed-TTS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.05407&#34;&gt;2024.7 Cosyvoice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.03283&#34;&gt;2024.9 Fireredtts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;language-model-based-zero-shot-tts-models--streaming-synthesis&#34;&gt;language model-based zero-shot TTS models  &lt;strong&gt;streaming&lt;/strong&gt; synthesis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.08093&#34;&gt;2024.2 &lt;strong&gt;BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data&lt;/strong&gt;&lt;/a&gt; | å°çº¢ä¹¦çš„FireRedTTS æ¥æºäºæ­¤ &lt;a href=&#34;https://github.com/FireRedTeam/FireRedTTS&#34;&gt;FireRedTTS paper code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.02897&#34;&gt;2024.6 LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.11210&#34;&gt;2024.9 Speak While You Think: Streaming Speech Synthesis During Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.00767&#34;&gt;2024.10 Zero-Shot Text-to-Speech from Continuous Text Streams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
