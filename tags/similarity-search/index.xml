<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>similarity search on </title>
    <link>https://weedge.github.io/tags/similarity-search/</link>
    <description>Recent content in similarity search on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 03 Nov 2023 15:00:23 +0800</lastBuildDate><atom:link href="https://weedge.github.io/tags/similarity-search/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>译：加速向量搜索：RAPIDS RAFT IVF-Flat 近似算法</title>
      <link>https://weedge.github.io/post/gpu/3.accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/</link>
      <pubDate>Fri, 03 Nov 2023 15:00:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/3.accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;执行详尽的精确 k 最近邻 (kNN) 搜索，也称为&lt;em&gt;暴力搜索(brute-force search)&lt;/em&gt;，成本高昂，并且它不能很好地扩展到更大的数据集。在向量搜索期间，暴力搜索需要计算每个查询向量和数据库向量之间的距离。对于常用的欧几里德和余弦距离，计算任务等同于大型矩阵乘法。&lt;/p&gt;
&lt;p&gt;虽然 GPU 在执行矩阵乘法方面效率很高，但随着数据量的增加，计算成本变得令人望而却步。然而，许多应用程序不需要精确的结果，而是可以为了更快的搜索而牺牲一些准确性。当不需要精确的结果时，近似最近邻 (ANN) 方法通常可以减少搜索期间必须执行的距离计算的数量。&lt;/p&gt;
&lt;p&gt;本文主要介绍了 IVF-Flat，这是 NVIDIA &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/&#34;&gt;RAPIDS RAFT&lt;/a&gt; 中的一种方法。IVF-Flat 方法使用原始（即Flat）向量的倒排索引 (IVF)。此算法提供了简单的调整手段，以减少整体搜索空间并在准确性和速度之间进行权衡。&lt;/p&gt;
&lt;p&gt;为了帮助了解如何使用 IVF-Flat，我们讨论了该算法的工作原理，并演示了&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#ivf-flat&#34;&gt;Python&lt;/a&gt;和&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_ivf_flat/&#34;&gt;C++ APIs&lt;/a&gt;我们介绍了索引构建的设置参数，并提供了如何配置 GPU 加速的 IVF-Flat搜索的技巧。这些步骤也可以在示例中遵循&lt;a href=&#34;https://github.com/rapidsai/raft/blob/a1002f8c8f4debc52fbab7191297a2f54ff42856/notebooks/ivf_flat_example.ipynb&#34;&gt;Python notebook&lt;/a&gt;和&lt;a href=&#34;https://github.com/rapidsai/raft/blob/a1002f8c8f4debc52fbab7191297a2f54ff42856/cpp/template/src/ivf_flat_example.cu&#34;&gt;C++ project&lt;/a&gt;.最后，我们演示了 GPU 加速的向量搜索比 CPU 搜索快一个数量级。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：加速向量搜索：微调 GPU 索引算法</title>
      <link>https://weedge.github.io/post/gpu/2.accelerating-vector-search-fine-tuning-gpu-index-algorithms/</link>
      <pubDate>Fri, 03 Nov 2023 14:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/2.accelerating-vector-search-fine-tuning-gpu-index-algorithms/</guid>
      <description>&lt;p&gt;这个 &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/accelerating-vector-search-using-gpu-accelerated-indexes-with-rapids-raft/&#34;&gt;系列的第一篇文章&lt;/a&gt; 介绍了向量搜索索引，解释了它们在实现广泛的重要应用中所起的作用，并使用了 &lt;a href=&#34;https://github.com/rapidsai/raft&#34;&gt;RAFT&lt;/a&gt; 库。&lt;/p&gt;
&lt;p&gt;在这篇文章中，我们深入探讨第 1 部分中提到的每种 GPU 加速索引方法，并简要解释了算法的工作原理，以及总结重要的微调参数。&lt;/p&gt;
&lt;p&gt;然后，我们通过一个简单的端到端示例，用预训练的大型语言模型演示 RAFT 在问答问题上的 Python API，并在涉及同时传递给搜索算法的不同查询向量数量的几个不同场景下，将 RAFT 的算法与 HNSW 的性能进行比较。&lt;/p&gt;
&lt;p&gt;内容如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可与 GPU 一起使用的向量搜索索引算法概述&lt;/li&gt;
&lt;li&gt;一个端到端的例子演示了使用 Python 在 GPU 上运行向量搜索是多么容易&lt;/li&gt;
&lt;li&gt;GPU 上的向量搜索与 CPU 上当前最先进的 HNSW 方法的性能比较&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>译：加速向量搜索：利用 GPU 索引的 RAPIDS RAFT</title>
      <link>https://weedge.github.io/post/gpu/1.accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/</link>
      <pubDate>Fri, 03 Nov 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/1.accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/</guid>
      <description>&lt;p&gt;在 2023 年的人工智能领域，向量搜索成为最热门的话题之一，因为它在&lt;a href=&#34;https://www.nvidia.cn/glossary/data-science/large-language-models/&#34;&gt;大语言模型&lt;/a&gt;（LLM）和&lt;a href=&#34;https://www.nvidia.cn/glossary/data-science/generative-ai/&#34;&gt;生成式人工智能&lt;/a&gt;中发挥了重要作用。语义向量搜索实现了一系列重要任务，如检测欺诈交易、向用户推荐产品、使用上下文信息增强全文搜索以及查找潜在安全风险的参与者。&lt;/p&gt;
&lt;p&gt;数据量持续飙升，传统的逐一比较的方法在计算上变得不可行。向量搜索方法使用近似查找，这种查找更具可扩展性，可以更有效地处理大量数据。正如我们在这篇文章中所展示的，在 GPU 上加速向量搜索不仅提供了更快的搜索时间，而且索引构建时间也可以更快。&lt;/p&gt;
&lt;p&gt;本文内容如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;向量搜索简介及流行应用综述&lt;/li&gt;
&lt;li&gt;在 GPU 上加速向量搜索的 RAFT 库综述&lt;/li&gt;
&lt;li&gt;GPU 加速向量搜索索引与 CPU 上最新技术的性能比较&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本系列的第二篇文章深入探讨了每一个 GPU 加速指数，并简要解释了算法的工作原理以及微调其行为的重要参数摘要。想要了解更多信息，请访问 &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/&#34;&gt;加速向量搜索：微调 GPU 索引算法&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：相似性搜索，第 7 部分：LSH 组合</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/7.lsh-compositions/</link>
      <pubDate>Tue, 26 Sep 2023 23:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/7.lsh-compositions/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;深入研究 LSH 函数的组合以保证更可靠的搜索&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;在本系列文章的最后两部分中，我们深入研究了 LSH —— 一种&lt;em&gt;将输入向量转换为低维散列值，同时保留有关其相似性的信息&lt;/em&gt;的算法。特别是，我们已经研究了两种适用于不同距离度量的算法：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://weedge.github.io/post/oneday/similarity-search/5.locality-sensitive-hashing/&#34;&gt;相似性搜索，第 5 部分：局部敏感哈希 (LSH)&lt;/a&gt;: 经典的LSH算法构造反映向量&lt;strong&gt;Jaccard系数&lt;/strong&gt;信息的签名。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://weedge.github.io/post/oneday/similarity-search/6.random-projections-with-lsh-forest/&#34;&gt;相似性搜索，第 6 部分：LSH 森林的随机投影&lt;/a&gt;: 随机投影方法构建了保持向量&lt;strong&gt;余弦相似性&lt;/strong&gt;的超平面森林。&lt;/p&gt;
&lt;p&gt;事实上，LSH 算法也适用于其他距离度量。尽管每种方法都有其独特的部分，但每种方法中都出现了许多共同的概念和公式。为了促进未来新方法的学习过程，我们将更多地关注理论并提供一些经常出现在高级 LSH 文献中的基本定义和定理。在本文结束时，我们将能够通过简单地将基本方案组合为乐高积木来构建更复杂的 LSH 方案。&lt;/p&gt;
&lt;p&gt;最后我们将了解如何将&lt;strong&gt;欧几里得距离&lt;/strong&gt;纳入 LSH 中。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意&lt;/em&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;作为主要先决条件，您应该已经熟悉本系列文章的第 5 部分和第 6 部分。如果没有，强烈建议先阅读它们。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cosine_similarity&#34;&gt;余弦距离&lt;/a&gt;定义在 [0, 2] 范围内。为简单起见，我们将其映射到区间 [0, 1]，其中 0 和 1 分别表示最低和最高可能的相似度。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>译：相似性搜索，第 6 部分：LSH 森林的随机投影</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/6.random-projections-with-lsh-forest/</link>
      <pubDate>Tue, 26 Sep 2023 21:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/6.random-projections-with-lsh-forest/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;了解如何对数据进行哈希处理并通过构造随机超平面来反映其相似性&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;在&lt;a href=&#34;https://medium.com/towards-data-science/similarity-search-part-5-locality-sensitive-hashing-lsh-76ae4b388203&#34;&gt;上一部分&lt;/a&gt;中，我们研究了 LSH 的主要范例，即将&lt;em&gt;输入向量转换为低维hash值，同时保留有关其相似性的信息&lt;/em&gt;。为了获取hash值（签名），使用了 minhash 函数。在本文中，我们将随机投影输入数据以获得类似的二进制向量。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：相似性搜索，第 5 部分：局部敏感哈希 (LSH)</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/5.locality-sensitive-hashing/</link>
      <pubDate>Tue, 26 Sep 2023 15:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/5.locality-sensitive-hashing/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;探索如何将相似性信息合并到哈希函数中&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;在本系列文章的前几部分中，我们讨论了倒排文件索引、产品量化和 HNSW 以及如何将它们结合使用来提高搜索质量。在本章中，我们将研究一种主要不同的方法，该方法可以保持高搜索速度和质量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;局部敏感哈希&lt;/strong&gt;（LSH）是一组方法，用于通过将数据向量转换为哈希值来缩小搜索范围，同时保留有关其相似性的信息。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们将讨论传统方法，该方法包括三个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Shingling&lt;/strong&gt;：将原始文本编码为向量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MinHashing&lt;/strong&gt; ：将向量转换为称为&lt;strong&gt;签名&lt;/strong&gt;的特殊表示，可用于比较它们之间的相似性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LSH函数&lt;/strong&gt;：将签名块散列到不同的桶中。如果一对向量的签名至少一次落入同一个桶中，则它们被视为候选向量。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们将在整篇文章中逐步深入探讨每个步骤的细节。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：相似性搜索，第 4 部分：分层可导航小世界 (HNSW)</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/4.hierarchical-navigable-small-world-hnsw/</link>
      <pubDate>Mon, 25 Sep 2023 15:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/4.hierarchical-navigable-small-world-hnsw/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;了解如何构建高效的多层图以提高海量数据的搜索速度&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1603.09320.pdf&#34;&gt;&lt;strong&gt;分层可导航小世界&lt;/strong&gt;&lt;/a&gt;(HNSW) 是一种用于近似搜索最近邻居的最先进算法。在底层，HNSW 构建了优化的图结构，使其与本系列文章前面部分中讨论的其他方法非常不同。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;HNSW 的主要思想是构建这样一个图，其中任何一对顶点之间的路径都可以通过少量步骤遍历。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;著名的&lt;a href=&#34;https://en.wikipedia.org/wiki/Six_degrees_of_separation&#34;&gt;六次握手规则&lt;/a&gt;的一个著名类比与此方法有关：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所有人之间的社会关系距离不超过 6 个。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在继续讨论 HNSW 的内部工作之前，让我们首先讨论跳跃列表和可导航小世界——HNSW 实现中使用的关键数据结构。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：相似性搜索，第 3 部分：混合倒排文件索引和乘积量化</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/3.blending-inverted-file-index-and-product-quantization/</link>
      <pubDate>Mon, 25 Sep 2023 11:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/3.blending-inverted-file-index-and-product-quantization/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;了解如何结合两个基本的相似性搜索索引以发挥两者的优势&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;在本系列的前两部分中，我们讨论了信息检索中的两种基本算法：&lt;strong&gt;倒排文件索引&lt;/strong&gt;和&lt;strong&gt;乘积量化&lt;/strong&gt;。它们都优化了搜索性能，但侧重于不同的方面：第一个加速了搜索速度，而后者将向量压缩为更小的、节省内存的表示形式。&lt;/p&gt;
&lt;p&gt;由于两种算法侧重于不同的方面，自然出现的问题是是否可以将这两种算法合并为一种新算法&lt;/p&gt;
&lt;p&gt;在本文中，我们将结合这两种方法的优点来产生快速且内存高效的算法。作为参考，大多数讨论的想法都将基于&lt;a href=&#34;https://inria.hal.science/inria-00514462v2/document&#34;&gt;本文&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;在深入研究细节之前，有必要了解残差向量(residual vectors)是什么，并对它们的有用属性有一个简单的直觉。稍后我们将在设计算法时使用它们。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：相似性搜索，第 2 部分：乘积量化</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/2.product-quantization/</link>
      <pubDate>Mon, 25 Sep 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/2.product-quantization/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/0.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习有效压缩大数据的强大技术&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;在本系列文章的&lt;a href=&#34;https://weedge.github.io/post/oneday/similarity-search/1.knn-inverted-file-index/&#34;&gt;第一部分&lt;/a&gt;中，我们研究了用于执行相似性搜索的 kNN 和倒排文件索引结构。正如我们所知，kNN 是最直接的方法，而倒排文件索引则在其之上发挥作用，建议在速度加速和准确性之间进行权衡。然而，这两种方法都不使用数据压缩技术，这可能会导致内存问题，特别是在数据集较大且 RAM 有限的情况下。在本文中，我们将尝试通过研究另一种称为“&lt;strong&gt;乘积量化(Product Quantization)&lt;/strong&gt;”的方法来解决此问题。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>译：相似性搜索，第 1 部分：kNN 和倒排文件索引</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/1.knn-inverted-file-index/</link>
      <pubDate>Sun, 24 Sep 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/1.knn-inverted-file-index/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;相似性搜索&lt;/strong&gt;(similarity-search)是给定一个查询，目标是在所有数据库文档中找到与其最相似的文档。本章介绍 kNN 的相似性搜索及其使用倒排文件的加速。&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。通常，文档或项目以文本或图像的形式表示。然而，机器学习算法不能直接处理原始文本或图像，这就是为什么文档和项目通常被预处理并存储为数字向量的原因。&lt;/p&gt;
&lt;p&gt;有时向量的每个分量都可以存储语义。在这种情况下，这些表示也称为&lt;strong&gt;嵌入&lt;/strong&gt;。这样的嵌入可以有数百个维度，数量可以达到数百万个！由于数量如此庞大，任何信息检索系统都必须能够快速检测相关文档。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在机器学习中，向量也称为&lt;strong&gt;对象&lt;/strong&gt;或&lt;strong&gt;点&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
  </channel>
</rss>
