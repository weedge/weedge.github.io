<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>时间飘过</title>
    <link>https://weedge.github.io/</link>
    <description>Recent content on 时间飘过</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 04 Dec 2023 10:26:23 +0800</lastBuildDate>
    
        <atom:link href="https://weedge.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>https://weedge.github.io/about/</link>
      <pubDate>Sun, 20 Jan 2013 21:38:52 +0800</pubDate>
      
      <guid>https://weedge.github.io/about/</guid>
      
        <description>
&lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css&#34;&gt;
&lt;style type=&#34;text/css&#34;&gt;.dark-theme .aplayer{background:#212121}.dark-theme .aplayer.aplayer-withlist .aplayer-info{border-bottom-color:#5c5c5c}.dark-theme .aplayer.aplayer-fixed .aplayer-list{border-color:#5c5c5c}.dark-theme .aplayer .aplayer-body{background-color:#212121}.dark-theme .aplayer .aplayer-info{border-top-color:#212121}.dark-theme .aplayer .aplayer-info .aplayer-music .aplayer-title{color:#fff}.dark-theme .aplayer .aplayer-info .aplayer-music .aplayer-author{color:#fff}.dark-theme .aplayer .aplayer-info .aplayer-controller .aplayer-time{color:#eee}.dark-theme .aplayer .aplayer-info .aplayer-controller .aplayer-time .aplayer-icon path{fill:#eee}.dark-theme .aplayer .aplayer-list{background-color:#212121}.dark-theme .aplayer .aplayer-list::-webkit-scrollbar-thumb{background-color:#999}.dark-theme .aplayer .aplayer-list::-webkit-scrollbar-thumb:hover{background-color:#bbb}.dark-theme .aplayer .aplayer-list li{color:#fff;border-top-color:#666}.dark-theme .aplayer .aplayer-list li:hover{background:#4e4e4e}.dark-theme .aplayer .aplayer-list li.aplayer-list-light{background:#6c6c6c}.dark-theme .aplayer .aplayer-list li .aplayer-list-index{color:#ddd}.dark-theme .aplayer .aplayer-list li .aplayer-list-author{color:#ddd}.dark-theme .aplayer .aplayer-lrc{text-shadow:-1px -1px 0 #666}.dark-theme .aplayer .aplayer-lrc:before{background:-moz-linear-gradient(top, #212121 0%, rgba(33,33,33,0) 100%);background:-webkit-linear-gradient(top, #212121 0%, rgba(33,33,33,0) 100%);background:linear-gradient(to bottom, #212121 0%, rgba(33,33,33,0) 100%);filter:progid:DXImageTransform.Microsoft.gradient( startColorstr=&#39;#212121&#39;, endColorstr=&#39;#00212121&#39;,GradientType=0 )}.dark-theme .aplayer .aplayer-lrc:after{background:-moz-linear-gradient(top, rgba(33,33,33,0) 0%, rgba(33,33,33,0.8) 100%);background:-webkit-linear-gradient(top, rgba(33,33,33,0) 0%, rgba(33,33,33,0.8) 100%);background:linear-gradient(to bottom, rgba(33,33,33,0) 0%, rgba(33,33,33,0.8) 100%);filter:progid:DXImageTransform.Microsoft.gradient( startColorstr=&#39;#00212121&#39;, endColorstr=&#39;#cc212121&#39;,GradientType=0 )}.dark-theme .aplayer .aplayer-lrc p{color:#fff}.dark-theme .aplayer .aplayer-miniswitcher{background:#484848}.dark-theme .aplayer .aplayer-miniswitcher .aplayer-icon path{fill:#eee}&lt;/style&gt;
&lt;script src=&#34;https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js&#34;&gt;&lt;/script&gt;

&lt;script src=&#34;https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js&#34;&gt;&lt;/script&gt;&lt;meting-js auto=&#34;https://music.163.com/#/playlist?id=304060&#34; theme=&#34;#2980b9&#34;&gt;&lt;/meting-js&gt;
&lt;h4 id=&#34;人生&#34;&gt;人生&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;“物来顺应，未来不迎，当时不杂，既过不恋” &amp;ndash; 曾国藩&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Yesterday is a history, tomorrow is a mystery, only today is a gift, that is why we call it present. &amp;quot; &amp;ndash; 功夫熊猫 (inner peace)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;strong&gt;Hope is the good thing and maybe the best of things And no good things ever dies&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;
~ Andy Dufresne – Shawshank Redemption&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;规律&#34;&gt;规律&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;”路漫漫其修远兮，吾将上下而求索“ &amp;ndash; 屈原《离骚》&lt;/li&gt;
&lt;li&gt;&amp;ldquo;众里寻他千百度。蓦然回首，那人却在，灯火阑珊处&amp;rdquo; &amp;ndash; 辛弃疾《青玉案·元夕》&lt;/li&gt;
&lt;li&gt;&amp;ldquo;道生一，一生二，二生三，三生万物&amp;rdquo; &amp;ndash; 老子《&lt;a href=&#34;https://baike.baidu.com/item/%E9%81%93%E5%BE%B7%E7%BB%8F/327138&#34;&gt;道德经&lt;/a&gt;》&lt;/li&gt;
&lt;li&gt;&amp;ldquo;&lt;strong&gt;万物之始，大道至简，衍化至繁&lt;/strong&gt;&amp;rdquo; &amp;ndash; 老子《&lt;a href=&#34;https://baike.baidu.com/item/%E9%81%93%E5%BE%B7%E7%BB%8F/327138&#34;&gt;道德经&lt;/a&gt;》&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>译：大型语言模型入门介绍</title>
      <link>https://weedge.github.io/post/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/</link>
      <pubDate>Mon, 04 Dec 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/transformers.svg&#34; alt=&#34;原始的 Transformer 模型结构&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;本文旨在让没有计算机科学背景的人深入了解 ChatGPT 和类似的 AI 系统（GPT-3、GPT-4、Bing Chat、Bard 等）的工作原理。ChatGPT 是一个聊天机器人——一种构建的对话式人工智能——但建立在&lt;em&gt;大型语言模型&lt;/em&gt;之上。这些绝对是文字，我们将把它们全部分解。在此过程中，我们将讨论它们背后的核心概念。本文不需要任何技术或数学背景。我们将大量使用隐喻来说明这些概念。我们将讨论为什么核心概念以它们的方式工作，以及我们可以期望或不期望像 ChatGPT 这样的大型语言模型做什么。&lt;/p&gt;
&lt;p&gt;这就是我们要做的事情。我们将温和地介绍一些与大型语言模型和 ChatGPT 相关的术语，不使用任何行话。如果我必须使用行话，我会不使用行话来分解它。我们将从“什么是人工智能”开始，然后逐步提高。我会尽可能地使用一些反复出现的隐喻。我将讨论这些技术的影响，即我们应该期望它们做什么或不应该期望它们做什么。let&amp;rsquo;s go~!&lt;/p&gt;
&lt;p&gt;注： 主要是结合论文「&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;」理解Transformer&lt;/p&gt;
&lt;h2 id=&#34;1什么是人工智能&#34;&gt;1.什么是人工智能？&lt;/h2&gt;
&lt;p&gt;但首先，让我们从一些您可能经常听到的基本术语开始。什么是&lt;em&gt;人工智能&lt;/em&gt;？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;人工智能&lt;/em&gt;：如果人类做类似的事情，则执行人们可以合理地称为智能行为的实体。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;用“智能 intelligent”这个词来定义人工智能有点问题，但没有人能就“智能”的一个好的定义达成一致。不过，我认为这仍然相当有效。它基本上是说，如果我们看到一些人造的东西，它做了一些引人入胜、有用的事情，而且看起来有些不平凡，那么我们可以称之为智能。例如，我们经常将“AI”一词归因于电脑游戏中由计算机控制的角色。大多数这些机器人都是简单的&lt;em&gt;if-then-else&lt;/em&gt;代码（例如，“如果玩家在射程内，则射击，否则移动到最近的巨石进行掩护”）。但如果我们所做的工作是让我们保持参与和娱乐，而不是做任何明显愚蠢的事情，那么我们可能会认为它们比实际情况更复杂。&lt;/p&gt;
&lt;p&gt;一旦我们了解了某些东西是如何工作的，我们可能不会留下深刻的印象，并期望幕后有更复杂的东西。这完全取决于您对幕后发生的事情的了解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;他们的关键点是人工智能不是魔法。而且因为它不是魔法，所以可以解释。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;那么让我们开始吧。&lt;/p&gt;
&lt;h2 id=&#34;2什么是机器学习&#34;&gt;2.什么是机器学习？&lt;/h2&gt;
&lt;p&gt;经常听到的与人工智能相关的另一个术语是&lt;em&gt;机器学习&lt;/em&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;机器学习&lt;/em&gt;：一种通过获取数据、形成模型然后执行该模型来创建行为的方法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有时，手动创建一堆 if-then-else 语句来捕获一些复杂的现象（例如语言）太困难了。在这种情况下，我们尝试找到一堆数据并使用可以在数据中找到模式的算法来建模。&lt;/p&gt;
&lt;p&gt;但什么是模型？模型是某些复杂&lt;em&gt;现象&lt;/em&gt;的简化。例如，模型车只是真车的更小、更简单的版本，具有许多属性，但并不意味着完全取代原车。模型车可能看起来很真实并且对某些用途有用，但我们无法将它开到商店。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/1.jpeg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;DALL-E 生成的桌面上模型汽车的图像&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;就像我们可以制造更小、更简单的汽车一样，我们也可以制造更小、更简单的人类语言。我们使用“&lt;em&gt;大语言模型&lt;/em&gt; ”这个术语，因为从使用它们需要多少内存的角度来看，这些模型很大。生产中最大的模型（例如 ChatGPT、GPT-3 和 GPT-4）足够大，需要在数据中心服务器中运行的大量超级计算机才能创建和运行。&lt;/p&gt;
&lt;h2 id=&#34;3什么是神经网络neural-network&#34;&gt;3.什么是神经网络(Neural Network)？&lt;/h2&gt;
&lt;p&gt;从数据中学习模型的方法有很多。神经网络就是这样一种方式。该技术大致基于人类大脑是如何由相互连接的脑细胞网络（称为神经元）组成的，&lt;em&gt;神经元&lt;/em&gt;来回传递电信号，以某种方式允许我们做我们所做的所有事情。神经网络的基本概念是在 20 世纪 40 年代发明的，而如何训练神经网络的基本概念是在 20 世纪 80 年代发明的。神经网络的效率非常低，直到 2017 年左右，计算机硬件才足够好，可以大规模使用它们。&lt;/p&gt;
&lt;p&gt;但我喜欢用电路来比喻神经网络，而不是大脑。您不必是一名电气工程师就知道电流通过电线流动，并且我们有一种称为电阻器(resistors)的东西，可以使电流更难流过电路的某些部分。&lt;/p&gt;
&lt;p&gt;想象一下，您想要制造一辆可以在高速公路上行驶的自动驾驶汽车。您已在汽车的正面、背面和侧面配备了接近传感器。当有物体非常接近时，接近传感器报告值 1.0；当附近检测不到任何物体时，接近传感器报告值 0.0。&lt;/p&gt;
&lt;p&gt;您还装备了您的汽车，以便机器人机构可以转动方向盘、踩刹车和踩油门。当加速器接收到值1.0时，它使用最大加速度，0.0表示没有加速度。同样，发送到制动机构的值 1.0 表示猛踩刹车，0.0 表示不制动。转向机构的值介于 -1.0 到 +1.0 之间，负值表示向左转向，正值表示向右转向，0.0 表示保持直行。&lt;/p&gt;
&lt;p&gt;您还记录了有关您驾驶方式的数据。当前面的道路畅通时，您就会加速。当前面有车时，你就减速。当一辆车距离左侧太近时，您会向右转并改变车道。当然，除非您右边也有一辆车。这是一个复杂的过程，涉及基于不同的传感器信息组合的不同动作组合（向左转向、向右转向、或多或少加速、制动）。&lt;/p&gt;
&lt;p&gt;现在您必须将传感器连接到机器人机构。你怎么做到这一点？目前还不清楚。因此，您将每个传感器连接到每个机器人执行器。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;神经网络作为连接传感器和执行器的电路&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;当你把车开到路上时会发生什么？电流从所有传感器流向所有机器人执行器，汽车同时向左转向、向右转向、加速和制动。一团糟。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/3.gif&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;当我们的一些传感器发送能量时，该能量会流向所有执行器，汽车会同时加速、制动和转向&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;那可不好。因此，我拿起电阻器(resistors)，开始将它们放在电路的不同部分，以便电流可以在某些传感器和某些机器人执行器之间更自由地流动。例如，我希望电流能够更自由地从前接近传感器流向制动器，而不是流向方向盘。我还放入了称为“门(gates)”的东西，它会停止电流流动，直到积累足够的电流来翻转开关（仅当前接近传感器和后接近传感器报告高数字时才允许电流流动），或者仅在以下情况下向前发送电能：输入电气强度低（当前接近传感器报告低值时，向加速器发送更多电力）。&lt;/p&gt;
&lt;p&gt;但我该把这些电阻器(resistors)和门(gates)放在哪里呢？我不知道。我开始将它们随机地放在各处。然后我再试一次。也许这次我的车开得更好，这意味着有时当数据表明最好制动时它会制动，当数据表明最好转向时它会转向，等等。但它并没有把所有事情都做对。有些事情它做得更糟（当数据表明最好刹车时加速）。所以我不断随机尝试电阻器(resistors)和门(gates)的不同组合。最终我会偶然发现一个效果很好的组合，我宣布成功。也许它看起来像这样：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;一个经过充分训练的神经网络。较暗的线表示电路中能量流动更自由的部分。中间的圆圈是门(gates)，在将任何能量发送到顶部之前，可能会从下方积累大量能量，或者甚至可能在下方能量很少时向上发送能量。&lt;/p&gt;
&lt;p&gt;（实际上，我们不会添加或减少门(gates)，它们总是存在的，但我们修改门(gates)，以便它们用更少的能量从下面激活或需要更多的能量从下面，或者只有当存在时才释放大量能量来自下面的能量非常少。机器学习纯粹主义者可能会在这种表征中吐出一点点。从技术上讲，这是通过调整门(gates)上的&lt;em&gt;偏差&lt;/em&gt;来完成的，这通常不会在诸如此类的图表中显示，而是在以下方面显示：电路比喻可以被认为是进入每个门(gates)的一根电线，直接插入电源，然后可以像所有其他电线一样进行修改。）&lt;/p&gt;
&lt;p&gt;让我们来试驾一下吧！&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/5.gif&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;随意尝试事情很糟糕。一种称为&lt;em&gt;反向传播的&lt;/em&gt;算法相当擅长猜测如何改变电路的配置。算法的细节并不重要，只是要知道它对电路进行了微小的改变，以使电路的行为更接近数据建议的情况，并且经过数千或数百万次调整，最终可以得到接近一致的结果数据。&lt;/p&gt;
&lt;p&gt;我们称电阻器(resistors)和门(gates)为&lt;strong&gt;参数&lt;/strong&gt;，因为实际上它们无处不在，反向传播算法所做的就是声明每个电阻器(resistors)更强或更弱。因此，如果我们知道电路的布局和参数值，则可以在其他汽车上复制整个电路。&lt;/p&gt;
&lt;h2 id=&#34;4什么是深度学习deep-learning&#34;&gt;4.什么是深度学习(Deep Learning)？&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;深度学习&lt;/em&gt;认识到除了电阻器(resistors)和门(gates)之外，我们还可以在电路中添加其他东西。例如，我们可以在电路中间进行数学计算，在向前发送电力之前将事物相加和相乘。深度学习仍然使用相同的基本增量猜测参数技术。&lt;/p&gt;
&lt;h2 id=&#34;5什么是语言模型language-model&#34;&gt;5.什么是语言模型(Language Model)？&lt;/h2&gt;
&lt;p&gt;当我们做汽车的例子时，我们试图让我们的神经网络执行与我们的数据一致的行为。我们询问是否可以创建一个电路来操纵汽车中的机制，就像驾驶员在类似情况下所做的那样。我们可以用同样的方式对待语言。我们可以查看人类编写的文本，并想知道电路是否可以产生看起来很像人类倾向于产生的单词序列的单词序列。现在，当我们看到单词时，我们的传感器就会启动，我们的输出机制也是单词。&lt;/p&gt;
&lt;p&gt;我们想做什么？我们正在尝试创建一个电路，在给定一堆输入单词的情况下猜测输出单词。例如：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Once upon a ____”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;似乎应该用“time”而不是“armadillo”来填补空白。&lt;/p&gt;
&lt;p&gt;我们倾向于从概率的角度来谈论语言模型。从数学上讲，我们将上面的例子写为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/6.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;如果您不熟悉该符号，请不要担心。这只是数学谈话，&lt;code&gt;|&lt;/code&gt;意思是在给定（条形符号表示&lt;em&gt;给定&lt;/em&gt;）一堆单词“once”、“upon”和“a”的情况下，单词“time”的概率（ &lt;em&gt;P&lt;/em&gt; ）。我们期望一个好的语言模型产生“time”一词比“armadillo”一词更高的概率。&lt;/p&gt;
&lt;p&gt;我们可以将其概括为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/7.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;这只是意味着在给定其之前的所有单词（位置 1 到n -1的单词）的情况下，计算序列中第n个单词的概率。&lt;/p&gt;
&lt;p&gt;想想老式打字机，那种带有striker arms的打字机。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/8.jpeg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;DALL-E2 制作了此图像。看看所有的striker arms！&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;除了每个字母都有不同的striker arms之外，我们为每个单词都有一个striker撞针。如果英语有 50,000 个单词，那么这就是一台大打字机！&lt;/p&gt;
&lt;p&gt;考虑一个类似的网络，而不是汽车网络，不同之处在于我们电路的顶部有 50,000 个输出连接到striker arms，每个单词一个。相应地，我们将拥有 50,000 个传感器，每个传感器检测不同输入单词的存在。因此，我们最终要做的就是选择一个能够获得最高电信号的striker arms，这就是空白处的单词。&lt;/p&gt;
&lt;p&gt;我们的立场是这样的：如果我想制作一个接受单个单词并生成单个&lt;em&gt;单词&lt;/em&gt;的简单电路，我必须制作一个具有 50,000 个传感器（每个单词一个）和 50,000 个输出（每个striker arm）的电路。我只需将每个传感器连接到每个striker arm，总共 50,000 x 50,000 = 25 亿根电线。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/9.gif&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;底部的每个圆圈都代表一个单词。需要 50,000 个传感器才能识别“once”这个词。该能量通过某个任意网络发送。顶部的所有圆圈都连接到每个单词的striker arms。所有striker arms都会接收到一些能量，但其中一只臂会比其他臂接收到更多的能量。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;那是一个大网络！&lt;/p&gt;
&lt;p&gt;但情况变得更糟。如果我想做“Once Upon a ___”示例，我需要感知三个输入位置中的每一个中是哪个单词。我需要 50,000 x 3 = 150,000 个传感器。连接多达 50,000 个striker arms，相当于 150,000 x 50,000 = 75 亿根电线。截至 2023 年，大多数大型语言模型可以容纳 4,000 个单词，最大的可以容纳 32,000 个单词。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/10.gif&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;以三个单词作为输入的网络每个单词需要 50,000 个传感器。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;我们需要一些技巧来处理这种情况。我们将分阶段进行。&lt;/p&gt;
&lt;h3 id=&#34;51-编码器encoders&#34;&gt;5.1 编码器(Encoders)&lt;/h3&gt;
&lt;p&gt;我们要做的第一件事是将电路分成两个电路，一个称为&lt;em&gt;编码器&lt;/em&gt;，另一个称为&lt;em&gt;解码器&lt;/em&gt;。许多单词的含义大致相同。考虑以下短语：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The king sat on the __&lt;/p&gt;
&lt;p&gt;The queen sat on the __&lt;/p&gt;
&lt;p&gt;The princess sat on the __&lt;/p&gt;
&lt;p&gt;The regent sat on the __&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对上述所有空白的合理猜测是“throne”（或者可能是“toilet”）。这就是说，我可能不需要在“king”和“throne”之间，或者“queen”和“throne”之间等之间使用单独的电线。相反，如果我有一些大约意味着皇室的东西，并且每次我看到“king”或“queen”，我用这个中间的东西代替。然后我只需要担心哪些词的意思大致相同，然后该怎么办（向“throne”发送大量能量）。&lt;/p&gt;
&lt;p&gt;这就是我们要做的。我们将建立一个电路，该电路需要 50,000 个字传感器并映射到一些较小的输出集，例如 256 个而不是 50,000 个。我们不仅能够触发一个striker arms，还可以一次粉碎一堆臂。striker arms的每种可能组合都可以代表不同的概念（例如“royalty”或“armored mammals”）。这 256 个输出将使我们能够表示 2²⁵⁶ = 1.15 x 10⁷⁸ 的概念。事实上，更重要的是，就像在汽车示例中我们可以半踩刹车一样，这 256 个输出中的每一个都可以不仅仅是 1.0 或 0.0，而是中间的任何数字。因此，也许更好的比喻是，所有 256 个striker arms都猛烈撞击，但每个猛烈撞击的力量不同。&lt;/p&gt;
&lt;p&gt;好吧……以前一个单词需要 50,000 个传感器(sensors)中的一个才能触发。现在，我们已将 1 个激活的传感器和 49,999 个关闭的传感器精简为 256 个数字。因此，“king”可能是 [0.1, 0.0 , 0.9, …, 0.4]，“queen”可能是 [0.1, 0.1 , 0.9, …, 0.4]，它们几乎彼此相同。我将这些数字列表称为&lt;em&gt;编码encodings&lt;/em&gt;（由于历史原因也称为&lt;em&gt;隐藏状态&lt;/em&gt;，但我不想解释这一点，所以我们将坚持使用编码）。我们将把 50,000 个传感器压缩成 256 个输出的电路称为&lt;em&gt;编码器encoder&lt;/em&gt;。它看起来像这样：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/11.gif&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;编码器网络将检测单个单词所需的 50,000 个传感器值压缩为 256 个数字的编码（较浅和较深的蓝色用于指示较高或较低的值）。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;52-解码器-decoders&#34;&gt;5.2 解码器 (Decoders)&lt;/h3&gt;
&lt;p&gt;但编码器并没有告诉我们接下来应该出现哪个单词。因此，我们将编码器与解码器网络配对。解码器是另一个电路，它采用 256 个数字进行编码，并激活原来的 50,000 个撞击臂(striker arms)，每个单词一个。然后我们会选择电力输出最高的单词。它看起来是这样的：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/12.gif&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;解码器网络，将编码中的 256 个值扩展为与每个可能单词相关的 50,000 个撞击臂的激活值。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;53-编码器和解码器一起使用&#34;&gt;5.3 编码器和解码器一起使用&lt;/h3&gt;
&lt;p&gt;这是编码器和解码器一起工作以构成一个大型神经网络：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/13.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;编码器-解码器网络，它只是位于编码器之上的解码器。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;顺便说一句，单个单词输入到单个单词输出经过编码只需要 (50,000 x 256) x 2 = 2560 万个参数。这看起来好多了。&lt;/p&gt;
&lt;p&gt;该示例针对一个单词输入并生成一个单词输出，因此如果我们想要读取n个单词，我们将有 50,000 x n个输入，以及 256 x n 的编码&lt;/p&gt;
&lt;p&gt;但为什么这会起作用呢？通过强制将 50,000 个单词全部放入一小组数字中，我们迫使网络做出妥协并将单词组合在一起，这可能会触发相同的输出单词猜测。这很像文件压缩。当您压缩文本文档时，您会得到一个不再可读的较小文档。但您可以解压缩文档并恢复原始可读文本。这是可以做到的，因为 zip 程序用速记符号替换了某些单词模式。然后，当它解压缩时，它知道要换回什么文本来替换速记符号。我们的编码器和解码器电路学习压缩然后解压缩单词的电阻器(resistors)和门(gates)的配置。&lt;/p&gt;
&lt;h3 id=&#34;54-自我监督self-supervision&#34;&gt;5.4 自我监督(Self-Supervision)&lt;/h3&gt;
&lt;p&gt;我们如何知道每个单词的最佳编码？换句话说，我们如何知道“king”的编码应该类似于“queen”而不是“armadillo”的编码？&lt;/p&gt;
&lt;p&gt;作为一个思想实验，考虑一个编码器-解码器网络，它应该接收单个单词（50,000 个传感器）并产生完全相同的单词作为输出。这是一件愚蠢的事情，但对接下来的事情很有启发。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/14.gif&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;编码器-解码器网络经过训练，输出与输入相同的单词（与之前的图像相同，但具有用于激活的颜色）。&lt;/p&gt;
&lt;p&gt;我输入“king”一词，单个传感器通过编码器发送电信号，并部分打开中间编码中的 256 个值。如果编码正确，那么解码器将向同一个单词“king”发送最高的电信号。对吧，容易吗？没那么快。很可能会看到带有“armadillo”一词且激活能量最高的撞击臂striker arm。假设“king”的撞击臂收到 0.051 电信号，“armadillo”的撞击臂收到 0.23 电信号。事实上，甚至不关心“armadillo”的价值是多少。只要看看“king”的输出能量就知道它不是1.0。1.0 和 0.051 之间的差异是错误（也称为&lt;em&gt;损失loss&lt;/em&gt;），可以使用反向传播对解码器和编码器进行一些更改，以便下次我们看到“king”一词时进行稍微不同的编码。&lt;/p&gt;
&lt;p&gt;我们对所有单词都这样做。编码器必须做出妥协，因为 256 比 50,000 小得多。也就是说，有些单词必须在中间使用相同的激活才能组合。因此，当有选择时，它会希望“king”和“queen”的编码几乎相同，而“armadillo”的编码则非常不同。这将使解码器能够通过查看 256 个编码值更好地猜测单词。如果解码器看到 256 个值的特定组合并猜测“king”为 0.43，“queen”为 0.42，只要“king”和“queen”获得最高的电信号并且每个49,998 个striker arms中的数字较小。另一种说法是，与网络在king和armadillo之间混淆相比，我们可能更愿意接受网络在king和queen之间混淆。&lt;/p&gt;
&lt;p&gt;我们说神经网络是&lt;em&gt;自我监督的&lt;/em&gt;，因为与汽车示例不同，不必收集单独的数据来测试输出。我们只需将输出与输入进行比较 - 我们不需要输入和输出的单独数据。&lt;/p&gt;
&lt;h3 id=&#34;55-遮盖语言模型masked-language-models&#34;&gt;5.5 遮盖语言模型（Masked Language Models）&lt;/h3&gt;
&lt;p&gt;如果上面的思想实验看起来很愚蠢，那么它是一种被称为“&lt;em&gt;遮盖语言模型&lt;/em&gt; ”的东西的构建模块。遮盖语言模型的思想是接收单词序列并生成单词序列。输入和输出中的单词之一被空白。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The [MASK] sat on the throne.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;网络猜测所有单词。嗯，很容易猜出未遮盖的单词。我们只关心网络对遮盖词的猜测。也就是说，对于输出中的每个单词，我们有 50,000 个striker arms。我们看看 50,000 个striker arms中的遮盖词。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/15.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;遮盖序列。厌倦了画很多连接线，所以我只会画红线来表示上面和下面的所有东西之间有很多很多的连接&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;可以移动遮盖，让网络在不同的地方猜测不同的单词。&lt;/p&gt;
&lt;p&gt;一种特殊类型的遮盖语言模型仅在末尾有遮盖。这称为&lt;em&gt;生成模型generative model&lt;/em&gt;，因为它猜测的遮盖始终是序列中的下一个单词，这相当于生成下一个单词，就好像下一个单词不存在一样。像这样：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The [MASK]&lt;/p&gt;
&lt;p&gt;The queen [MASK]&lt;/p&gt;
&lt;p&gt;The queen sat [MASK]&lt;/p&gt;
&lt;p&gt;The queen sat on [MASK]&lt;/p&gt;
&lt;p&gt;The queen sat on the [MASK]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们也称其为&lt;em&gt;自回归auto-regressive模型&lt;/em&gt;。regressive回归这个词听起来不太好。但回归只是意味着试图理解事物之间的关系，比如已经输入的单词和应该输出的单词。Auto的意思是“self”。自回归模型是自我预测的。它预测一个词；然后该单词用于预测下一个单词，下一个单词又用于预测下一个单词，依此类推。这有一些有趣的含义，我们稍后会再讨论。&lt;/p&gt;
&lt;h2 id=&#34;6什么是transformer&#34;&gt;6.什么是transformer？&lt;/h2&gt;
&lt;p&gt;截至撰写本文时，我们听到了很多有关 GPT-3、GPT-4 和 ChatGPT 的信息。GPT 是 OpenAI 公司开发的一种大型语言模型的特定品牌。GPT 代表&lt;em&gt;基于转换器的生成式预训练模型 Generative Pre-trained transformer&lt;/em&gt;。让我们来分解一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;生成式 Generative&lt;/em&gt;。该模型能够生成所提供输入的延续。也就是说，给定一些文本，模型会尝试猜测接下来出现哪些单词。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;预训练 Pre-trained&lt;/em&gt;。该模型是在非常大的通用文本语料库上进行训练的，并且意味着只需训练一次即可用于许多不同的事情，而无需从头开始重新训练。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有关预训练的更多信息…… 该模型是在一个非常大的通用文本语料库上进行训练的，该语料库表面上涵盖了大量可以想象的主题。这或多或少意味着“从互联网上抓取”，而不是从一些专门的文本存储库中获取。通过对一般文本进行训练，语言模型比在非常特定类型的文本（例如来自医疗文档的文本）上训练的语言模型更能够响应更广泛的输入。理论上，在通用语料库上训练的语言模型可以合理地响应互联网文档中可能出现的任何内容。它可能适合医学文本。仅针对医疗文档进行训练的语言模型可能会对与医疗环境相关的输入做出很好的响应，但在响应闲聊或食谱等其他输入时却表现不佳。&lt;/p&gt;
&lt;p&gt;要么模型在很多方面都足够好，以至于人们永远不需要训练自己的模型，要么可以进行称为&lt;em&gt;微调fine-tuning&lt;/em&gt;的操作，这意味着采用预先训练的模型并对其进行一些更新以使其更好地工作执行专门任务（例如医疗）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;transformer。一种特定类型的自监督编码器-解码器深度学习模型，具有一些非常有趣的属性，使其擅长语言建模。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;转换器transformer是一种特殊类型的深度学习模型，它以特定方式转换编码，从而更容易猜测空白单词。Vaswani 等人在一篇名为&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;&lt;em&gt;《Attention is All You Need》&lt;/em&gt;&lt;/a&gt;的论文中介绍了它。2017 年。Transformer 的核心是经典的编码器-解码器网络(encoder-decoder network)。编码器执行非常标准的编码过程。如此香草，你会感到震惊。但随后它添加了其他东西，称为&lt;em&gt;self-attention&lt;/em&gt;。&lt;/p&gt;
&lt;h3 id=&#34;61-自注意力self-attention&#34;&gt;6.1 自注意力(Self-Attention)&lt;/h3&gt;
&lt;p&gt;这是自注意力的想法：序列中的某些单词与序列中的其他单词相关。考虑一下这句话“外星人登陆地球是因为它需要躲在一个星球上。” 如果我们要掩盖第二个词“外星人alien”并要求神经网络猜测这个词，那么由于“landed”和“earth”等词，它的猜测会更好。同样，如果我们屏蔽“it”并要求网络猜测这个词，“alien”这个词的存在可能会让它更喜欢“it”而不是“he”或“she”。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/16.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;单词通过功能、指代同一事物或通过告知彼此的含义而与其他单词相关。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;我们说序列中的单词关注其他单词，因为它们捕获了某种关系。这种关系不一定是已知的。它可以是解析代词，可以是动词和主语关系，可以是与同一概念相关的两个单词（“地球”和“行星”）。不管是什么，知道单词之间存在某种关系对于预测很有用。&lt;/p&gt;
&lt;p&gt;下一节将讨论自注意力的数学原理，但主要要点是transformer学习输入序列中的哪些单词是相关的，然后为输入序列中的每个位置创建一个新的编码，该编码是所有单词的合并。相关词汇。你可以将其视为学习创造一个由“外星人alien”、“登陆landed”和“地球earth”混合而成的新词（aliandearth？）。这是可行的，因为每个单词都被编码为数字列表。如果&lt;em&gt;Alien&lt;/em&gt; = [0.1, 0.2, 0.3, …, 0.4] 且&lt;em&gt;landed&lt;/em&gt; = [0.5, 0.6, 0.7, …, 0.8] 且&lt;em&gt;Earth&lt;/em&gt; = [0.9, 1.0, 1.1, …, 1.2]，则第二个单词位置可能是编码为所有这些编码的总和，[1.5, 1.8, 2.1, …, 2.4]，它本身不对应于任何单词，但捕获所有单词的片段。这样，当解码器最终在第二个位置看到该单词的新编码时，它就可以获得有关该单词在序列中如何使用的大量信息，从而更好地猜测任何遮盖。（该示例只是将编码添加在一起，但会比这更复杂一些）。&lt;/p&gt;
&lt;h3 id=&#34;62-自注意力如何发挥作用&#34;&gt;6.2. 自注意力如何发挥作用？&lt;/h3&gt;
&lt;p&gt;自注意力是对普通编码器-解码器网络的显着改进，因此如果您想了解更多有关其工作原理的信息，请继续阅读。否则，请随意跳过本节。TL;DR：自注意力是称为&lt;em&gt;点积dot product&lt;/em&gt;的数学运算的一个奇特名称。&lt;/p&gt;
&lt;p&gt;自注意力发生在三个阶段。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;像平常一样对输入序列中的每个单词进行编码。我们制作了四个单词编码副本。我们将其中的一种称为&lt;em&gt;残留物 residual&lt;/em&gt;并设置安全保存。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我们对其他三个运行第二轮编码（我们正在编码）。每个都经历了不同的编码过程，因此它们都变得不同。我们将其中一个称为查询 ( &lt;em&gt;q&lt;/em&gt; )，将一个称为键 ( &lt;em&gt;k&lt;/em&gt; )，将一个称为值 ( &lt;em&gt;v&lt;/em&gt; )。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我想让你考虑一下哈希表（在 python 中也称为字典）。在表中存储了大量信息。表中的每一行都有一个&lt;em&gt;key&lt;/em&gt;、一些唯一标识符和&lt;em&gt;value&lt;/em&gt;，即存储在该行中的数据。要从哈希表中检索一些信息，需要发出查询。如果查询与键匹配，则提取值。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/17.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;可以使用哈希表来查询教授在哪所大学工作&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;自注意力的工作原理有点像&lt;em&gt;模糊 Fuzzy&lt;/em&gt;哈希表(LSH属于这类hash)。提供一个查询，它不会查找与键的精确匹配，而是根据查询和键之间的相似性查找近似匹配。但如果匹配不是完美匹配怎么办？它返回值的一部分。嗯，只有当查询、键和值都是数字时，这才有意义。它们是：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/18.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;具有部分匹配的哈希表&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;这就是我们要做的。对于输入中的每个单词位置，我们将采用&lt;em&gt;q&lt;/em&gt;编码和&lt;em&gt;k&lt;/em&gt;编码并计算相似度。我们使用称为点积的东西，也称为余弦相似度。这不重要，关键是每个单词都是 256 个数字的列表（基于我们之前的示例），我们可以计算数字列表的相似度并将相似度记录在矩阵中。我们将此矩阵称为&lt;em&gt;自注意力分数 self-attention scores&lt;/em&gt;。如果我们有一个三个单词的输入序列，我们的注意力分数可能如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/19.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;每个单元格指示一个位置中的编码字对另一位置中的编码字的关注程度。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;网络将第一个单词视为查询，并与第二个键进行匹配（我们可以说第一个单词“参与”第二个单词）。如果第二个单词是查询，它将与第三个键匹配。如果第三个单词是查询，它将与第一个键匹配。事实上，我们永远不会有这样的 1 和 0；我们将在 0 和 1 之间进行部分匹配，并且每个查询（行）将部分匹配多个键（列）。&lt;/p&gt;
&lt;p&gt;现在继续使用检索隐喻，我们将该矩阵与&lt;em&gt;v&lt;/em&gt;编码相乘，然后发生了一些有趣的事情。假设我们的&lt;em&gt;v&lt;/em&gt;编码如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/20.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;每行都是序列中一个单词的编码。&lt;/p&gt;
&lt;p&gt;也就是说，第一个单词被编码为数字0.10…0.19的列表，第二个单词被编码为数字0.20…0.29的列表，第三个单词被编码为数字0.30…0.39的列表。这些数字是为了说明目的而编造的，永远不会如此整齐。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/21.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;将注意力与值相乘&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;第一个查询与第二个键匹配，因此检索第二个编码字。第二个查询与第三个键匹配，因此检索第三个编码字。第三个查询与第一个键匹配，因此检索第一个编码的单词。我们有效地做的是交换行！&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/a-very-gentle-introduction-to-large-language-models-without-the-hype/22.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;实际上，分数不会是完美的 1 和 0，结果将是每种编码的一点点混合在一起（例如 97% 的单词 1 加 1% 或单词 3 加 2% 的单词 2）。但这说明了自注意力是如何混合和交换的。在这个极端版本中，第一个单词已被交换为第二个单词，依此类推。所以也许“earth”这个词已经被“planet”这个词互换了。&lt;/p&gt;
&lt;p&gt;我们如何知道我们正确编码了&lt;em&gt;q&lt;/em&gt;、&lt;em&gt;k&lt;/em&gt;和&lt;em&gt;v&lt;/em&gt;？如果整个网络猜测遮盖的最佳单词的能力得到提高，那么我们就可以正确编码&lt;em&gt;q&lt;/em&gt;、&lt;em&gt;k&lt;/em&gt;和&lt;em&gt;v&lt;/em&gt;。如果没有，我们下次会更改参数以进行稍微不同的编码。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;我们做的第三件事是获取所有数学结果并将其添加到残差中。请记住我们预留的原始编码的第一个副本。没错，我们添加了混合和交换的版本。现在“earth”不仅仅是“earth”的编码，而是某种虚构的单词，是“earth”和“planet”的混搭……pearth？ealanet？不是那样的。无论如何，这是将发送到解码器的最终转换编码。我们可能会同意，在每个位置都有一个真正编码两个或多个单词的假单词对于基于每个位置的单个单词进行预测更有用。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;然后，再一次又一次地重复此操作几次（多层）。&lt;/p&gt;
&lt;p&gt;我省略了很多关于编码器的最终编码如何进入解码器的细节（另一种注意力，称为&lt;em&gt;source-attention&lt;/em&gt;，其中编码器的每个位置的编码用作&lt;em&gt;q&lt;/em&gt;和&lt;em&gt;k&lt;/em&gt;来应用于另一个位置&lt;em&gt;v&lt;/em&gt;的不同版本），但此时您应该有一个大概的要点。最后，解码器从编码器接收编码，将能量发送到单词的撞击臂，然后我们选择能量最强的单词。&lt;/p&gt;
&lt;h2 id=&#34;7-为什么大型语言模型如此强大&#34;&gt;7. 为什么大型语言模型如此强大？&lt;/h2&gt;
&lt;p&gt;那么，这意味着什么？大型语言模型，包括 ChatGPT、GPT-4 等，只做一件事：它们接收一堆单词并尝试猜测接下来应该出现什么单词。如果这是“推理”或“思考”，那么它只是一种非常专业的形式。&lt;/p&gt;
&lt;p&gt;但即使是这种专门的形式似乎也非常强大，因为 ChatGPT 和类似的东西可以做很多看起来非常好的事情：写诗、回答有关科学和技术的问题、总结文档、起草电子邮件，甚至编写代码，仅举几例。为什么他们应该工作得这么好？&lt;/p&gt;
&lt;p&gt;秘诀有两个。第一个我们已经讨论过：transformer学习以一种使其非常擅长猜测下一个单词的方式混合单词上下文。另一部分是系统的训练方式。大型语言模型是根据从互联网上抓取的大量信息进行训练的。这包括书籍、博客、新闻网站、维基百科文章、reddit 讨论、社交媒体对话。在训练期间，我们从这些来源之一提供一段文本，并要求它猜测下一个单词。记住：自我监督。如果它猜错了，我们会稍微调整模型，直到猜对为止。&lt;strong&gt;如果我们要考虑LLM接受训练的目的，那就是生成可以合理出现在互联网上的文本&lt;/strong&gt;。它无法记住互联网，因此它使用编码来做出妥协，并让事情变得有点错误，但希望不会错得太离谱。&lt;/p&gt;
&lt;p&gt;重要的是不要低估互联网上文本主题的多样性。LLM 已经看到了这一切。他们已经看到了几乎每个主题的数十亿次对话。因此，LLM可以产生看起来像是在与您对话的语言。它已经看到了数十亿首诗歌和音乐歌词，几乎涵盖了所有可以想象的内容，因此它可以生成看起来像诗歌的文本。它已经看到了数十亿份作业及其解决方案，因此即使略有不同，它也可以对您的作业做出合理的猜测。它已经看到了数十亿的标准化测试问题及其答案。我们真的认为今年的 SAT 题目与去年有很大不同吗？它见过人们谈论他们的假期计划，因此它可以猜测看起来像假期计划的单词。它已经看到了数十亿个执行各种操作的代码示例。计算机程序员所做的很多事情都是将用于执行非常典型和易于理解的事情的代码片段组装成更大的代码块。因此，LLM可以为您编写那些小的、常见的片段。人们在 stackoverflow.com 上看到了数十亿个错误代码的示例及其更正。是的，所以它可以接收损坏的代码并提出修复建议。数十亿人在推特上表示，他们触摸了热炉并烧伤了手指，因此LLM知道一些常识。它阅读了大量的科学论文，因此它可以猜测众所周知的科学事实，即使它们对你来说并不熟悉。它已经看到了数十亿个人们总结、将文本重写为要点的例子，描述了如何使文本更加语法、简洁或有说服力。&lt;/p&gt;
&lt;p&gt;重点是：当您要求 ChatGPT 或其他大型语言模型做一些聪明的事情（并且它有效）时，您很有可能要求它做一些它已经见过数十亿个示例的事情。即使你想出一些非常独特的东西，比如“告诉我闪电侠戈登吃了六个墨西哥卷饼后会做什么”（这是独特的，我什至不知道），它已经看到了关于闪电侠戈登的粉丝小说，它已经看到了人们谈论吃太多墨西哥卷饼，并且由于自我关注，可以混合搭配零碎的东西来组合出听起来合理的反应。&lt;/p&gt;
&lt;p&gt;与大型语言模型交互时，我们的第一直觉不应该是“哇这些东西一定非常聪明或非常有创意或非常理解”。我们的第一直觉应该是“我可能要求它做一些它以前见过的事情”。这可能意味着它仍然非常有用，即使它不是“认真思考”或“做一些非常复杂的推理”。&lt;/p&gt;
&lt;p&gt;我们不必使用拟人化来理解它正在做什么来为我们提供响应。&lt;/p&gt;
&lt;p&gt;关于这个主题的最后一点是：由于大型语言模型的工作方式和它们的训练方式，它们往往提供的答案在某种程度上是中值响应。对于我来说，模型在询问有关飞侠戈登的故事后往往会给出平均的回答，这似乎很奇怪。但在一个故事或一首诗的背景下，这些回应可以被认为是很多人（在互联网上写作）如果不得不妥协的话会想到的。不会是坏事。按照一个人坐着思考自己的事情的标准来看，这可能相当不错。但你的故事和诗歌可能也只是一般（但它们对你来说很特别）。&lt;/p&gt;
&lt;h2 id=&#34;8-我应该注意什么&#34;&gt;8. 我应该注意什么？&lt;/h2&gt;
&lt;p&gt;transformer的工作方式和训练方式会产生一些非常微妙的影响。以下是技术细节的直接影响。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;大型语言模型是在互联网上训练的。这意味着他们也对人性的所有黑暗部分进行了训练。大型语言模型接受过种族主义咆哮、性别歧视长篇大论、针对各种类型的人的各种侮辱、人们对他人做出刻板假设、阴谋论、政治错误信息等的训练。这意味着语言模型选择生成的单词可能会重复出现这样的语言。&lt;/li&gt;
&lt;li&gt;大型语言模型没有“核心信念”。他们是猜词者；他们试图预测如果同一个句子出现在互联网上，下一个单词会是什么。因此，人们可以要求大型语言模型写出一个支持某事物或反对同一事物的句子，并且语言模型将遵守这两种方式。这些并不表明它相信一件事或另一件事，或者改变它的信念，或者一个比另一个更正确。如果训练数据有更多的一件事与另一件事的例子，那么大型语言模型往往会对其训练数据中更频繁出现的内容做出更一致的响应，因为它更频繁地出现在互联网上。请记住：该模型正在努力模拟最常见的响应。&lt;/li&gt;
&lt;li&gt;大型语言模型没有任何真理或正确或错误的感觉。有些事情我们认为是事实，比如地球是圆的。LLM 往往会这么说。但如果上下文正确，它也会说相反的内容，因为互联网上确实有关于地球是平的文字。无法保证LLM会提供真相。可能会倾向于猜测我们同意的词语是真实的，但这是我们可能对LLM“知道”真相或正确或错误做出任何声明的最接近的说法。&lt;/li&gt;
&lt;li&gt;大型语言模型可能会出错。训练数据可能有很多不一致的材料。当我们提出问题时，自我注意力可能不会关注到我们想要关注的所有事情。作为一个单词猜测器，它可能会做出不幸的猜测。有时，训练数据多次看到一个单词，以至于它更喜欢该单词，即使它对输入没有意义。上述情况导致了一种称为“&lt;strong&gt;幻觉&lt;/strong&gt;”的现象，其中一个单词是猜测的，既不是从输入中得出的，也不是“正确的”。LLM倾向于猜测小数字而不是大数字，因为小数字更常见。所以LLM不擅长数学。LLM偏爱数字“42”，因为人类会因为一本特别的名著而偏爱数字“42”。LLM更喜欢更常见的名字，因此可能会使用作者的名字。&lt;/li&gt;
&lt;li&gt;大型语言模型是自回归的。因此，当他们做出我们可能认为很差的猜测时，这些猜测的单词会被添加到他们自己的输入中，以进行下一个单词的猜测。即：错误不断累积。即使只有 1% 的错误机会，自注意力也可以注意到错误的选择，并加倍关注该错误。即使只犯了一个错误，随后发生的所有事情都可能与该错误有关。然后语言模型可能会在此基础上产生额外的错误。transformer没有办法“改变主意”、重试或自我纠正。他们随波逐流。&lt;/li&gt;
&lt;li&gt;人们应该始终验证大型语言模型的输出。如果你要求它做你自己无法胜任的事情，那么你应该考虑一下你是否愿意对所犯的任何错误采取行动。对于低风险任务，比如写短篇小说，这可能没问题。对于高风险任务，例如尝试获取信息来决定投资哪些股票，这些错误可能会导致您做出代价高昂的决定。&lt;/li&gt;
&lt;li&gt;自注意力意味着您在输入提示中提供的信息越多，响应就会越专业，因为它会将您的更多单词混合到其猜测中。响应的质量与输入提示的质量成正比。更好的提示会产生更好的结果。尝试几种不同的提示，看看哪种最适合您。不要假设语言模型“理解”了您想要做的事情，并且会在第一次就给出最好的结果。&lt;/li&gt;
&lt;li&gt;并没有真正与大型语言模型“对话”。大型语言模型不会“记住”交换中发生的事情。初始化输入，响应出来，LLM什么都不记得。初始输入、响应以及对响应的响应都会输入。因此，如果它看起来像是在记住，那是因为对话日志变成了全新的输入。这是front-end的一个编程技巧，使大语言模型看起来像是在进行对话。由于这个技巧，它可能会留在主题上，但不能保证它不会与之前的回应相矛盾。此外，可以输入大型语言模型的单词数量也有限制（目前 ChatGPT 允许大约 4,000 个单词，GPT-4 允许大约 32,000 个单词）。输入大小可能非常大，因此对话通常会在一段时间内显得保持连贯。最终，累积的日志将变得太大，对话的开头将被删除，系统将“忘记”之前的事情。&lt;/li&gt;
&lt;li&gt;大型语言模型不解决问题或规划。但你可以要求他们制定计划并解决问题。我要在这里吹毛求疵。&lt;em&gt;问题解决Problem-solving&lt;/em&gt;和&lt;em&gt;规划planning&lt;/em&gt;是人工智能研究社区中某些团体保留的术语，含义非常具体。特别是，它们意味着有一个目标——你想在未来完成的事情——并通过在可能使人更接近该目标的替代方案之间做出选择来努力实现该目标。大型语言模型没有目标。他们的目标是选择一个最有可能出现在给定输入序列的训练数据中的单词。它们是模式匹配的。特别是规划，通常涉及所谓的“&lt;em&gt;前瞻look-ahead&lt;/em&gt; ”。当人类进行规划时，他们会想象自己行动的结果，并根据目标分析未来。如果看起来离目标更近了，那么这是一个很好的举动。如果没有，我们可能会尝试想象另一个行动的结果。事情远不止这些，但关键点是大型语言模型没有&lt;em&gt;目标&lt;/em&gt;，也不做&lt;em&gt;前瞻&lt;/em&gt;。transformer是向后看的。自注意力只能应用于已经出现过的输入词。现在，大型语言模型可以生成看起来像计划的输出，因为它们在训练数据中看到了很多计划。他们知道计划是什么样子，他们知道关于他们所看到的某些主题的计划中应该出现什么内容。它将对该计划做出很好的猜测。该计划可能会忽略有关世界的特定细节，而倾向于最通用的计划。大型语言模型当然没有“仔细考虑替代方案”，或者尝试了一件事然后回溯并尝试另一件事。transformer内部没有任何机制可以指出可以对未来进行如此反复的考虑。（对此有一个警告，将在下一节中提出。）在询问计划时始终验证输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;9-是什么让-chatgpt-如此特别&#34;&gt;9. 是什么让 ChatGPT 如此特别？&lt;/h2&gt;
&lt;p&gt;“所以我听说 RLHF 使 ChatGPT 变得非常智能。”&lt;/p&gt;
&lt;p&gt;“ChatGPT 使用强化学习，这就是它如此智能的原因。”&lt;/p&gt;
&lt;p&gt;嗯……有点。&lt;/p&gt;
&lt;p&gt;*截至撰写本文时，人们对 RLHF（即人类反馈强化学习）*感到非常兴奋。我们做了一些事情来特别训练 ChatGPT（以及越来越多的其他大型语言模型）。它们并不是全新的，但在 ChatGPT 发布时被广泛引入并产生了巨大的效果。&lt;/p&gt;
&lt;p&gt;ChatGPT 是一个基于 Transformer 的大型语言模型。ChatGPT 因其非常擅长对输入提示做出响应以及拒绝回答有关某些可能被认为有毒或固执己见的主题的问题而赢得了声誉。它没有做任何与上面描述的特别不同的事情。事实上，它很香。但有一个区别：它是如何训练的。ChatGPT 的训练方式与往常一样——抓取互联网的一大块内容，获取该文本的片段，然后让系统预测下一个单词。这产生了一个已经是非常强大的单词预测器的基本模型（相当于 GPT-3）。但接下来还有两个额外的训练步骤。通过人类反馈进行指令调整和强化学习。&lt;/p&gt;
&lt;h3 id=&#34;91-指令调优-instruction-tuning&#34;&gt;9.1. 指令调优 Instruction Tuning&lt;/h3&gt;
&lt;p&gt;大型语言模型有一个特殊问题：它们只想获取输入的单词序列并生成接下来的内容。大多数时候，这就是一个人想要的。但不总是。考虑以下输入提示：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“写一篇关于亚历山大·汉密尔顿的文章。”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;你认为回应应该是什么。您可能认为应该是这样的内容：“亚历山大·汉密尔顿 1757 年出生于尼维斯。他是一位政治家、律师、陆军上校和美国第一任财政部长……”但您实际上可能会这样想得到的是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“你的文章应该至少有五页，双倍行距，并且至少包含两次引用。”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;刚刚发生了什么？好吧，语言模型可能已经看到了很多学生作业的例子，这些例子以“写一篇关于……”的文章开头，并包含详细说明长度和格式的单词。当然，当你写“写一篇文章……”时，你认为你正在向语言模型编写指令，就好像它是一个理解意图的人一样。语言模型不理解你的意图或者有自己的意图；他们只将输入与他们在训练数据中看到的模式进行匹配。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，可以采取一种称为&lt;em&gt;指令调整的&lt;/em&gt;方法。这个想法相当简单。如果您得到错误的响应，请写下正确的响应应该是什么，并通过神经网络发送原始输入和新的校正输出作为训练数据。有了足够多的校正输出示例，系统将学习改变其电路，以便首选新答案。&lt;/p&gt;
&lt;p&gt;一个人不必做任何太花哨的事情。&lt;strong&gt;只需让很多人与大型语言模型进行交互，并要求它做很多事情，并在其行为不正确时写下更正即可；然后收集所有出错的例子和新的、正确的输出，并进行更多的训练&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这使得大语言模型表现得好像它理解输入提示的意图并且表现得好像它正在遵循指令一样。除了尝试猜测下一个单词之外，它没有做任何其他事情。但现在新的训练数据可以猜测似乎对输入更敏感的单词。&lt;/p&gt;
&lt;h3 id=&#34;92-从人类反馈中强化学习&#34;&gt;9.2. 从人类反馈中强化学习&lt;/h3&gt;
&lt;p&gt;训练的下一步是根据人类反馈进行强化学习。我认为这需要一些解释。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;是一种传统上用于一些机器人研究和虚拟游戏代理的人工智能技术（想想可以下国际象棋、围棋或星际争霸的人工智能系统）。&lt;em&gt;强化学习&lt;/em&gt;特别擅长弄清楚当获得奖励时要做什么。奖励只是一个数字，表明它做得有多好（+100 表示做得很好；-100 表示做得很差）。在现实世界和游戏中，奖励通常很少给予。在游戏中，您可能需要做很多动作才能获得分数。也许你只有在比赛结束时才能获得积分。在现实世界中，当你做得很好时，没有足够多的人告诉你（你确实做得很好）。除非你是一只狗（他们都是好孩子）。您真正需要知道的唯一一件事是，强化学习系统尝试预测他们将获得多少未来奖励，然后选择最有可能获得更多未来奖励的行动。这与人们使用狗粮来教狗行为的方式并非完全不同。&lt;/p&gt;
&lt;p&gt;好的，把它们全部藏起来并考虑以下提示：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What is Mark an expert in? (马克是哪方面的专家？)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假设语言模型的输出为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Mark has many publications in artificial intelligence, graphics, and human-computer interaction. (马克在人工智能、图形学和人机交互方面发表了许多出版物。)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这只是部分正确。我不以图形形式发布。我真的只是想给它一个反对，或者-1分。但只有一处是错误的：图形一词。如果我告诉系统整个句子都是错误的，语言模型可能会知道应该避免所有这些单词。嗯，其中很多话都是有道理的。&lt;/p&gt;
&lt;p&gt;这就是强化学习的用武之地。强化学习的工作原理是尝试不同的替代方案，看看哪些替代方案能获得最大的回报。假设我要求它对原始提示生成三个不同的响应。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Mark has many publications in artificial intelligence, graphics, and human-computer interaction. (马克在人工智能、图形学和人机交互方面发表了许多出版物。)&lt;/p&gt;
&lt;p&gt;Mark has worked in artificial intelligence, safe NLP systems, and human-computer interaction. (马克曾从事人工智能、安全 NLP 系统和人机交互领域的工作。)&lt;/p&gt;
&lt;p&gt;Mark as researched artificial intelligence, game AI, and graphics. (标记为研究人工智能、游戏人工智能和图形。)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我可以对第一个选项表示反对 (-1)，对第二个选项表示赞成 (+1)，对第三个选项表示反对 (-1)。就像玩游戏一样，强化学习算法可以回顾并找出导致 -1 的共同点是“图形”一词。现在，系统可以将该单词归零，并调整神经网络电路，以不将该单词与特定的输入提示结合使用。&lt;/p&gt;
&lt;p&gt;我们将再次让一群人与大型语言模型进行交互。这次我们将为人们提供三种（或更多）可能的答案。我们可以通过要求大型语言模型多次响应提示并为前锋武器的选择引入一点随机性来做到这一点（没有忘记这些，不是吗？）。我们有时可能会选择第二或第三高激活的striker arms，而不是选择最高激活的striker arms。这会给出不同的文本响应，我们要求人们选择他们最喜欢的第一个响应、第二个最喜欢的响应，依此类推。现在我们有替代方案，我们有数字。现在我们可以使用强化学习来调整神经网络电路。&lt;/p&gt;
&lt;p&gt;[实际上，我们使用这些赞成和反对反馈来训练第二个神经网络来预测人们是否会赞成或反对。如果该神经网络足以预测人们的喜好，那么我们可以使用第二个神经网络来猜测语言模型的响应是否会得到赞成或反对，并用它来训练语言模型。]&lt;/p&gt;
&lt;p&gt;强化学习将文本的生成视为一个游戏，其中每个动作都是一个单词。在序列结束时，语言模型会被告知它是赢得了一些分数还是失去了一些分数。语言模型并不完全像上一节中讨论的那样进行前瞻，但在某种意义上它已经被训练来预测哪些单词会获得好评。大语言模型仍然没有明确的目标，但它有一个“获得点赞”的隐含目标（或者我们也可以说它有“让普通人满意”的隐含目标），并且已经学会了关联对某些提示的某些响应并获得竖起大拇指。这具有很多规划特性，但没有明确的前瞻机制。更像是它记住了在很多情况下都有效的获得奖励的策略。&lt;/p&gt;
&lt;p&gt;重要的是 RLHF 是否使 ChatGPT 更加智能……它使 ChatGPT 更有可能产生我们希望看到的响应类型。它看起来更聪明，因为它的输出似乎传达了一种感觉，即它理解我们输入的意图并有自己的响应意图。这是一种错觉，因为它仍然只是对单词进行编码和解码。但话又说回来，这就是我们开始这篇文章的地方。&lt;/p&gt;
&lt;p&gt;指令调整和 RLHF 还使 ChatGPT 能够抵抗某些类型的滥用，例如生成种族主义、性别歧视或带有政治色彩的内容。它仍然可以做到，而且无论如何旧版本的 GPT-3 始终能够做到这一点。然而，作为一项免费的面向公众的服务，ChatGPT 针对某些类型的滥用行为产生的摩擦传达了一种安全感。它还拒绝将意见作为事实提供，这也消除了对用户的一种潜在伤害。&lt;/p&gt;
&lt;p&gt;[使用强化学习来修改预先训练的语言模型并不新鲜。它至少可以追溯到 2016 年，并已被用于使大型语言模型更加安全。大多数基于强化学习的大型语言模型调整都使用第二个模型来提供奖励，这也是通过 ChatGPT 完成的。ChatGPT 值得注意的是通过强化学习调整的系统规模，以及大规模的人类反馈收集工作。&lt;/p&gt;
&lt;h2 id=&#34;10-结论&#34;&gt;10. 结论&lt;/h2&gt;
&lt;p&gt;当手绘神经网络时，它看起来像鲸须。不管怎样，希望能够过滤掉一些围绕大型语言模型的炒作。&lt;/p&gt;
&lt;p&gt;LLM 可视化过程： &lt;a href=&#34;https://bbycroft.net/llm&#34;&gt;https://bbycroft.net/llm&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&#34;&gt;https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&#34;&gt;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bbycroft.net/llm&#34;&gt;https://bbycroft.net/llm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://time.geekbang.org/column/article/682762&#34;&gt;https://time.geekbang.org/column/article/682762&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/owgDAUGnrsXmNwUXY2Ya0w&#34;&gt;https://mp.weixin.qq.com/s/owgDAUGnrsXmNwUXY2Ya0w&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/PIh2gPhqF8r-9k8QZQ8GEw&#34;&gt;https://mp.weixin.qq.com/s/PIh2gPhqF8r-9k8QZQ8GEw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nvidia.github.io/TensorRT-LLM/&#34;&gt;https://nvidia.github.io/TensorRT-LLM/&lt;/a&gt;   &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;https://github.com/NVIDIA/TensorRT-LLM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://transformers.run/&#34;&gt;https://transformers.run/&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：使用大型语言模型进行开发所需了解的知识</title>
      <link>https://weedge.github.io/post/llm/all-you-need-to-know-to-develop-using-large-language-models/</link>
      <pubDate>Sun, 03 Dec 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/llm/all-you-need-to-know-to-develop-using-large-language-models/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/0.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;导读&#34;&gt;导读&lt;/h2&gt;
&lt;p&gt;本文的目的是简单地解释开始开发基于 LLM 的应用程序所需的关键技术。它面向对机器学习概念有基本了解并希望深入研究的软件开发人员、数据科学家和&lt;strong&gt;人工智能爱好者&lt;/strong&gt;。本文还提供了许多有用的链接以供进一步研究。这会很有趣！&lt;/p&gt;
&lt;p&gt;注：本文可以作为一个索引目录(进一步阅读资料深入学习)，从整体上了解下，毕竟现在LLM发展很快，可以发散或者focus某个领域；大部分LLM相关开源实现，可以手动demo下过程，至于炼丹了解过程即可，主要在场景上结合工程去利用好大力神丸在生产环境落地；还有就是应用场景，国内app应该可以复刻，如果模型和数据有了，缺个落地idea的话~&lt;/p&gt;
&lt;h2 id=&#34;1-大型语言模型llm简介&#34;&gt;1. 大型语言模型（LLM）简介&lt;/h2&gt;
&lt;p&gt;我想你已经听过一千遍什么是大语言模型，所以不会让你负担过重。需要知道的是：大型语言模型（LLM）是一种大型神经网络模型，它根据先前预测的标记来预测下一个标记。就这样。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;模型参数数量的比较。看看 GPT-3 有多大就知道了。没有人知道 GPT-4……&lt;/p&gt;
&lt;p&gt;大语言模型的受欢迎程度归因于其多功能性和有效性。它们完美地应对翻译、摘要、意义分析等任务。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;大语言模型能力&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;使用大语言模型的一些项目示例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.notion.so/product/ai&#34;&gt;&lt;strong&gt;Notion AI&lt;/strong&gt;&lt;/a&gt; — 帮助提高写作质量、生成内容、纠正拼写和语法、编辑语音和语调、翻译等。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/features/copilot&#34;&gt;&lt;strong&gt;GitHub Copilot&lt;/strong&gt;&lt;/a&gt; — 通过提供自动完成风格的建议来改进您的代码。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.dropbox.com/topics/product/introducing-AI-powered-tools&#34;&gt;&lt;strong&gt;Dropbox Dash&lt;/strong&gt;&lt;/a&gt; — 提供自然语言搜索功能，并且还专门引用了答案源自哪些文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果想详细了解大语言模型的工作原理，建议您阅读优秀的文章“&lt;a href=&#34;https://medium.com/@mark-riedl/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&#34;&gt;对大型语言模型的非常温和的介绍，无需炒作&lt;/a&gt;”&lt;/p&gt;
&lt;h2 id=&#34;2-开源与闭源模型&#34;&gt;2. 开源与闭源模型&lt;/h2&gt;
&lt;p&gt;主要差异：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;隐私&lt;/strong&gt;——大公司选择自托管解决方案的最重要原因之一。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;快速原型制作&lt;/strong&gt;——非常适合小型初创公司快速测试他们的想法，而无需过多的支出。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成质量&lt;/strong&gt;——可以针对特定任务微调模型，也可以使用付费 API。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于什么是好什么是坏，没有明确的答案。强调了以下几点：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;如果有兴趣深入研究细节，建议阅读之前的文章“&lt;a href=&#34;https://medium.com/better-programming/you-dont-need-hosted-llms-do-you-1160b2520526&#34;&gt;您不需要托管大语言模型，是吗？&lt;/a&gt;”。探索&lt;a href=&#34;https://www.promptingguide.ai/models/collection&#34;&gt;大语言模型系列&lt;/a&gt;以查看所有模型。&lt;/p&gt;
&lt;h2 id=&#34;3-提示词工程的艺术&#34;&gt;3. 提示词工程的艺术&lt;/h2&gt;
&lt;p&gt;许多人认为这是伪科学或只是暂时的炒作。但事实是，我们仍然不完全了解大语言模型是如何运作的。为什么他们有时会提供高质量的答复，有时会捏造事实（&lt;a href=&#34;https://medium.com/better-programming/fixing-hallucinations-in-llms-9ff0fd438e33&#34;&gt;产生幻觉&lt;/a&gt;）？或者为什么在提示中添加“让我们逐步思考”会突然提高质量？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;添加情感色彩可以提高任何模型的质量。&lt;a href=&#34;https://arxiv.org/pdf/2307.11760.pdf&#34;&gt;来源&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;由于这一切，科学家和爱好者只能尝试不同的提示，试图让模型表现得更好。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;说明大语言模型解决问题的各种方法的示意图&lt;/p&gt;
&lt;p&gt;我不会用复杂的提示链让你感到厌烦；相反，我只给出一些可以立即提高性能的示例：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.11916.pdf&#34;&gt;&amp;ldquo;让我们一步一步思考&amp;rdquo;&lt;/a&gt;  对于推理或逻辑任务非常有效。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2309.03409.pdf&#34;&gt;&amp;ldquo;深吸一口气，一步步解决这个问题&amp;rdquo;&lt;/a&gt;  上一点的改进版本。它可以增加几个百分点的质量。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.11760.pdf&#34;&gt;&amp;ldquo;这对我的职业生涯非常重要&amp;rdquo;&lt;/a&gt; 只需将其添加到提示的末尾，发现质量提高了 5-20%。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另外，我会立即分享一个有用的提示模板：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;让我们结合我们的&lt;strong&gt;X&lt;/strong&gt;命令和清晰的思维，以循序渐进的方式快速准确地破译答案。提供详细信息并在答案中包含来源。这对我的职业生涯非常重要。&lt;/p&gt;
&lt;p&gt;其中&lt;strong&gt;X&lt;/strong&gt;是您正在解决的任务的行业，例如编程。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我强烈建议您花几个晚上探索快速的工程技术。这不仅可以让您更好地控制模型的行为，还有助于提高质量并减少幻觉。为此，我建议阅读&lt;a href=&#34;https://www.promptingguide.ai/introduction/basics&#34;&gt;《提示词工程指南》&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;有用的链接&#34;&gt;有用的链接：&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hegelai/prompttools&#34;&gt;Prompttools&lt;/a&gt; — 快速测试和实验，支持两种 LLM（例如 OpenAI、LLaMA）。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/promptfoo/promptfoo&#34;&gt;Promptfoo&lt;/a&gt; — 测试和评估 LLM 输出质量。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/f/awesome-chatgpt-prompts&#34;&gt;Awesome ChatGPT Prompts&lt;/a&gt; — 用于 ChatGPT 模型的提示示例集合。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4合并新数据检索增强生成rag&#34;&gt;4.合并新数据：检索增强生成（RAG）&lt;/h2&gt;
&lt;p&gt;RAG是一种将LLM与外部知识库相结合的技术。这允许模型将原始训练集中未包含的相关信息或特定数据添加到模型中。&lt;/p&gt;
&lt;p&gt;尽管这个名字令人生畏（有时我们会在其中添加“reranker”一词），但它实际上是一种相当古老且出奇简单的技术：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/6.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;RAG 工作原理示意图&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将文档转换为数字，称之为&lt;a href=&#34;https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526&#34;&gt;&lt;strong&gt;嵌入(embedding)&lt;/strong&gt;&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;然后，使用相同的模型将用户的搜索查询转换为嵌入。&lt;/li&gt;
&lt;li&gt;查找前 K 个最接近的文档，通常基于&lt;a href=&#34;https://en.wikipedia.org/wiki/Cosine_similarity&#34;&gt;余弦相似度&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;要求大语言模型根据这些文件生成回复。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;何时使用&#34;&gt;何时使用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;对当前信息的需求&lt;/strong&gt;：当应用程序需要不断更新的信息（例如新闻文章）时。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特定领域的应用程序&lt;/strong&gt;：适用于需要大语言模型培训数据之外的专业知识的应用程序。例如，公司内部文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;何时不使用&#34;&gt;何时不使用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;通用会话应用程序&lt;/strong&gt;：信息需要通用且不需要附加数据的地方。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;有限的资源场景&lt;/strong&gt;： RAG 的检索组件涉及搜索大型知识库，这可能在计算上昂贵且缓慢，但仍然比微调更快且成本更低。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;使用-rag-构建应用程序&#34;&gt;使用 RAG 构建应用程序&lt;/h3&gt;
&lt;p&gt;一个很好的起点是使用&lt;a href=&#34;https://github.com/run-llama/llama_index&#34;&gt;LlamaIndex 库&lt;/a&gt;。它允许您快速将数据连接到大语言模型。为此，只需要几行代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;llama_index&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;VectorStoreIndex&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SimpleDirectoryReader&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 1. Load your documents:&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;documents&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SimpleDirectoryReader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;YOUR_DATA&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 2. Convert them to vectors:&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;VectorStoreIndex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_documents&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;documents&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 3. Ask the question:&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;query_engine&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;as_query_engine&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query_engine&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;When&amp;#39;s my boss&amp;#39;s birthday?&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在实际应用中，事情明显更加复杂。就像在任何开发中一样，您会遇到许多细微差别。例如，检索到的文档可能并不总是与问题相关，或者可能存在速度问题。然而，即使在这个阶段，也可以显着提高搜索系统的质量。&lt;/p&gt;
&lt;h3 id=&#34;进一步阅读&#34;&gt;进一步阅读&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1&#34;&gt;构建基于 RAG 的 LLM 应用程序用于生产&lt;/a&gt; 一篇关于 RAG 主要组件的优秀详细文章。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/why-your-rag-is-not-reliable-in-a-production-environment-9e6a73b3eddb&#34;&gt;为什么您的 RAG 在生产环境中不可靠&lt;/a&gt; 一篇很棒的文章，它以清晰的语言解释了使用 RAG 时可能出现的困难。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://betterprogramming.pub/7-query-strategies-for-navigating-knowledge-graphs-with-llamaindex-ed551863d416&#34;&gt;使用 LlamaIndex 导航知识图的 7 种查询策略&lt;/a&gt; 一篇内容丰富的文章，详细而细致地了解了使用 LlamaIndex 构建 RAG 管道。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/assistants/tools/knowledge-retrieval&#34;&gt;OpenAI 检索工具 &lt;/a&gt;如果想要最少的工作，使用 OpenAI 的 RAG。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;5-微调大语言模型&#34;&gt;5. 微调大语言模型&lt;/h2&gt;
&lt;p&gt;微调是在特定数据集上继续训练预训练的 LLM 的过程。可能会问，如果已经可以使用 RAG 添加数据，为什么还需要进一步训练模型。简单的答案是，只有微调才能定制模型以理解特定领域或定义其风格。例如，&lt;a href=&#34;https://medium.com/better-programming/unleash-your-digital-twin-how-fine-tuning-llm-can-create-your-perfect-doppelganger-b5913e7dda2e&#34;&gt;通过对个人信件进行微调来创建自己的副本&lt;/a&gt;：&lt;/p&gt;
&lt;p&gt;如果已经相信了它的重要性，那么看看它是如何工作的：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/7.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;对特定领域数据进行微调的经典方法（所有图标均来自&lt;a href=&#34;http://flaticon.com/&#34;&gt;flaticon&lt;/a&gt;）&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;接受训练有素的大语言模型，有时称为基础大语言模型。可以从&lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34;&gt;HuggingFace&lt;/a&gt;下载它们。&lt;/li&gt;
&lt;li&gt;准备您的训练数据。只需编写说明和响应即可。这是此类数据集的&lt;a href=&#34;https://huggingface.co/datasets/databricks/databricks-dolly-15k&#34;&gt;示例&lt;/a&gt;；还可以使用 GPT-4&lt;a href=&#34;https://www.promptingguide.ai/applications/generating&#34;&gt;生成合成数据&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;选择合适的微调方法。目前比较流行的是&lt;a href=&#34;https://github.com/microsoft/LoRA&#34;&gt;LoRA&lt;/a&gt;和&lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;根据新数据微调模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;何时使用-1&#34;&gt;何时使用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;垂直领域应用程序&lt;/strong&gt;：当应用程序处理专门或非常规主题时。例如，法律文档应用需要理解和处理法律术语。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自定义语言风格&lt;/strong&gt;：适用于需要特定语气或风格的应用程序。例如，创建一个&lt;a href=&#34;https://beta.character.ai/&#34;&gt;人工智能角色&lt;/a&gt;，无论是名人还是书中的角色。（衍生场景：旅游景点数字人，游戏NPC，二次元社区）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;何时不使用-1&#34;&gt;何时不使用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;广泛的应用&lt;/strong&gt;：应用范围是一般性的，不需要专业知识。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据有限&lt;/strong&gt;：微调需要大量相关数据。但是，始终可以&lt;a href=&#34;https://www.confident-ai.com/blog/how-to-generate-synthetic-data-using-llms-part-1&#34;&gt;使用另一个 LLM 生成它们&lt;/a&gt;。例如，今年早些时候，使用包含 52k LLM 生成的指令响应对的&lt;a href=&#34;https://github.com/gururise/AlpacaDataCleaned&#34;&gt;Alpaca 数据集&lt;/a&gt;创建了第一个微调&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;Llama v1模型&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;微调llm&#34;&gt;微调LLM&lt;/h3&gt;
&lt;p&gt;可以找到大量致力于模型微调的文章。仅在 Medium 上就有数千个。因此，不想太深入地研究这个主题，而是展示一个高级库&lt;a href=&#34;https://github.com/Lightning-AI/lit-gpt&#34;&gt;Lit-GPT&lt;/a&gt;，它隐藏了所有的魔力。是的，它不允许对训练过程进行太多定制，但可以快速进行实验并获得初步结果。只需要几行代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 1. Download the model:&lt;/span&gt;
python scripts/download.py --repo_id meta-llama/Llama-2-7b

&lt;span class=&#34;c1&#34;&gt;# 2. Convert the checkpoint to the lit-gpt format:&lt;/span&gt;
python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/llama

&lt;span class=&#34;c1&#34;&gt;# 3. Generate an instruction tuning dataset:&lt;/span&gt;
python scripts/prepare_alpaca.py  &lt;span class=&#34;c1&#34;&gt;# it should be your dataset&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 4. Run the finetuning script&lt;/span&gt;
python finetune/lora.py &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --checkpoint_dir checkpoints/llama/
    --data_dir your_data_folder/
    --out_dir my_finetuned_model/ 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;就是这样！训练过程将开始：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/8.gif&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;请注意，该过程可能需要很长时间。在单个 A100 GPU 上微调 Falcon-7B需要大约&lt;strong&gt;10 小时&lt;/strong&gt;和&lt;strong&gt;30 GB内存。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当然，我有点过于简单化了，我们只触及了表面。实际上，微调过程要复杂得多，为了获得更好的结果，需要了解各种适配器及其参数等等。然而，即使经过如此简单的迭代，会得到一个遵循特定指示的新模型。&lt;/p&gt;
&lt;h3 id=&#34;进一步阅读-1&#34;&gt;进一步阅读&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/better-programming/unleash-your-digital-twin-how-fine-tuning-llm-can-create-your-perfect-doppelganger-b5913e7dda2e&#34;&gt;使用微调的大语言模型创建自己的克隆&lt;/a&gt; 作者在文章中写了有关收集数据集、使用参数的文章，并提供了有关微调的有用技巧。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lightning.ai/pages/community/article/understanding-llama-adapters/&#34;&gt;了解大型语言模型的参数高效微调&lt;/a&gt; 如果想了解微调概念和流行的参数高效替代方案的详细信息，这是一个很好的教程。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lightning.ai/pages/community/lora-insights/&#34;&gt;使用 LoRA 和 QLoRA 微调大语言模型：数百次实验的见解 &lt;/a&gt;了解 LoRA 功能的文章之一。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/fine-tuning&#34;&gt;OpenAI 微调 &lt;/a&gt;如果想以最小的工作微调 GPT-3.5。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;6-部署-llm-工程化&#34;&gt;6. 部署 LLM (工程化)&lt;/h2&gt;
&lt;p&gt;有时，如果有用于推理的计算资源和模型,数据存储资源，想直接利用训练好的开源大模型，仅仅简单地按下“部署”按钮&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/9.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;幸运的是，这是相当可行的。有大量专门用于部署大型语言模型的框架。是什么让他们如此出色？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;许多预构建的包装器和集成。&lt;/li&gt;
&lt;li&gt;大量可用型号可供选择。&lt;/li&gt;
&lt;li&gt;大量内部优化。&lt;/li&gt;
&lt;li&gt;快速原型制作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;选择正确的框架&#34;&gt;选择正确的框架&lt;/h3&gt;
&lt;p&gt;部署LLM应用程序的框架的选择取决于多种因素，包括模型的大小、应用程序的可扩展性要求和部署环境。目前，框架的多样性并不丰富，因此理解它们的差异应该不会太困难。下面，准备了一份备忘单，可以快速入门：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/10.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;此外，在文章“&lt;a href=&#34;https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407&#34;&gt;为大语言模型提供服务的 7 个框架&lt;/a&gt;”中，对现有解决方案进行了更详细的概述。如果打算部署模型，建议检查一下。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/11.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;LLM 推理框架比较&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;部署示例代码&#34;&gt;部署示例代码&lt;/h3&gt;
&lt;p&gt;从理论转向实践，并尝试使用&lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;文本生成推理(Text Generation Inference)&lt;/a&gt;来部署 LLaMA-2 。通过运行已经打包好的模型服务镜像，demo只需要几行代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 1. Create a folder where your model will be stored:&lt;/span&gt;
mkdir data

&lt;span class=&#34;c1&#34;&gt;# 2. Run Docker container (launch RestAPI service):&lt;/span&gt;
docker run --gpus all --shm-size 1g -p 8080:80 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    -v &lt;span class=&#34;nv&#34;&gt;$volume&lt;/span&gt;:/data &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    ghcr.io/huggingface/text-generation-inference:1.1.0
    --model-id meta-llama/Llama-2-7b

&lt;span class=&#34;c1&#34;&gt;# 3. And now you can make requests:&lt;/span&gt;
curl 127.0.0.1:8080/generate &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    -X POST &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{&amp;#34;inputs&amp;#34;:&amp;#34;Tell me a joke!&amp;#34;,&amp;#34;parameters&amp;#34;:{&amp;#34;max_new_tokens&amp;#34;:20}}&amp;#39;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    -H &lt;span class=&#34;s1&#34;&gt;&amp;#39;Content-Type: application/json&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;设置了带有内置日志记录的 RestAPI 服务、用于监控的 Prometheus 端点、令牌流，并且模型已完全优化。这不是很神奇吗？这些工程化实现方案相对成熟，移动互联网时代的工程累积。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weedge/mypic/master/llm/all-you-need-to-know-to-develop-using-large-language-models/12.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;API文档&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;进一步阅读-2&#34;&gt;进一步阅读&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407&#34;&gt;为大语言模型提供服务的 7 个框架&lt;/a&gt; 大语言模型推理和服务的综合指南，并进行了详细比较。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/inference-endpoints&#34;&gt;Inference Endpoints&lt;/a&gt; HuggingFace 的一款产品，只需点击几下即可部署任何 LLM。当需要快速原型设计时，这是一个不错的选择。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;7-幕后还剩下什么&#34;&gt;7. 幕后还剩下什么&lt;/h2&gt;
&lt;p&gt;尽管我们已经介绍了开发基于 LLM 的应用程序所需的主要概念，但将来可能会遇到一些问题。所以，想留下一些有用的链接：&lt;/p&gt;
&lt;h3 id=&#34;优化&#34;&gt;优化&lt;/h3&gt;
&lt;p&gt;当启动第一个模型时，不可避免地会发现它没有想要的那么快并且消耗了大量资源。需要了解如何对其进行优化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/better-programming/speed-up-llm-inference-83653aa24c47&#34;&gt;加速托管 LLM 推理的 7 种方法 &lt;/a&gt;加速 LLM 推理的技术，以提高令牌生成速度并减少内存消耗。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/&#34;&gt;优化 PyTorch 中训练 LLM 的内存使用&lt;/a&gt; 文章提供了一系列技术，可以将 PyTorch 中的内存消耗减少约 20 倍，而不会牺牲建模性能和预测准确性。&lt;/p&gt;
&lt;p&gt;补充：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/owgDAUGnrsXmNwUXY2Ya0w&#34;&gt;飞桨大模型分布式训练技术&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/PIh2gPhqF8r-9k8QZQ8GEw&#34;&gt;⻜桨⼤模型推理部署⾼性能优化&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;评估&#34;&gt;评估&lt;/h3&gt;
&lt;p&gt;假设有一个经过微调的模型。但怎么能确定它的质量已经提高了呢？应该使用什么指标？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://explodinggradients.com/all-about-evaluating-large-language-models&#34;&gt;所有关于评估大型语言模型的内容 &lt;/a&gt;一篇关于基准和指标的很好的概述文章。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openai/evals&#34;&gt;evals&lt;/a&gt; 用于评估大语言模型和大语言模型系统的最流行框架。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;向量数据库&#34;&gt;向量数据库&lt;/h3&gt;
&lt;p&gt;如果使用 RAG，在某些时候，将向量存储在内存中转移到数据库中。为此，了解当前市场上的产品及其局限性非常重要。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb&#34;&gt;All You Need to Know about Vector Databases&lt;/a&gt; 发现并利用向量数据库的力量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://benchmark.vectorview.ai/vectordbs.html&#34;&gt;选择向量数据库：2023 年的比较和指南 &lt;/a&gt; Pinecone、Weviate、Milvus、Qdrant、Chroma、Elasticsearch 和 PGvector 数据库的比较。&lt;/p&gt;
&lt;p&gt;补充：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/-io_q8WCdAxfSCY9qlrgCw&#34;&gt;向量检索在大模型应用场景的技术和实践&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注： 尽管大模型提高了输入token数量(长文本)，但是对向量数据库的影响不大，特别是多模场景混合搜索的场景。&lt;/p&gt;
&lt;h3 id=&#34;大语言模型agents&#34;&gt;大语言模型Agents&lt;/h3&gt;
&lt;p&gt;在我看来，大语言模型最有前途的发展。如果希望多个模型协同工作，建议浏览以下链接：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/paitesanshi/llm-agent-survey#-more-comprehensive-summarization&#34;&gt;基于 LLM 的自治agents的调查 &lt;/a&gt;这可能是基于 LLM 的agents最全面的概述。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/autogen&#34;&gt;autogen&lt;/a&gt;  是一个框架，允许使用多个agents来开发 LLM 应用程序，这些agents可以相互对话来解决任务。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/xlang-ai/OpenAgents&#34;&gt;OpenAgents&lt;/a&gt;  一个用于在使用和托管语言agents的开放平台。&lt;/p&gt;
&lt;p&gt;补充：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/PL-QjlvVugUfmRD4g0P-qQ&#34;&gt;&lt;strong&gt;从第一性原理看大模型Agent技术&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;存储-补充&#34;&gt;存储 (补充)&lt;/h3&gt;
&lt;p&gt;注： 这块主要是聚焦用于训练模型的数据，以及训练好的模型数据，相关链接：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/c4MWpBuYK0b1DDufeDV1vg&#34;&gt;面向大模型的存储加速方案设计和实践&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;根据人类反馈进行强化学习-rlhf&#34;&gt;根据人类反馈进行强化学习 (RLHF)&lt;/h3&gt;
&lt;p&gt;一旦允许用户访问模型，如果对方反应粗鲁怎么办？或者揭示制造炸弹的成分？为了避免这种情况，请查看这些文章：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/rlhf&#34;&gt;&lt;strong&gt;介绍 根据人类反馈进行强化学习(RLHF)&lt;/strong&gt;&lt;/a&gt;  一篇详细介绍 RLHF 技术的概述文章。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/allenai/RL4LMs&#34;&gt;RL4LMs&lt;/a&gt;  一个模块化 RL 库，用于根据人类偏好微调语言模型。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/trl&#34;&gt;TRL&lt;/a&gt;  一组使用强化学习训练 Transformer 语言模型的工具，从监督微调步骤 (SFT)、奖励建模步骤 (RM) 到近端策略优化 (PPO) 步骤。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;尽管我们都有点厌倦了炒作，但大语言模型将陪伴我们很长一段时间，并且理解他们的堆栈和编写简单应用程序的能力可以给你带来显着的提升。希望本文已经成功地让您稍微沉浸在这个领域，并向您展示它没有什么复杂或可怕的。&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/all-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&#34;&gt;https://towardsdatascience.com/all-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Large_language_model&#34;&gt;https://en.wikipedia.org/wiki/Large_language_model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bbycroft.net/llm&#34;&gt;https://bbycroft.net/llm&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：更快的字符串转整数</title>
      <link>https://weedge.github.io/post/simd/faster_integer_parsing/</link>
      <pubDate>Thu, 30 Nov 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/simd/faster_integer_parsing/</guid>
      
        <description>&lt;p&gt;​                        &lt;img src=&#34;https://github.com/weedge/mypic/raw/master/simd/faster_integer_parsing/0.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;导读&#34;&gt;导读&lt;/h2&gt;
&lt;p&gt;字符串转换成整数，或者浮点类型数据，是在编程中经常遇到的问题，各种语言的标准库中会有实现，本文通过一个常见问题场景，来研究优化如何使用cpu 硬件SIMD指令集，并结合编译器在 log(n) 时间内完成此类parse操作；由于最终的优化需要结合对应cpu arch的指令集，这里硬件平台cpu为Intel x86，整数类型以uint64_t为例，最大2^64-1 20个字符表示。目的：结合场景优化思路(以小见大)，熟悉下Intel cpu simd相关指令的使用。常见场景： &lt;a href=&#34;https://github.com/simdjson/simdjson&#34;&gt;simdjson&lt;/a&gt;  (PS: 不因过早优化，在对应场景下整体稳定性和优化成本/收益上折中)&lt;/p&gt;
&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;
&lt;p&gt;假设有一些基于网络(socket)的传输协议字符串或包含微秒时间戳的文件。需要尽快解析这些时间戳。也许是 json，也许是 csv 文件，也许是其他定制的文件。它有 16 个字符长(8*16)，这也适用于信用卡号码。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;timestamp,event_id
1585201087123567,a
1585201087123585,b
1585201087123621,c
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;实现类似这样的功能：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;parse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;//c++17
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;标准库中的方法&#34;&gt;标准库中的方法&lt;/h2&gt;
&lt;p&gt;在c/c++中可以调用标准库方法进行解析，比如 c中 &lt;code&gt;atoll&lt;/code&gt;相关函数； c++中 &lt;a href=&#34;https://en.cppreference.com/w/cpp/string/byte/atoi&#34;&gt;&lt;code&gt;std::atoll&lt;/code&gt;&lt;/a&gt;一个从 C 继承的函数； &lt;a href=&#34;https://en.cppreference.com/w/cpp/io/basic_stringstream&#34;&gt;&lt;code&gt;std::stringstream&lt;/code&gt;&lt;/a&gt; 流方式处理；标准C++17 引入的  &lt;a href=&#34;https://en.cppreference.com/w/cpp/header/charconv&#34;&gt;&lt;code&gt;&amp;lt;charconv&amp;gt;&lt;/code&gt;&lt;/a&gt;头文件中的方法；以及boost库 &lt;a href=&#34;https://www.boost.org/doc/libs/1_73_0/libs/spirit/doc/html/spirit/qi/reference/basics.html&#34;&gt;&lt;code&gt;boost::spirit::qi&lt;/code&gt;&lt;/a&gt;中的方法。比如 使用from_chars 方法&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;kr&#34;&gt;inline&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parse_char_conv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;noexcept&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;errc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{}&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// I have an error !
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;优化思路&#34;&gt;优化思路&lt;/h2&gt;
&lt;h3 id=&#34;常规线性遍历&#34;&gt;常规线性遍历&lt;/h3&gt;
&lt;p&gt;将展开的解决方案中的操作绘制为一棵树，以将“1234”解析为 32 位整数的简化示例为例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/simd/faster_integer_parsing/1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“1234”操作的展开解图&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;可以看到，乘法和加法的数量与字符数量成线性关系。很难看出如何改进这一点，因为每次乘法都是通过不同的因子（所以不能“一次性”相乘），并且在一天结束时需要将所有中间结果相加。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;kr&#34;&gt;inline&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parse_naive&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;noexcept&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;digit&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;digit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;kr&#34;&gt;inline&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parse_unrolled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;noexcept&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000000000000000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100000000000000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000000000000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000000000000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100000000000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000000000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000000000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100000000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;14&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10ULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h3 id=&#34;字节交换byteswap&#34;&gt;字节交换(byteswap)&lt;/h3&gt;
&lt;p&gt;然而，它仍然非常有规律。一方面，字符串中的第一个字符乘以最大的因子，因为它是最高有效的数字。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在小端机器（如 x86）上，整数的第一个字节包含最低有效数字，而字符串中的第一个字节包含最高有效数字。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/simd/faster_integer_parsing/2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;将字符串视为整数，可以通过更少的操作更接近最终的解析状态 - 十六进制表示(机器只识别是01)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;现在，要将字符串的字节重新解释为整数，需要使用 &lt;code&gt;std::memcpy&lt;/code&gt;（&lt;a href=&#34;https://blog.regehr.org/archives/1307&#34;&gt;以避免严格别名违规&lt;/a&gt;），并且编译器本身&lt;code&gt;__builtin_bswap64&lt;/code&gt;来交换一条指令中的字节。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;k&#34;&gt;template&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;typename&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;kr&#34;&gt;inline&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_zeros_string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;noexcept&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;template&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;kr&#34;&gt;inline&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_zeros_string&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;noexcept&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;constexpr&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;00000000&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;sizeof&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;// 64 = 8*8
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kr&#34;&gt;inline&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parse_8_chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;noexcept&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;sizeof&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;__builtin_bswap64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_zeros_string&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里使用了内置的64位swap进行反转&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;分而治之&#34;&gt;分而治之&lt;/h3&gt;
&lt;p&gt;从上一步中，最终得到一个整数，其位表示形式将每个数字放置在单独的字节中。即，尽管一个字节8位最多可以表示 256 个值(0~2^8-1)，但整数的每个字节中都有值 0-9。它们也采用正确的小端顺序。现在只需要以某种方式将它们“粉碎”在一起即可。&lt;/p&gt;
&lt;p&gt;知道线性执行会太慢，下一个可能性是什么？ &lt;strong&gt;O(log(n))&lt;/strong&gt;！需要一步将每个相邻数字组合成一对，然后将每对数字组合成四个一组，依此类推，直到得到整个整数。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.reddit.com/r/cpp/comments/gr18ig/faster_integer_parsing/frx9agb&#34;&gt;reddit 上的 Sopel97&lt;/a&gt; 指出 byteswap 不是必需的。无论哪种方式都可以组合相邻数字 - 它们的顺序并不重要。我意识到它有助于我获得下一个见解，但可以在最终代码中省略。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;关键是同时处理相邻的数字。这允许操作树在 O(log(n)) 时间内运行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这涉及将偶数索引数字乘以 10 的幂并保留奇数索引数字。这可以通过位掩码来选择性地应用操作来完成&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/simd/faster_integer_parsing/3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;通过使用位掩码，我们可以一次对多个数字应用运算，将它们组合成一个更大的组&lt;/p&gt;
&lt;p&gt;让&lt;code&gt;parse_8_chars&lt;/code&gt;通过使用这个掩码技巧来完成之前开始的函数。作为屏蔽的一个巧妙的副作用，不需要减去 &lt;code&gt;&#39;0&#39;&lt;/code&gt;，因为它会被屏蔽掉。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;kr&#34;&gt;inline&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parse_8_chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;noexcept&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;sizeof&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;

  &lt;span class=&#34;c1&#34;&gt;// 1-byte mask trick (works on 4 pairs of single digits)
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lower_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;mh&#34;&gt;0x0f000f000f000f00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;upper_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;mh&#34;&gt;0x000f000f000f000f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lower_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;upper_digits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

  &lt;span class=&#34;c1&#34;&gt;// 2-byte mask trick (works on 2 pairs of two digits)
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;lower_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;mh&#34;&gt;0x00ff000000ff0000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;upper_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;mh&#34;&gt;0x000000ff000000ff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lower_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;upper_digits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

  &lt;span class=&#34;c1&#34;&gt;// 4-byte mask trick (works on pair of four digits)
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;lower_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;mh&#34;&gt;0x0000ffff00000000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;upper_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;mh&#34;&gt;0x000000000000ffff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lower_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;upper_digits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h3 id=&#34;组合&#34;&gt;组合&lt;/h3&gt;
&lt;p&gt;把它们放在一起，为了解析 16 位整数，将它分成两个 8 字节的块，运行&lt;code&gt;parse_8_chars&lt;/code&gt;刚刚编写的代码，并对它进行基准测试！&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;kr&#34;&gt;inline&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parse_trick&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;noexcept&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;upper_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parse_8_chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lower_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parse_8_chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;upper_digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100000000&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lower_digits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;static&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;BM_trick&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;benchmark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;benchmark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DoNotOptimize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parse_trick&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;example_stringview&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;还不错，将展开循环(unrolled)&lt;a href=&#34;https://quick-bench.com/q/PJAjDeGoSS_OsTrSdtPq1alye34&#34;&gt;基准测试&lt;/a&gt;降低了近 50%左右；（注：这个和编译器优化相关，本文采用的是gcc 9.0 版本，高版本中unrolled版本指令有所优化，autosimd）尽管如此，感觉就像正在手动执行一堆屏蔽和元素操作。也许可以让 CPU SIMD指令集来进一优化，将指令存入更宽的寄存器，较少指令执行次数和执行周期。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;simd&#34;&gt;SIMD&lt;/h3&gt;
&lt;p&gt;有以下主要优化：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同时组合数字组以实现 O(log(n)) 时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;还有一个 16 个字符或 128 位字符串需要解析 - 可以使用 SIMD 吗？当然可以！&lt;a href=&#34;https://en.wikipedia.org/wiki/SIMD&#34;&gt;SIMD 代表单指令多数据，&lt;/a&gt;Intel 和 AMD CPU 均支持 SSE 和 AVX 指令，并且它们通常适用于更宽的寄存器(128bit i)。&lt;/p&gt;
&lt;p&gt;使用&lt;a href=&#34;https://software.intel.com/sites/landingpage/IntrinsicsGuide/&#34;&gt;Intel 内部函数指南&lt;/a&gt;来为正确的 SIMD CPU 指令找到正确的编译器内部函数。&lt;/p&gt;
&lt;p&gt;首先设置 16 个字节中每个字节的数字：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;kr&#34;&gt;inline&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parse_16_chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;noexcept&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;c1&#34;&gt;//__m128i  
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// This intrinsic may perform better than _mm_loadu_si128 when the data crosses a cache line boundary
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_lddqu_si128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;reinterpret_cast&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;__m128i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;_mm_set1_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  
  &lt;span class=&#34;c1&#34;&gt;// ...
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;现在，最引人注目的是&lt;code&gt;madd&lt;/code&gt;功能。这些 SIMD 函数的作用与使用位掩码技巧所做的完全一样 - 它们采用宽寄存器，将其解释为较小整数的向量，将每个乘数乘以给定的乘数，并将相邻的乘数加在一起形成更宽整数的向量。全部在一个指令中！&lt;/p&gt;
&lt;p&gt;作为获取每个字节，将奇数乘以 10 并将相邻对加在一起的示例，可以使用 &lt;a href=&#34;https://www.felixcloutier.com/x86/pmaddubsw&#34;&gt;&lt;code&gt;_mm_maddubs_epi16&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// The 1-byte &amp;#34;trick&amp;#34; in one instruction
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mult&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_set_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
  &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_maddubs_epi16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mult&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;还有另一条用于 2 字节技巧的指令&lt;code&gt;_mm_maddubs_epi16&lt;/code&gt;，但不幸的是找不到用于 4 字节技巧的指令&lt;code&gt;_mm_maddubs_epi32&lt;/code&gt;木有 - 这需要两条指令。这是完成的&lt;code&gt;parse_16_chars&lt;/code&gt;函数：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-C++&#34; data-lang=&#34;C++&#34;&gt;&lt;span class=&#34;kr&#34;&gt;inline&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;uint64_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parse_16_chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;noexcept&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_lddqu_si128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;reinterpret_cast&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;__m128i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;_mm_set1_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

  &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mult&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_set_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
      &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_maddubs_epi16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mult&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mult&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_set_epi16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_madd_epi16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mult&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_packus_epi32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mult&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_set_epi16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_madd_epi16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mult&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;mh&#34;&gt;0xffffffff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100000000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;0.75纳秒&lt;/strong&gt;！哇哦。&lt;/p&gt;
&lt;p&gt;在实际生产环境中，需要对输入验证或长度检查；例如如下naive代码：(加上__device__可在gpu kernel上运行)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;k&#34;&gt;static&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;h_atoi&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;isMinus&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;+&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;isMinus&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;9&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2147483647&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;\0&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;9&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;isMinus&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;?&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;本文介绍的方法适用于固定长度的整数场景。一方面，当知道整数很长时，这可以用作“快速路径”，而在其他情况下则可以回退到简单循环。其次，通过使用一些更聪明的 SIMD 指令，可以在 2 纳秒内运行一些东西(甚至更快)，并完成验证和长度检查。比如simdjson场景。&lt;/p&gt;
&lt;h2 id=&#34;基准测试&#34;&gt;基准测试&lt;/h2&gt;
&lt;p&gt;使用&lt;a href=&#34;https://github.com/google/benchmark&#34;&gt;Google Benchmark&lt;/a&gt;来衡量性能，并获得基线，与将最终结果直接加载到寄存器中进行比较 - 即不涉及实际解析。&lt;/p&gt;
&lt;p&gt;运行基准测试！代码在这里并不重要，它只是显示正在进行基准测试的内容。&lt;/p&gt;
&lt;p&gt;最终结果： &lt;a href=&#34;https://quick-bench.com/q/NlmsLut8ol_JGwurPfKG22n2Mbs&#34;&gt;https://quick-bench.com/q/NlmsLut8ol_JGwurPfKG22n2Mbs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;注： boost库不是标准库，未显示在结果中，可以在本机上运行基准测试。&lt;/p&gt;
&lt;h1 id=&#34;使用-avx-512-快速解析整数&#34;&gt;使用 AVX-512 快速解析整数&lt;/h1&gt;
&lt;p&gt;最近的英特尔处理器有新的指令 AVX-512，它可以一次处理多个字节并进行屏蔽，以便您可以仅选择一系列数据。&lt;/p&gt;
&lt;p&gt;我假设您知道数字序列的开头和结尾。以下带有 AVX-512 内在函数的代码执行以下操作：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算以字节为单位的跨度 (digit_count)，&lt;/li&gt;
&lt;li&gt;如果有超过 20 个字节，就知道该整数太大，无法容纳 64 位整数，&lt;/li&gt;
&lt;li&gt;计算一个“掩码”：一个 32 位值，只有最高有效的 digital_count 位设置为 1，&lt;/li&gt;
&lt;li&gt;将 ASCII 或 UTF-8 字符串加载到 256 位寄存器中，&lt;/li&gt;
&lt;li&gt;减去字符值“0”以获得 0 到 9 之间的值（数字值），&lt;/li&gt;
&lt;li&gt;检查某个值是否超过 9，在这种情况下有一个非数字字符。&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;n&#34;&gt;size_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;digit_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;end&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;// if (digit_count &amp;gt; 20) { error ....}
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;simd8x32&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ASCII_ZERO&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm256_set1_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sc&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;simd8x32&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NINE&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm256_set1_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;kt&#34;&gt;uint32_t&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;uint32_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mh&#34;&gt;0xFFFFFFFF&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm256_maskz_loadu_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;base10_8bit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm256_maskz_sub_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;in&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ASCII_ZERO&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nondigits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm256_mask_cmpgt_epu8_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;base10_8bit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NINE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nondigits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;// there is a non-digit
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这是使用 AVX-512 功能的关键步骤。之后，对于熟悉传统 x64 处理器上的高级 Intel 内在函数的人来说，可以使用“老式”处理……大多数情况下，只需乘以 10、乘以 100、乘以 100000 即可创建四个 32 位值：第一个对应于最低有效的 8 个 ASCII 字节，第二个到下一个最高有效的 8 个 ASCII 字节，以及最多 4 个最高有效字节。当数字为 8 位或更少时，只有其中一个单词相关；当数字为 16 位或更少时，前两个单词有意义。总是浪费一个由零组成的 32 位值。代码如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DIGIT_VALUE_BASE10_8BIT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm256_set_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                               &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                               &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                               &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DIGIT_VALUE_BASE10E2_8BIT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_set_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DIGIT_VALUE_BASE10E4_16BIT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_set_epi16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;base10e2_16bit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm256_maddubs_epi16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;base10_8bit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DIGIT_VALUE_BASE10_8BIT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;base10e2_8bit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm256_cvtepi16_epi8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;base10e2_16bit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;base10e4_16bit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_maddubs_epi16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;base10e2_8bit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DIGIT_VALUE_BASE10E2_8BIT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;base10e8_32bit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_mm_madd_epi16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;base10e4_16bit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DIGIT_VALUE_BASE10E4_16BIT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/tree/master/2023/09/22&#34;&gt;c++代码实现&lt;/a&gt;，并使用GCC12编译。在 Ice Lake 服务器上运行基准测试。使用随机 32 位整数进行测试。AVX-512 的速度是标准方法&lt;code&gt;std::from_chars&lt;/code&gt;的两倍多。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;AVX-512&lt;/th&gt;
&lt;th&gt;1.8GB/秒&lt;/th&gt;
&lt;th&gt;57 指令/数量&lt;/th&gt;
&lt;th&gt;17 周期/次数&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;std::from_chars&lt;/td&gt;
&lt;td&gt;0.8GB/秒&lt;/td&gt;
&lt;td&gt;128条指令/数量&lt;/td&gt;
&lt;td&gt;39 周期/次数&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;目前的比较并不完全公平，因为 AVX-512 函数假设它知道数字序列的开头和结尾。&lt;/p&gt;
&lt;p&gt;假设正在循环内按顺序解析数字，可以通过使用内联函数来提高性能，使其达到 2.3 GB/s，性能提升 30%。&lt;/p&gt;
&lt;p&gt;原始代码将返回奇特的 std::Optional 值，但 GCC 受到负面影响，因此我将函数签名更改为更常规。甚至，在我的测试中，与 GCC 相比，LLVM/clang 稍微快一些。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这里没有进行传统的解析，这涉及从左到右（最重要到最不重要）的解析。使这项工作有效的原因是“右对齐”SIMD 寄存器中的数字（将最低有效数字放入 SIMD 寄存器的最高有效字节中）。&lt;/p&gt;
&lt;p&gt;这与传统的从左到右的数字解析器形成对比：通过知道数字的最低有效数字在哪里，我们确切地知道每个数字的价值（最右边的是数字 10^0，倒数第二个数字是数字 10^1，等等） ……）。我们不再需要执行传统的“读取下一个数字，将总数乘以 10，读取下一个数字并将其添加”，这会产生数字之间的数据依赖性。&lt;/p&gt;
&lt;p&gt;因为支持最多 20 个字符数字(uint64_t max 2^64-1)，20*8=160位，所以从支持 32 字节寄存器的SIMD指令集开始（尽管在步骤 2 中很快缩小到 16 字节寄存器，因为 2 位数字仍然可以容纳在一个字节中）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;String: 1234567890
SIMD step 1 (8-bit x 32): 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3 4 5 6 7 8 9 0
SIMD step 2 (8-bit x 16): 00 00 00 00 00 00 00 00 00 00 00 12 34 56 78 90
SIMD step 3 (16-bit x 8): 0000 0000 0000 0000 0000 0012 3456 7890
SIMD step 4 (32-bit x 4): 00000000 00000000 00000012 34567890
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后，提取 32 位数字，将它们乘以 10^16、10^8 和 10^0，然后将它们相加以获得 64 位结果（在本例中为 1234567890）。&lt;/p&gt;
&lt;p&gt;值得注意的一点是：使用 AVX-512，还可以同时解析 2 个数字（每个数字字符串可由32字节(256位)寄存器存放处理），而无需修改算法。（如果知道数字字符串都是 8 字节(64位)或更少，可以解析更多！）如果想要 32 位数字，您可以一次解析 4 个！16位，8个！&lt;/p&gt;
&lt;p&gt;请注意，如果加载 32 个字节（假设填充，对齐操作），查找非数字，并使用它来查找末尾，然后使用“右对齐”所有数字，则可以在不知道数字的完整大小的情况下执行此操作字节移位/洗牌(shift/shuffle)。&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://kholdstare.github.io/technical/2020/05/26/faster-integer-parsing.html&#34;&gt;https://kholdstare.github.io/technical/2020/05/26/faster-integer-parsing.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/cpp/comments/gr18ig/faster_integer_parsing/&#34;&gt;https://www.reddit.com/r/cpp/comments/gr18ig/faster_integer_parsing/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://quick-bench.com/q/-E78g-dkbnDvKlGVkgZc0owGrn0&#34;&gt;https://quick-bench.com/q/-E78g-dkbnDvKlGVkgZc0owGrn0&lt;/a&gt;  &lt;a href=&#34;https://godbolt.org/z/czvqh6v1a&#34;&gt;https://godbolt.org/z/czvqh6v1a&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://0x80.pl/articles/simd-parsing-int-sequences.html&#34;&gt;http://0x80.pl/articles/simd-parsing-int-sequences.html&lt;/a&gt;&lt;/strong&gt; &lt;a href=&#34;https://github.com/WojciechMula/parsing-int-series&#34;&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lemire.me/blog/2023/09/22/parsing-integers-quickly-with-avx-512/&#34;&gt;https://lemire.me/blog/2023/09/22/parsing-integers-quickly-with-avx-512/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikichip.org/wiki/x86/avx512_vnni&#34;&gt;https://en.wikichip.org/wiki/x86/avx512_vnni&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.regehr.org/archives/1307&#34;&gt;https://blog.regehr.org/archives/1307&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lab.cs.tsinghua.edu.cn/hpc/doc/assignments/5.simd/&#34;&gt;https://lab.cs.tsinghua.edu.cn/hpc/doc/assignments/5.simd/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://en.algorithmica.org/hpc/simd/&#34;&gt;https://en.algorithmica.org/hpc/simd/&lt;/a&gt;&lt;/strong&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=vIRjSdTCIEU&#34;&gt;https://www.youtube.com/watch?v=vIRjSdTCIEU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html&#34;&gt;https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html&lt;/a&gt;&lt;/strong&gt; &lt;a href=&#34;https://github.com/intel/optimization-manual&#34;&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：掌握 RAPIDS libcudf 中的字符串转换</title>
      <link>https://weedge.github.io/post/gpu/mastering-string-transformations-in-rapids-libcudf/</link>
      <pubDate>Tue, 07 Nov 2023 15:00:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/mastering-string-transformations-in-rapids-libcudf/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/mastering-string-transformations-in-rapids-libcudf/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;字符串数据的高效处理对于许多数据科学应用至关重要。为了从字符串数据中提取有价值的信息，&lt;a href=&#34;https://github.com/rapidsai/cudf&#34;&gt;RAPIDS libcudf&lt;/a&gt;提供了强大的工具来加速字符串数据转换。libcudf 是一个 C++ GPU DataFrame 库，用于加载、连接、聚合和过滤数据。&lt;/p&gt;
&lt;p&gt;在数据科学中，字符串数据代表语音、文本、基因序列、日志记录和许多其他类型的信息。在使用字符串数据进行机器学习和特征工程时，必须经常对数据进行规范化和转换，然后才能将其应用于特定用例。libcudf 提供通用 API 和设备端实用程序，以支持各种自定义字符串操作。&lt;/p&gt;
&lt;p&gt;这篇文章演示了如何使用 libcudf 通用 API 巧妙地转换字符串列。您将获得有关如何使用自定义内核和 libcudf 设备端实用程序解锁峰值性能的新知识。这篇文章还向您介绍了如何最好地管理 GPU 内存和高效构建 libcudf 列以加速字符串转换的示例。&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;注&lt;/strong&gt;：从文件中获取数据到buffer中，都需要通过字符串处理操作，特别是split操作，如果是数值，需要atoi，atof操作进行数据分析，向量化操作等， 和数据处理打交道的 super 马里奥 应该学会这个工具，这里直接使用底层操作库libcudf；集成的其他语言有java(JNI)和python(cython), 主要是方便和现有 大数据生态打通(会有一些内存方面的性能损耗)，大多是离线处理场景，特别是LLM的预训练场景)&lt;/p&gt;
&lt;h2 id=&#34;引入字符串arrow格式&#34;&gt;引入字符串Arrow格式&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-layout&#34;&gt;libcudf 使用Arrow 格式&lt;/a&gt;将字符串数据存储在设备内存中，该格式将字符串列表示为两个子列：&lt;code&gt;chars and offsets&lt;/code&gt; 图 1所示。&lt;/p&gt;
&lt;p&gt;该&lt;code&gt;chars&lt;/code&gt;列将字符串数据保存为连续存储在内存中的 UTF-8 编码字符字节。&lt;/p&gt;
&lt;p&gt;该&lt;code&gt;offsets&lt;/code&gt;列包含递增的整数序列，这些整数是标识 chars 数据数组中每个单独字符串的开头的字节位置。最后的偏移量元素是 chars 列中的字节总数。这意味着行中单个字符串的大小&lt;code&gt;i&lt;/code&gt;定义为 ( &lt;code&gt;offsets[i+1]-offsets[i])&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/mastering-string-transformations-in-rapids-libcudf/2.png&#34; alt=&#34;显示字符串向量 {&amp;ldquo;this&amp;rdquo;, &amp;ldquo;is&amp;rdquo;, &amp;ldquo;a&amp;rdquo;, &amp;ldquo;column&amp;rdquo;, &amp;ldquo;of&amp;rdquo;, &amp;ldquo;strings&amp;rdquo;} 及其表示为大小为 6 的字符串类型列的示意图，从而产生“偏移量” INT32 类型的子列和 INT8 类型的“字符”子列。&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 1. 示意图显示箭头格式如何表示带有&lt;code&gt;chars&lt;/code&gt;子&lt;code&gt;offsets&lt;/code&gt;列的字符串列&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;字符串编辑功能示例&#34;&gt;字符串编辑功能示例&lt;/h2&gt;
&lt;p&gt;为了说明字符串转换示例，请考虑一个函数，该函数接收两个输入字符串列并生成一个经过编辑的输出字符串列。&lt;/p&gt;
&lt;p&gt;输入数据具有以下形式：“name”列包含用空格分隔的名字和姓氏，以及包含“public”或“private”状态的“visibilities”列。&lt;/p&gt;
&lt;p&gt;我们提出了“redact”函数，该函数对输入数据进行操作以生成由姓氏的第一个首字母后跟空格和整个名字组成的输出数据。但是，如果相应的可见性列是“private”，则输出字符串应完全编辑为“X X”。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/mastering-string-transformations-in-rapids-libcudf/3.png&#34; alt=&#34;该表显示“编辑”字符串转换示例，该转换接收名称和可见性字符串列作为输入，并接收部分或完全编辑的数据作为输出。&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;表 1.“编辑”字符串转换示例，该转换接收名称和可见性字符串列作为输入，并接收部分或完全编辑的数据作为输出&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;使用-libcudf-api-转换字符串&#34;&gt;使用 libcudf API 转换字符串&lt;/h2&gt;
&lt;p&gt;首先，可以使用&lt;a href=&#34;https://docs.rapids.ai/api/libcudf/nightly/group__strings__apis.html&#34;&gt;libcudf strings API&lt;/a&gt;完成字符串转换。通用 API 是一个很好的起点，也是比较性能的良好基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;API 函数对整个字符串列进行操作，每个函数至少启动一个内核，并为每个字符串分配一个线程。每个线程在 GPU 上并行处理单行数据，并输出单行作为新输出列的一部分&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;要使用通用 API 完成 redact 示例函数，请按照以下步骤操作：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用以下命令将“visibilities”字符串列转换为布尔列&lt;code&gt;contains&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;每当布尔列中的相应行条目为“false”时，通过复制“X X”，从名称列创建一个新的字符串列&lt;/li&gt;
&lt;li&gt;将“redacted”列拆分为名字和姓氏列&lt;/li&gt;
&lt;li&gt;将姓氏的第一个字符切片作为姓氏首字母&lt;/li&gt;
&lt;li&gt;通过使用空格 (“ “) 分隔符连接最后一个姓名缩写列和第一个姓名列来构建输出列。&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// convert the visibility label into a boolean
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;visible&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_scalar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;public&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;allowed&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;// redact names 
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_scalar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;X X&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redacted&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;copy_if_else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;allowed&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;// split the first name and last initial into two columns
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strings_column_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;redacted&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first_last&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first_last&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;column&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first_last&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;column&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;slice_strings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;last&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;  

&lt;span class=&#34;c1&#34;&gt;// assemble a result column
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;table_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;});&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;concatenate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在具有 600K 行数据的 A6000 上，此方法大约需要 3.5 毫秒。此示例使用&lt;code&gt;contains&lt;/code&gt;,&lt;code&gt;copy_if_else, split, slice_strings&lt;/code&gt;和&lt;code&gt;concatenate&lt;/code&gt;来完成自定义字符串转换。&lt;a href=&#34;https://developer.nvidia.com/nsight-systems&#34;&gt;Nsight Systems&lt;/a&gt;的分析显示该&lt;code&gt;split&lt;/code&gt;函数花费的时间最长，其次是&lt;code&gt;slice_strings&lt;/code&gt;和&lt;code&gt;concatenate&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;图 2 显示了来自 Nsight Systems 的 redact 示例的分析数据，显示了每秒高达约 6 亿个元素的端到端字符串处理。这些区域对应于与每个功能相关的 NVTX 范围。浅蓝色范围对应于 CUDA 内核运行的时间段。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/mastering-string-transformations-in-rapids-libcudf/4.png&#34; alt=&#34;显示使用 libcudf strings API 实现的 redact 示例的分析数据的水平条形图。 时间线显示了 contains、copy_if_else、split、slice_strings 和 concatenate 在 600K 到 10M 的行数范围内运行。 时间线将内核执行与字符串 API 函数重叠。 &#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 2. 对来自 Nsight Systems 的 redact 示例的数据进行分析&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;使用自定义内核转换字符串&#34;&gt;使用自定义内核转换字符串&lt;/h2&gt;
&lt;p&gt;libcudf strings API 是一个快速高效的字符串转换工具包，但有时性能关键的函数需要运行得更快。libcudf 字符串 API 中额外工作的一个关键来源是为每个 API 调用在全局设备内存中创建至少一个新字符串列，从而提供了将多个 API 调用组合到自定义内核中的机会。&lt;/p&gt;
&lt;h3 id=&#34;内核-malloc-调用的性能限制&#34;&gt;内核 malloc 调用的性能限制&lt;/h3&gt;
&lt;p&gt;首先，我们将构建一个自定义内核来实现编辑示例转换。在设计这个内核时，我们必须记住 libcudf 字符串列是不可变的。&lt;/p&gt;
&lt;p&gt;字符串列无法就地更改，因为字符字节是连续存储的，并且对字符串长度的任何更改都会使偏移量数据无效。因此，&lt;code&gt;redact_kernel&lt;/code&gt;自定义内核通过使用 libcudf 列工厂来构建新的字符串列&lt;code&gt;offsets&lt;/code&gt;和&lt;code&gt;chars&lt;/code&gt;子列。&lt;/p&gt;
&lt;p&gt;在第一种方法中，每行的输出字符串是使用内核内部的 malloc 调用在&lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#dynamic-global-memory-allocation-and-operations&#34;&gt;动态设备内存(dynamic device memory)&lt;/a&gt;中创建的。自定义内核输出是指向每行输出的设备指针向量，并且该向量用作字符串列工厂的输入。&lt;/p&gt;
&lt;p&gt;自定义内核接受 &lt;a href=&#34;https://docs.rapids.ai/api/libcudf/nightly/classcudf_1_1column__device__view.html&#34;&gt;&lt;code&gt;cudf::column_device_view&lt;/code&gt;&lt;/a&gt;来访问字符串列数据，并使用该&lt;code&gt;element&lt;/code&gt;方法返回&lt;a href=&#34;https://docs.rapids.ai/api/libcudf/nightly/classcudf_1_1string__view.html&#34;&gt;&lt;code&gt;cudf::string_view&lt;/code&gt;&lt;/a&gt;表示指定行索引处的字符串数据。内核输出是一个向量类型&lt;code&gt;cudf::string_view&lt;/code&gt;，它保存指向设备内存的指针，其中包含输出字符串以及该字符串的大小（以字节为单位）。&lt;/p&gt;
&lt;p&gt;该类&lt;code&gt;cudf::string_view&lt;/code&gt;与  C++17 &lt;code&gt;std::string_view&lt;/code&gt; 类类似，但专门为 libcudf 实现，并将固定长度的字符数据包装在设备内存中编码为 UTF-8。它具有许多与std相关函数相同的特性（例如&lt;code&gt;find&lt;/code&gt;，&lt;code&gt;substr&lt;/code&gt;功能）以及 限制（没有空终止符）。&lt;code&gt;cudf::string_view&lt;/code&gt;表示存储在设备内存中的字符序列，因此我们可以在此处使用它来记录输出向量的 malloc 内存。&lt;/p&gt;
&lt;h3 id=&#34;malloc内核&#34;&gt;Malloc内核&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// note the column_device_view inputs to the kernel
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;__global__&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;redact_kernel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;column_device_view&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;column_device_view&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;c1&#34;&gt;// get index for this thread
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;threadIdx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;blockIdx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;blockDim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;visible&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;public&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;element&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vis&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;element&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vis&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt;    &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;find&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sc&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;        &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;substr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;substr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    
    &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;static_cast&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;malloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;// build output string
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()};&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;__global__&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;free_kernel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;threadIdx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;blockIdx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;blockDim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ptr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const_cast&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ptr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;free&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// free everything that does match the redaction string
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在测量内核性能之前，这似乎是一种合理的方法。这种方法在具有 60 万行数据的 A6000 上大约需要 108 毫秒，比上面使用 libcudf strings API 提供的解决方案慢了 30 倍以上。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;redact_kernel         60.3ms
free_kernel           45.5ms
make_strings_column    0.5ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;主要瓶颈是&lt;code&gt;malloc/free&lt;/code&gt;两个内核内部的调用。CUDA动态设备内存需要&lt;code&gt;malloc/free&lt;/code&gt;同步内核中的调用，导致并行执行退化为顺序执行。&lt;/p&gt;
&lt;p&gt;（&lt;strong&gt;注&lt;/strong&gt;：这个方法主要是为了对比 提前分配内存的消除核内分配内存的情况，以及后面的内存资源管理rmm）&lt;/p&gt;
&lt;h3 id=&#34;预分配工作内存以消除瓶颈&#34;&gt;预分配工作内存以消除瓶颈&lt;/h3&gt;
&lt;p&gt;通过在启动内核之前用预先&lt;code&gt;malloc/free&lt;/code&gt;分配的工作内存替换内核中的调用&lt;code&gt;malloc/free&lt;/code&gt;来消除瓶颈。&lt;/p&gt;
&lt;p&gt;对于redact示例，此示例中每个字符串的输出大小不应大于输入字符串本身，因为逻辑仅删除字符。因此，可以使用与输入缓冲区大小相同的单个设备内存缓冲区。使用输入偏移量来定位每行位置。&lt;/p&gt;
&lt;p&gt;访问字符串列的偏移量涉及用&lt;code&gt;cudf::strings_column_view&lt;/code&gt;包装&lt;code&gt;cudf::column_view&lt;/code&gt;并调用其&lt;code&gt; offsets_begin&lt;/code&gt;方法。还可以使用&lt;code&gt;chars_size&lt;/code&gt;方法访问&lt;code&gt;chars&lt;/code&gt;子列的大小。然后在内核之前调用&lt;code&gt;rmm::device_uvector&lt;/code&gt;预先分配内存，来存储字符输出数据。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strings_column_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offsets&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;offsets_begin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;working_memory&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rmm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_uvector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chars_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;预分配内核&#34;&gt;预分配内核&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;n&#34;&gt;__global__&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;redact_kernel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;column_device_view&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;column_device_view&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;working_memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;offset_type&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;threadIdx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;blockIdx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;blockDim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;visible&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;public&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;element&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vis&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;element&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vis&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt;    &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;find&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sc&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;        &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;substr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;substr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;// resolve output string location
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;working_memory&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;// build output string into output_ptr
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()};&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;内核输出一个传递 &lt;code&gt;cudf::string_view&lt;/code&gt; 给 &lt;strong&gt;&lt;a href=&#34;https://docs.rapids.ai/api/libcudf/nightly/group__column__factories.html#ga163234e4e6b8f95d7a8f1796a0c3c79d&#34;&gt;&lt;code&gt;cudf::make_strings_column&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; 工厂函数的对象向量。该函数的第二个参数用于识别输出列中的空条目。本文中的示例没有 null 条目，因此&lt;code&gt;cudf::string_view{nullptr,0}&lt;/code&gt;使用 nullptr 占位符。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;str_ptrs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rmm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_uvector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;redact_kernel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;blocks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;block_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                                         &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                                         &lt;span class=&#34;n&#34;&gt;d_redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
                                                         &lt;span class=&#34;n&#34;&gt;working_memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
                                                         &lt;span class=&#34;n&#34;&gt;offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                                         &lt;span class=&#34;n&#34;&gt;str_ptrs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_strings_column&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str_ptrs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;nullptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这种方法在具有 60 万行数据的 A6000 上大约需要 1.1 毫秒，因此比基线快 2 倍以上。大致细分如下所示：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;  redact_kernel            66us
  make_strings_column     400us
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;剩余时间花费在&lt;code&gt;cudaMalloc, cudaFree, cudaMemcpy,&lt;/code&gt;管理临时&lt;code&gt;rmm::device_uvector&lt;/code&gt;实例的典型开销中。如果保证所有输出字符串的大小等于或小于输入字符串，则此方法效果很好。&lt;/p&gt;
&lt;p&gt;总体而言，使用 RAPIDS RMM 切换到批量工作内存分配是一项重大改进，也是自定义字符串函数的良好解决方案。&lt;/p&gt;
&lt;h3 id=&#34;优化列创建以缩短计算时间&#34;&gt;优化列创建以缩短计算时间&lt;/h3&gt;
&lt;p&gt;有没有办法进一步改善这一点？现在的瓶颈是&lt;code&gt;cudf::make_strings_column&lt;/code&gt; 工厂函数，它从 &lt;code&gt;cudf::string_view&lt;/code&gt; 对象向量构建两个字符串列组件 &lt;code&gt;offsets&lt;/code&gt; 和  &lt;code&gt;chars&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;在 libcudf 中，包含了许多工厂函数来构建字符串列。前面示例中使用的工厂函数获取&lt;code&gt;cudf::string_view&lt;/code&gt;对象的&lt;code&gt;cudf::device_span&lt;/code&gt;，然后通过对底层字符数据执行&lt;code&gt;gather&lt;/code&gt;来构造列，以构建偏移量和字符子列。&lt;code&gt;rmm::device_uvector&lt;/code&gt;可自动转换为&lt;code&gt;cudf::device_span&lt;/code&gt;，而无需复制任何数据。&lt;/p&gt;
&lt;p&gt;但是，如果直接构建字符向量和偏移向量，则可以使用不同的工厂函数，该函数只需创建字符串列，而不需要收集来复制数据。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sizes_kernel&lt;/code&gt;首先传递输入数据，以计算每个输出行的确切输出大小：&lt;/p&gt;
&lt;h3 id=&#34;优化内核第-1-部分&#34;&gt;优化内核：第 1 部分&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;n&#34;&gt;__global__&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sizes_kernel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;column_device_view&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                             &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;column_device_view&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                             &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_sizes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;threadIdx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;blockIdx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;blockDim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;visible&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;public&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;X X&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;element&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vis&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;element&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

  &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_type&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// init to redaction size
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vis&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt;    &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;find&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sc&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;        &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;substr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;substr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

  &lt;span class=&#34;n&#34;&gt;d_sizes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后通过执行 in-place &lt;code&gt;exclusive_scan&lt;/code&gt;将输出大小转换为偏移量。请注意，&lt;code&gt;offsets&lt;/code&gt;向量是用&lt;code&gt;names.size()+1&lt;/code&gt;元素创建的。最后一个条目将是字节总数（所有大小加在一起），而第一个条目将为 0。这些都由&lt;code&gt;exclusive_scan&lt;/code&gt;调用处理。从&lt;code&gt;offsets&lt;/code&gt;列的最后一个条目检索&lt;code&gt;chars&lt;/code&gt;列的大小，以构建字符向量。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// create offsets vector
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offsets&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rmm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_uvector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;// compute output sizes
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sizes_kernel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;blocks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;block_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
  &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;thrust&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exclusive_scan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rmm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exec_policy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;begin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;end&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;begin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;redact_kernel&lt;/code&gt;逻辑仍然非常相同，只是它接受输出&lt;code&gt;d_offsets&lt;/code&gt;向量来解析每行的输出位置：&lt;/p&gt;
&lt;h3 id=&#34;优化内核第-2-部分&#34;&gt;优化内核：第 2 部分&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;n&#34;&gt;__global__&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;redact_kernel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;column_device_view&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;column_device_view&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_type&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                              &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;threadIdx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;blockIdx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;blockDim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;visible&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;public&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;X X&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

  &lt;span class=&#34;c1&#34;&gt;// resolve output_ptr using the offsets vector
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_chars&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;

  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;element&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vis&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;element&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vis&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt;    &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;find&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sc&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;        &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;substr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;substr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;space_idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;// build output string
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last_initial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;redaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_bytes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从&lt;code&gt;d_offsets&lt;/code&gt;列的最后一个条目检索输出&lt;code&gt;d_chars&lt;/code&gt;列的大小以分配字符向量。内核使用预先计算的偏移向量启动并返回填充的字符向量。最后，libcudf 字符串列工厂创建输出字符串列。&lt;/p&gt;
&lt;p&gt;此&lt;a href=&#34;https://docs.rapids.ai/api/libcudf/nightly/group__column__factories.html#ga86f7623f0d230c96491ef88d665385cc&#34;&gt;&lt;code&gt;cudf::make_strings_column&lt;/code&gt;&lt;/a&gt;工厂函数构建字符串列而不复制数据。&lt;code&gt;offsets&lt;/code&gt;数据和&lt;code&gt; chars&lt;/code&gt;数据已经采用正确的预期格式，该工厂只是从每个向量中移动数据并在其周围创建列结构。完成后，&lt;code&gt;offsets&lt;/code&gt;和&lt;code&gt;chars&lt;/code&gt;的&lt;code&gt;rmm::device_uvectors&lt;/code&gt;为空，它们的数据已移动到输出列中。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size_type&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;back_element&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chars&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rmm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_uvector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;redact_kernel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;blocks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;block_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_visibilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;// from pre-assembled offsets and character buffers
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cudf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_strings_column&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;move&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;offsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;move&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这种方法在具有 600K 行数据的 A6000 上大约需要 300 us (0.3 ms)，比之前的方法提高了 2 倍以上。您可能会注意到&lt;code&gt;sizes_kernel&lt;/code&gt;和&lt;code&gt;redact_kernel&lt;/code&gt;共享很多相同的逻辑：一次测量输出的大小，然后再次填充输出。&lt;/p&gt;
&lt;p&gt;从代码质量的角度来看，将转换重构为由&lt;code&gt;sizes_kernel&lt;/code&gt;和&lt;code&gt;redact_kernel&lt;/code&gt;调用的设备函数是有益的。从性能角度来看，您可能会惊讶地发现转换的计算成本被支付了两倍。&lt;/p&gt;
&lt;p&gt;内存管理和更高效的列创建的好处通常超过执行两次转换的计算成本。&lt;/p&gt;
&lt;p&gt;表 2 显示了本文讨论的四种解决方案的计算时间、内核计数和处理的字节数。“内核启动总数”反映了启动的内核总数，包括计算内核和辅助内核。“处理的总字节数”是累积的 DRAM 读取和写入吞吐量，“处理的最小字节数”是我们的测试输入和输出的平均每行 37.9 字节。理想的“内存带宽有限”情况假设带宽为 768 GB/s，这是 A6000 的理论峰值吞吐量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/mastering-string-transformations-in-rapids-libcudf/5.png&#34; alt=&#34;该表显示了本文讨论的四种解决方案的计算时间、内核计数和处理的字节数。&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;表 2. 本文讨论的四种解决方案的计算时间、内核计数和处理的字节数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;由于内核启动次数减少和处理的总字节数减少，“优化内核”提供了最高的吞吐量。借助高效的自定义内核，内核启动总数从 31 次减少到 4 次，处理的总字节数从输入加输出大小的 12.6 倍减少到 1.75 倍。&lt;/p&gt;
&lt;p&gt;因此，定制内核的吞吐量比用于编辑转换的通用字符串 API 高 10 倍以上。&lt;/p&gt;
&lt;h2 id=&#34;峰值性能分析&#34;&gt;峰值性能分析&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/fast-flexible-allocation-for-cuda-with-rapids-memory-manager/&#34;&gt;RAPIDS 内存管理器 (RMM)&lt;/a&gt;中的池内存资源是另一个可用于提高性能的工具。上面的示例使用默认的“CUDA 内存资源”来分配和释放全局设备内存。然而，分配工作内存所需的时间会增加字符串转换步骤之间的显着延迟。RMM 中的“内存池资源”通过预先分配大量内存并在处理过程中根据需要分配子分配来减少延迟。&lt;/p&gt;
&lt;p&gt;使用 CUDA 内存资源，“优化内核”显示了 10 倍到 15 倍的加速，但由于分配大小的增加，加速在行数增加时开始下降（图 3）。使用池内存资源可以减轻这种影响，并比 libcudf strings API 方法保持 15-25 倍的加速。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/mastering-string-transformations-in-rapids-libcudf/6.png&#34; alt=&#34;对于 600K 到 10M 的行计数范围，使用自定义内核与 libcudf 字符串 API 的加速效果的散点图。 加速数据包括 4 个条件：“预分配内核”和“优化内核”，具有 CUDA 内存资源和池内存资源。 在每种情况下，池内存资源的性能均优于 CUDA 内存资源。 “优化内核 + RMM 池”显示大约 12-25 倍加速，“预分配内核 + RMM 池”显示 5-10 倍加速。&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 3. 使用默认 CUDA 内存资源（实线）和池内存资源（虚线)的自定义内核“预分配内核”和“优化内核”的加速与使用默认 CUDA 内存资源的 libcudf 字符串 API 的加速&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;利用池内存资源，证明了端到端内存吞吐量接近两遍算法的理论极限。使用输入大小加上输出大小和计算时间来测量，“优化内核”的吞吐量达到 320-340 GB/s（图 4）。&lt;/p&gt;
&lt;p&gt;两遍方法首先测量输出元素的大小，分配内存，然后使用输出设置内存。给定两遍处理算法，“优化内核”中的实现的性能接近内存带宽限制。“端到端内存吞吐量”定义为输入加输出大小（以 GB 为单位）除以计算时间。RTX A6000 内存带宽 (768 GB/s)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/mastering-string-transformations-in-rapids-libcudf/7.png&#34; alt=&#34;散点图显示“优化内核”、“预分配内核”和“libcudf 字符串 API”的内存吞吐量与输入/输出行计数的函数关系。 “端到端内存吞吐量”定义为输入加输出大小（以 GB 为单位）除以计算时间。 使用池内存资源时，“libcudf strings API”饱和度约为 40 GB/s，“预分配内核”饱和度约为 150 GB/s，“优化内核”饱和度约为 340 GB/s。&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 4. “优化内核”、“预分配内核”和“libcudf 字符串 API”的内存吞吐量与输入/输出行计数的关系&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;这篇文章演示了在 &lt;a href=&#34;https://docs.rapids.ai/api/libcudf/nightly/index.html&#34;&gt;libcudf&lt;/a&gt; 中编写高效字符串数据转换的两种方法。libcudf 通用 API 对于开发人员来说快速、简单，并且提供良好的性能。libcudf 还提供了专为与自定义内核一起使用而设计的设备端实用程序，在本例中解锁了 10 倍以上的更快性能。&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/mastering-string-transformations-in-rapids-libcudf/&#34;&gt;https://developer.nvidia.com/blog/mastering-string-transformations-in-rapids-libcudf/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.rapids.ai/api/libcudf/nightly/index.html&#34;&gt;https://docs.rapids.ai/api/libcudf/nightly/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rapidsai/cudf/tree/HEAD/cpp/examples/strings&#34;&gt;https://github.com/rapidsai/cudf/tree/HEAD/cpp/examples/strings&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：加速向量搜索：RAPIDS RAFT IVF-Flat 近似算法</title>
      <link>https://weedge.github.io/post/gpu/3.accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/</link>
      <pubDate>Fri, 03 Nov 2023 15:00:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/3.accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;执行详尽的精确 k 最近邻 (kNN) 搜索，也称为&lt;em&gt;暴力搜索(brute-force search)&lt;/em&gt;，成本高昂，并且它不能很好地扩展到更大的数据集。在向量搜索期间，暴力搜索需要计算每个查询向量和数据库向量之间的距离。对于常用的欧几里德和余弦距离，计算任务等同于大型矩阵乘法。&lt;/p&gt;
&lt;p&gt;虽然 GPU 在执行矩阵乘法方面效率很高，但随着数据量的增加，计算成本变得令人望而却步。然而，许多应用程序不需要精确的结果，而是可以为了更快的搜索而牺牲一些准确性。当不需要精确的结果时，近似最近邻 (ANN) 方法通常可以减少搜索期间必须执行的距离计算的数量。&lt;/p&gt;
&lt;p&gt;本文主要介绍了 IVF-Flat，这是 NVIDIA &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/&#34;&gt;RAPIDS RAFT&lt;/a&gt; 中的一种方法。IVF-Flat 方法使用原始（即Flat）向量的倒排索引 (IVF)。此算法提供了简单的调整手段，以减少整体搜索空间并在准确性和速度之间进行权衡。&lt;/p&gt;
&lt;p&gt;为了帮助了解如何使用 IVF-Flat，我们讨论了该算法的工作原理，并演示了&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#ivf-flat&#34;&gt;Python&lt;/a&gt;和&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_ivf_flat/&#34;&gt;C++ APIs&lt;/a&gt;我们介绍了索引构建的设置参数，并提供了如何配置 GPU 加速的 IVF-Flat搜索的技巧。这些步骤也可以在示例中遵循&lt;a href=&#34;https://github.com/rapidsai/raft/blob/a1002f8c8f4debc52fbab7191297a2f54ff42856/notebooks/ivf_flat_example.ipynb&#34;&gt;Python notebook&lt;/a&gt;和&lt;a href=&#34;https://github.com/rapidsai/raft/blob/a1002f8c8f4debc52fbab7191297a2f54ff42856/cpp/template/src/ivf_flat_example.cu&#34;&gt;C++ project&lt;/a&gt;.最后，我们演示了 GPU 加速的向量搜索比 CPU 搜索快一个数量级。&lt;/p&gt;
&lt;h2 id=&#34;ivf-flat-算法&#34;&gt;IVF-Flat 算法&lt;/h2&gt;
&lt;p&gt;IVF 方法通过将数据集向量分组为簇(cluster)并将搜索限制在每个查询的一些最近簇来加速向量搜索(图 1)。&lt;/p&gt;
&lt;p&gt;在 IVF-Flat 算法中，只搜索几个簇(而不是整个数据集)是实际的近似值。使用此近似值，可能会错过分配给未搜索的簇的一些近邻，但它极大地缩短了搜索时间。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/2.png&#34; alt=&#34;Two diagrams show a) dataset points grouped into clusters and b) a subset of the clusters highlighted.&#34;&gt;&lt;em&gt;图 1.分为簇的数据集(左)，且搜索仅限于查询附近的簇(右)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;在搜索数据集之前，必须构建索引，这是一种存储高效搜索所需信息的结构。对于 IVF-Flat，索引存储簇的描述：其中心坐标和属于簇的向量列表。此列表是倒排列表，也称为倒排文件，这就是 IVF 的首字母缩写词。&lt;/p&gt;
&lt;p&gt;在讨论倒排文件后，我们将在以下部分演示如何构建索引并解释如何执行搜索。&lt;/p&gt;
&lt;h3 id=&#34;ivf-含义&#34;&gt;IVF 含义&lt;/h3&gt;
&lt;p&gt;为完整起见，以下是一些历史语境。倒排&lt;em&gt;文件&lt;/em&gt;(或倒排索引)来自信息检索字段。&lt;/p&gt;
&lt;p&gt;以几个简单的文本文档为例。如果要搜索包含给定单词的文档，&lt;a href=&#34;https://en.wikipedia.org/wiki/Search_engine_indexing#The_forward_index&#34;&gt; forward index&lt;/a&gt;会存储每个文档的单词列表。必须明确阅读每个文档才能找到相关的文档。&lt;/p&gt;
&lt;p&gt;相比之下，&lt;a href=&#34;https://en.wikipedia.org/wiki/Inverted_index&#34;&gt;倒排索引&lt;/a&gt;包含了可以搜索的所有单词的字典，并且对于每个单词，都有一个该单词所在的文档索引列表。这就是所谓的倒排列表（倒排文件），可以将搜索限制在选定的列表中。&lt;/p&gt;
&lt;p&gt;如今，文本数据通常表示为向量嵌入(embedding)。IVF-Flat 方法定义了簇中心，这些中心类似于前面示例中的词典。对于每个簇中心，都有属于该簇的向量索引列表，并且搜索速度加快，因为只需检查选定的簇。&lt;/p&gt;
&lt;h2 id=&#34;索引构建&#34;&gt;索引构建&lt;/h2&gt;
&lt;p&gt;索引构建主要是对数据集进行聚类运算。ivf_flat可以使用以下代码示例在 Python 中创建索引：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pylibraft.neighbors&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;build_params&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;IndexParams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;n_lists&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;metric&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sqeuclidean&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;build&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;build_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 C++中，有以下语法：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;raft/neighbors/ivf_flat.cuh&amp;gt; &lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;using&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;namespace&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;neighbors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_resources&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dev_resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_params&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;index_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_lists&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;index_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;metric&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;distance&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DistanceType&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;L2Expanded&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
 
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;build&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dev_resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_const_mdspan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;创建索引的最重要超参数是n_lists其中会指明要使用的簇数量。还可以指定距离计算指标。&lt;/p&gt;
&lt;h2 id=&#34;搜索&#34;&gt;搜索&lt;/h2&gt;
&lt;p&gt;构建索引后，搜索很简单。在 Python 中，以下调用返回两个数组：相邻数组的索引及其与查询向量的距离：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;distances&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;indices&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;search&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SearchParams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_probes&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;queries&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;C++中的等效调用需要预先分配输出数组：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topk&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;neighbors&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_device_matrix&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int64_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int64_t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dev_resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_queries&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;distances&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_device_matrix&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int64_t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dev_resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_queries&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;search_params&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;search_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_probes&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;search&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dev_resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;search_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_const_mdspan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;queries&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()),&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;neighbors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;distances&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在这里，可以搜索k=10每个查询的近邻。参数 n_probes 会告知每个查询要搜索(或探测)的簇数量，并确定搜索的准确性。&lt;/p&gt;
&lt;p&gt;仅通过测试 n_probes 对于每个查询的簇，可以省略分配给簇的一些近邻，簇的中心距离查询点更远。搜索质量通常以*召回率，*这是实际最近 k 近邻在所有返回近邻中的百分比。&lt;/p&gt;
&lt;p&gt;在内部，搜索分两个步骤执行(图 2)：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;粗略搜索选择 n_probes 每个查询的附近簇。&lt;/li&gt;
&lt;li&gt;精细搜索将查询向量与选定簇中的所有数据集向量进行比较。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/3.png&#34; alt=&#34;Diagram of clusters represented by their centers with the clusters highlighted that are closest to the queries. Selected clusters shown with the individual points within these clusters.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 2.两步搜索：通过比较查询与簇中心来选择附近的簇(左)，并比较选定簇中的所有向量与相应的查询(右)&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;粗略搜索&#34;&gt;粗略搜索&lt;/h3&gt;
&lt;p&gt;粗略搜索使用簇中心和查询向量之间的精确 kNN 搜索完成。选择最近的簇中心，&lt;code&gt;n_probes&lt;/code&gt; 个簇粗略搜索相对便宜，因为簇数量远小于数据集大小(例如，1 亿个向量的簇数量为 1 万个)。&lt;/p&gt;
&lt;h3 id=&#34;精细搜索&#34;&gt;精细搜索&lt;/h3&gt;
&lt;p&gt;对于 IVF-Flat，精细搜索也是精确搜索。但每个查询都有自己的一组要搜索(要探测)的簇，并且计算查询向量与被探测簇中所有向量之间的距离。&lt;/p&gt;
&lt;p&gt;对于小批量，在查询点周围搜索的区域不会重叠。因此，问题结构变为批量矩阵向量乘法 (GEMV) 运算。此运算受内存带宽限制，GPU 显存的大带宽大大加速了此步骤。&lt;/p&gt;
&lt;p&gt;选中每个探测簇的 top-k 近邻，结果是&lt;code&gt;n_probes * k&lt;/code&gt; 个近邻候选。&lt;/p&gt;
&lt;h2 id=&#34;调整索引构建参数&#34;&gt;调整索引构建参数&lt;/h2&gt;
&lt;p&gt;在前面的部分中，概述了索引构建和搜索。下面详细介绍了如何设置索引构建的参数。&lt;/p&gt;
&lt;p&gt;索引构建包含两个阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;训练或计算簇（构建）&lt;/strong&gt;：平衡的分层 k-means 算法会对训练数据进行聚类。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;将数据集向量添加到索引(扩展)&lt;/strong&gt;：将数据集向量分配给其簇，并将其添加到相应簇的向量列表中。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;簇数量&#34;&gt;簇数量&lt;/h3&gt;
&lt;p&gt;参数 &lt;code&gt;n_list&lt;/code&gt; 对训练和搜索期间的整体性能有着深远的影响：它定义了索引数据所划分的簇数量。设置n_lists=sqrt (n_samples)是一个很好的起点(&lt;code&gt;n_samples&lt;/code&gt;是数据集中的向量数)。&lt;/p&gt;
&lt;p&gt;为确保高效利用 GPU 资源，簇的平均大小(即&lt;code&gt;n_samples/n_lists&lt;/code&gt;)应在至少 1K 个向量的范围内，以保持单个流多处理器 (SM) 的繁忙状态。&lt;/p&gt;
&lt;h3 id=&#34;使用自动数据子采样构建索引&#34;&gt;使用自动数据子采样构建索引&lt;/h3&gt;
&lt;p&gt;K-means 聚类是计算密集型的。为加速索引构建，对数据集进行子采样。使用参数&lt;code&gt;kmeans_trainset_fraction=0.1&lt;/code&gt;这意味着将十分之一的数据集用于训练簇中心。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;build_params&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;IndexParams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;n_lists&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;metric&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sqeuclidean&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;kmeans_trainset_fraction&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;kmeans_n_iters&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在训练期间，参数 kmeans_n_iters 将直接传递给 k-means 算法。将其设置为适用于大多数数据集的合理默认值 20.但是，此参数只是聚类算法的建议。在幕后，它通常在“平衡”阶段执行更多迭代，以确保簇具有相似的大小。&lt;/p&gt;
&lt;h3 id=&#34;使用用于聚类的特定训练数据构建索引&#34;&gt;使用用于聚类的特定训练数据构建索引&lt;/h3&gt;
&lt;p&gt;在前面的示例中，只需调用ivf_flat.build执行聚类并将整个数据集添加到索引中。或者，可以调用ivf_flat.build无需将向量添加到索引中即可训练向量(通过设置add_data_on_build=False).这允许精确控制用于训练索引的向量。随后，ivf_flat.extend可用于向索引中添加向量。&lt;/p&gt;
&lt;p&gt;如下 Python 代码示例所示：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;n_train&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;train_set&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choice&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;replace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),:]&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;build_params&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;IndexParams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;n_lists&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;metric&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sqeuclidean&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;kmeans_trainset_fraction&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;kmeans_n_iters&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;add_data_on_build&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;build&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;build_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;int64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;数据集向量只需调用ivf_flat.extend。在内部，如果需要减少内存消耗，则对数据进行批量处理。相应的 C++代码如下所示：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;n&#34;&gt;index_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_data_on_build&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;// Sub sample the dataset to create trainset.
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// ...
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// Run k-means clustering using the training set
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;build&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dev_resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_const_mdspan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;trainset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()));&lt;/span&gt;
 
&lt;span class=&#34;c1&#34;&gt;// Fill the index with the dataset vectors
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dev_resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_const_mdspan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()),&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optional&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_vector_view&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int64_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int64_t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;向索引中添加新向量&#34;&gt;向索引中添加新向量&lt;/h3&gt;
&lt;p&gt;可以通过调用ivf_flat.extend.默认情况下，增加向量列表的成本将通过在增加列表大小时分配额外空间来抵消。C++API 用户可以通过设置以下参数来更改此行为：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;n&#34;&gt;index_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conservative_memory_allocation&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果聚类数量较大且预计不会经常添加向量，则此操作会非常有用。&lt;/p&gt;
&lt;p&gt;默认情况下，当向数据集添加向量时，簇中心不会发生变化。adaptive_centers，如果希望簇中心随新数据移动，则可以在索引构建期间启用标志。&lt;/p&gt;
&lt;h2 id=&#34;调整搜索参数&#34;&gt;调整搜索参数&lt;/h2&gt;
&lt;p&gt;以下是设置搜索参数的方法：高效使用 GPU 资源并增加 n_probes。&lt;/p&gt;
&lt;h3 id=&#34;gpu-资源&#34;&gt;GPU 资源&lt;/h3&gt;
&lt;p&gt;在搜索过程中，需要创建内部工作空间内存。建议使用&lt;strong&gt;池化分配器来减少内存分配&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;构建 RAFT资源对象非常耗时。资源对象应该复用， 通过资源handle传递给搜索函数 。在 Python 中，可以通过以下方式配置设备资源和内存池：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pylibraft.common&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DeviceResources&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;rmm&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rmm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PoolMemoryResource&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
     &lt;span class=&#34;n&#34;&gt;rmm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CudaMemoryResource&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
     &lt;span class=&#34;n&#34;&gt;initial_pool_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;rmm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_current_device_resource&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;handle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DeviceResources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;search_params&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SearchParams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_probes&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;distances&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;indices&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;search&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;search_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;queries&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;handle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;handle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;handle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sync&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;C++API 的用户必须始终传递显式 device_resources handle，并且应在单独调用之间重复使用此handle进行搜索。可以通过以下方式设置池分配器：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_resources&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dev_resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raft&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;resource&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_workspace_to_pool_resource&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dev_resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1024ull&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ivf_flat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;search&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dev_resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;...);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;C++ 用户可以为临时工作空间数组指定一个单独的分配器，这在前面的示例中已经使用过。全局分配器（用于创建输入/输出数组）可以使用 &lt;a href=&#34;https://docs.rapids.ai/api/rmm/stable/api/#rmm.mr.set_current_device_resource&#34;&gt;rmm::mr::set_current_device_resource&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&#34;探针数量&#34;&gt;探针数量&lt;/h3&gt;
&lt;p&gt;比率&lt;code&gt;n_probes/n_lists&lt;/code&gt;表明数据集的哪一部分与每个查询进行比较。距离计算的数量减少到&lt;code&gt;n_probes/n_clusters&lt;/code&gt;暴力搜索计算量的一小部分。搜索质量以及计算时间会随着&lt;code&gt;n_probes&lt;/code&gt; 的增加而增加，正确的值取决于数据集。&lt;/p&gt;
&lt;p&gt;在图 3 和图 4 中，可以分别观察吞吐量（每秒查询次数）和搜索精度（召回率）如何取决于探测器数量。这里，从&lt;a href=&#34;https://research.yandex.com/blog/benchmarks-for-billion-scale-similarity-search#14h2&#34;&gt;DEEP1B 数据集&lt;/a&gt;搜索 100M 个向量，并使用 H100 GPU 进行搜索。&lt;/p&gt;
&lt;p&gt;吞吐量与探针的数量成反比。数据集被分为 10 万个簇；每个查询仅搜索 100 个最接近的簇可实现 96% 的召回率，而搜索 1000 个簇（数据集的 1%）可实现 99.8% 的准确率。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/4.png&#34; alt=&#34;The throughput graph follows 1/x trend.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 3.搜索吞吐量(每秒查询次数)作为n_probes搜索参数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/5.png&#34; alt=&#34;Search accuracy graph shows that recall improves quickly as you increase n_probes from 20 to 200 and flattens out above that (region with 99% recall).&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 4.精度(召回)作为n_probes搜索参数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;通常会将这些图形组合到单个 QPS 与召回图中(图 5)。当想要紧凑地权衡准确性和搜索吞吐量时，这很有用。在比较不同的 ANN 方法时，这也很有用。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/combined-qps-recall.png&#34;&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/6.png&#34; alt=&#34;Graph shows that the QPS drops when you require high recall.&#34;&gt;&lt;/a&gt;&lt;em&gt;图 5.组合 QPS 召回图&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;如果 &lt;code&gt;n_lists=n_probes&lt;/code&gt; 这就像精确(暴力)搜索：将所有数据集向量与所有查询向量进行比较。在这种情况下，预计召回率等于 1 (除了小的舍入误差)。&lt;/p&gt;
&lt;p&gt;当 &lt;code&gt;n_probes&lt;/code&gt; 接近 &lt;code&gt;n_lists&lt;/code&gt; 时，IVF-Flat 由于算法所做的额外工作(粗略加精细搜索)，速度比暴力慢。在实践中，搜索大约 0.1-1%的列表足以处理许多数据集。但这取决于输入的聚类效果。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bib.dbvis.de/uploadedFiles/155.pdf&#34;&gt;On the Surprising Behavior of Distance Metrics in High Dimensional Space&lt;/a&gt;论文中提到，如果数据集没有结构（例如，统一随机数），聚类会变得困难。在这种情况下，IVF 方法的效果不佳。&lt;/p&gt;
&lt;p&gt;（注：IVF-Flat算法依赖输入的数据集，以及聚类效果）&lt;/p&gt;
&lt;h2 id=&#34;性能&#34;&gt;性能&lt;/h2&gt;
&lt;p&gt;RAFT 库可快速实施 IVF-Flat 算法。索引 1 亿个向量可在一分钟内完成(图 6)。这比使用 CPU 快 14 倍。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/7.png&#34; alt=&#34;Bar chart showing high index building time on the CPU and significantly faster times with GPU implementations.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 6.不同数据集和簇大小的索引构建时间&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;我们在 NVIDIA H100 SXM GPU (使用 RAFT 23.10 进行 GPU 测试)和 Intel Xeon Platinum 8480CL CPU 上执行了&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;FAISS&lt;/a&gt; 1.7.4 的测量。&lt;/p&gt;
&lt;p&gt;实现这种加速有两个主要因素：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU 的高计算吞吐量&lt;/strong&gt;：RAFT 利用 &lt;a href=&#34;https://blog.paperspace.com/understanding-tensor-cores/&#34;&gt;Tensor Core&lt;/a&gt; 在索引构建期间加速 k-means 聚类。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;改进的算法&lt;/strong&gt;：RAFT 使用平衡的分层 k-means 聚类，即使数据集的向量数量达到数亿，也能高效地进行聚类。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;还可以观察到，构建索引的时间随向量数量线性增加，随聚类数量线性增加。&lt;/p&gt;
&lt;p&gt;GPU 的高内存吞吐量有助于搜索索引。RAFT 的 IVF-Flat 索引使用优化的内存布局。向量交错以进行向量化内存访问，以确保在遍历每个探测簇中的数据集向量时实现高带宽利用率。&lt;/p&gt;
&lt;p&gt;精细搜索过程中的另一个重要步骤是过滤掉前 k 个候选项。最新top-k算法on GPU论文 &lt;a href=&#34;https://sc23.supercomputing.org/presentation/?id=pap294&amp;amp;sess=sess156&#34;&gt;&lt;strong&gt;Parallel Top-K Algorithms on GPU: A Comprehensive Study and New Methods&lt;/strong&gt;&lt;/a&gt;。将优化的 block-select-k 内核融合到距离计算内核中。如图 7 所示，与 CPU 实现的性能相比，这可以将 RAFT IVF-Flat 的速度提高 20 倍以上（回顾值=0.95）。&lt;/p&gt;
&lt;p&gt;（&lt;strong&gt;注&lt;/strong&gt;：top-k 这篇论文期待一下；和&lt;a href=&#34;https://github.com/weedge/learn/blob/main/gpu/Efficient%20Top-K%20Query%20Processing%20on%20Massively%20Parallel%20Hardware.pdf&#34;&gt;Efficient Top-K Query Processing on Massively Parallel Hardware&lt;/a&gt; 对比学习下~）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/8.png&#34; alt=&#34;Graph compares IVF-Flat search throughput on the GPU and on the CPU.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 7.不同召回率(准确性)的搜索吞吐量&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;在此基准测试中，我们使用了 FAISS IVF-Flat 的 CPU 实现。FAISS 还提供了此算法的 GPU 实现。如果使用 FAISS，则只需对代码进行细微更改即可从 GPU 加速中受益。与 Meta 合作，将 RAFT 的性能改进引入 FAISS，因此很快也可以通过 FAISS 使用 RAFT。&lt;/p&gt;
&lt;p&gt;（&lt;strong&gt;注&lt;/strong&gt;： 相关PR见Reference,  IVF-Flat 已集成）&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在大型数据库中执行向量搜索时，务必要注意精确搜索的高昂成本，因为这会导致不适合在线服务的低延迟。&lt;/p&gt;
&lt;p&gt;RAPIDS RAFT 库提供了高效的算法，通过将搜索集中到数据集中最相关的部分，以提高向量搜索的延迟和吞吐量。本文讨论了 RAFT IVF-Flat 算法的工作原理，以及如何设置索引构建和搜索的参数。最后，提供基准测试，以强调 GPU 在 IVF – Flat 搜索中的卓越性能。可以使用&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/raft_ann_benchmarks/&#34;&gt;基准测试工具&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/&#34;&gt;https://developer.nvidia.com/blog/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rapidsai/raft/blob/a1002f8c8f4debc52fbab7191297a2f54ff42856/notebooks/ivf_flat_example.ipynb&#34;&gt;https://github.com/rapidsai/raft/blob/a1002f8c8f4debc52fbab7191297a2f54ff42856/notebooks/ivf_flat_example.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rapidsai/raft/blob/a1002f8c8f4debc52fbab7191297a2f54ff42856/cpp/template/src/ivf_flat_example.cu&#34;&gt;https://github.com/rapidsai/raft/blob/a1002f8c8f4debc52fbab7191297a2f54ff42856/cpp/template/src/ivf_flat_example.cu&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rapidsai/raft/blob/branch-23.12/docs/source/vector_search_tutorial.md&#34;&gt;https://github.com/rapidsai/raft/blob/branch-23.12/docs/source/vector_search_tutorial.md&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://bib.dbvis.de/uploadedFiles/155.pdf&#34;&gt;On the Surprising Behavior of Distance Metrics in High Dimensional Space&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://sc23.supercomputing.org/presentation/?id=pap294&amp;amp;sess=sess156&#34;&gt;&lt;strong&gt;Parallel Top-K Algorithms on GPU: A Comprehensive Study and New Methods&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kdnuggets.com/2018/05/wtf-tensor.html&#34;&gt;https://www.kdnuggets.com/2018/05/wtf-tensor.html&lt;/a&gt; ， &lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor&#34;&gt;https://en.wikipedia.org/wiki/Tensor&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/understanding-tensor-cores/&#34;&gt;https://blog.paperspace.com/understanding-tensor-cores/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/pull/2521&#34;&gt;https://github.com/facebookresearch/faiss/pull/2521&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/pull/2707&#34;&gt;https://github.com/facebookresearch/faiss/pull/2707&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/pull/3044&#34;&gt;https://github.com/facebookresearch/faiss/pull/3044&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：加速向量搜索：微调 GPU 索引算法</title>
      <link>https://weedge.github.io/post/gpu/2.accelerating-vector-search-fine-tuning-gpu-index-algorithms/</link>
      <pubDate>Fri, 03 Nov 2023 14:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/2.accelerating-vector-search-fine-tuning-gpu-index-algorithms/</guid>
      
        <description>&lt;p&gt;这个 &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/accelerating-vector-search-using-gpu-accelerated-indexes-with-rapids-raft/&#34;&gt;系列的第一篇文章&lt;/a&gt; 介绍了向量搜索索引，解释了它们在实现广泛的重要应用中所起的作用，并使用了 &lt;a href=&#34;https://github.com/rapidsai/raft&#34;&gt;RAFT&lt;/a&gt; 库。&lt;/p&gt;
&lt;p&gt;在这篇文章中，我们深入探讨第 1 部分中提到的每种 GPU 加速索引方法，并简要解释了算法的工作原理，以及总结重要的微调参数。&lt;/p&gt;
&lt;p&gt;然后，我们通过一个简单的端到端示例，用预训练的大型语言模型演示 RAFT 在问答问题上的 Python API，并在涉及同时传递给搜索算法的不同查询向量数量的几个不同场景下，将 RAFT 的算法与 HNSW 的性能进行比较。&lt;/p&gt;
&lt;p&gt;内容如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可与 GPU 一起使用的向量搜索索引算法概述&lt;/li&gt;
&lt;li&gt;一个端到端的例子演示了使用 Python 在 GPU 上运行向量搜索是多么容易&lt;/li&gt;
&lt;li&gt;GPU 上的向量搜索与 CPU 上当前最先进的 HNSW 方法的性能比较&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;向量搜索索引&#34;&gt;向量搜索索引&lt;/h2&gt;
&lt;p&gt;使用向量搜索时，向量通常会转换为索引格式，该格式针对快速查找进行了优化。选择正确的索引算法很重要，因为它会影响索引构建和搜索时间。此外，每种不同的索引类型都有自己的一组参数，用于微调行为、权衡索引构建时间、存储成本、搜索质量和搜索速度。&lt;/p&gt;
&lt;p&gt;当正确的索引算法与正确的参数设置配对时， GPU 上的向量搜索为各种召回率提供了更快的构建和搜索时间。&lt;/p&gt;
&lt;h3 id=&#34;ivf-flat&#34;&gt;IVF-Flat&lt;/h3&gt;
&lt;p&gt;由于它是最简单的索引类型，我们建议您从 &lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#ivf-flat&#34;&gt;IVF-Flat 算法&lt;/a&gt; 开始。在该算法中，一组训练向量首先被分割成一些聚类，然后被存储在由它们最近的聚类中心组织的 GPU 内存中。索引构建步骤比本文中介绍的其他算法更快，即使在大量簇的情况下也是如此。&lt;/p&gt;
&lt;p&gt;为了搜索 IVF-Flat 索引，选择与每个查询向量最近的聚类，并根据这些最接近的聚类中的每个来计算 k 个最近邻居（k-NN）。因为 IVF-Flat 以原始形式存储向量，意味着没有压缩，它的优势是计算它搜索的每个簇内的精确距离。正如我们稍后在本文中所描述的，当搜索相同数量的最接近聚类时，这提供了一个优势，通常比 IVF-PQ 具有更高的召回率。当 GPU 内存可以容纳全量索引时，IVF-Flat 索引是一个很好的选择。&lt;/p&gt;
&lt;p&gt;RAFT 的 IVF-Flat 索引包含几个参数，有助于权衡查询性能和准确性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;训练索引时，n_lists参数确定用于对训练数据集进行分区的簇的数量。&lt;/li&gt;
&lt;li&gt;搜索参数n_probes确定要搜索的最近聚类的数量，以计算一组查询点的最近邻居。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一般来说，较少的探针数量会以召回为代价，导致搜索速度更快。当探测器的数量设置为列表的数量时，会计算出确切的结果。但是，在这种情况下，&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_brute_force/&#34;&gt;RAFT 的暴力搜索&lt;/a&gt;表现更佳。&lt;/p&gt;
&lt;h3 id=&#34;ivf-pq&#34;&gt;IVF-PQ&lt;/h3&gt;
&lt;p&gt;当数据集变得太大而无法放在 GPU 上时，您可以通过使用 &lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#ivf-pq&#34;&gt;IVF-PQ 索引类型&lt;/a&gt; 来解决。与 IVF Flat 一样，IVF-PQ 将点划分为多个簇（也由 n_lists 参数指定）并搜索最近的聚类以计算最近的邻居（也由 n_probes 参数指定），但它使用一种称为 &lt;a href=&#34;https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/&#34;&gt;product quantization&lt;/a&gt; 的方法。&lt;/p&gt;
&lt;p&gt;压缩索引最终允许在 GPU 上存储更多的向量。压缩量可以通过调整参数来控制，我们稍后将对此进行描述，但更高级别的压缩可以以调用为代价提供更快的查找时间。IVF-PQ 是目前 RAFT 最具内存效率的向量索引。&lt;/p&gt;
&lt;p&gt;RAFT 的 IVF-PQ 提供了两个控制内存使用的参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pq_dim 设置压缩向量的目标维度。&lt;/li&gt;
&lt;li&gt;pq_bits 设置压缩后每个向量元素的位数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;建议将前者设置为 32 的倍数，而后者限制在 4-8 位范围内。默认情况下，RAFT 根据 来选择最小化量化损失的维度值&lt;code&gt;pq_bits&lt;/code&gt;，但可以调整该值以降低每个向量的内存占用量。&lt;/p&gt;
&lt;p&gt;当使用大量压缩时，&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#candidate-refinement&#34;&gt;refinement step&lt;/a&gt; 可以通过在 IVF-PQ 索引中查询比所需数量更多的邻居，并计算对结果邻居的精确搜索，以将集合减少到最终期望的数量来执行。细化步骤需要主机内存上的原始未压缩数据集。&lt;/p&gt;
&lt;p&gt;想要获取更多关于构建 IVF-PQ 索引的信息，以及深入的细节和建议，请参阅在 GitHub 存储库上的 &lt;a href=&#34;https://github.com/rapidsai/raft/blob/branch-23.08/notebooks/tutorial_ivf_pq.ipynb&#34;&gt;RAFT IVF-PQ 完整指南&lt;/a&gt; 笔记本 。&lt;/p&gt;
&lt;h3 id=&#34;cagra&#34;&gt;CAGRA&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.rapids.ai/api/raft/nightly/pylibraft_api/neighbors/#cagra&#34;&gt;CAGRA&lt;/a&gt; 是 RAFT 最新的、最先进的 ANN索引。这是一种高性能的、 GPU 加速的、基于图的方法，专门针对小批量情况进行了优化，其中每次查找只包含一个或几个查询向量。与其他流行的基于图的方法一样，如分层可导航小世界图（HNSW）和 SONG，优化的 k-NN 图是在索引训练时构建的，具有各种qualities，可以在合理的召回水平上产生有效的搜索。&lt;/p&gt;
&lt;p&gt;CAGRA 通过首先从图中随机选择候选顶点，然后扩展或遍历这些顶点来计算到其子顶点的距离，并在搜索过程中存储最近的邻居来执行搜索（图 1）。每次遍历一组顶点时，它都会执行一次迭代。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerating-vector-search-fine-tuning-gpu-index-algorithms/1.png&#34; alt=&#34;Diagram shows how CAGRA can map subgraphs to separate thread blocks, enabling parallelism even for a single query.&#34;&gt;&lt;em&gt;图 1. 使用多个线程块的 CAGRA&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;在图 1 中，CAGRA 使用多个线程块并行访问更多的图节点。这最大限度地提高了单查询搜索的 GPU 利用率。&lt;/p&gt;
&lt;p&gt;因为 CAGRA 像前面描述的算法一样返回近似的最近邻居，所以它还提供了一些参数来控制召回和速度。&lt;/p&gt;
&lt;p&gt;可以调整以权衡搜索速度的主要参数是&lt;code&gt;itopk_size&lt;/code&gt;，指定内部排序列表的大小，该列表存储可以在下一次迭代中探索的节点。&lt;code&gt;itopk_size&lt;/code&gt;的较高值在内存中保留更大的搜索上下文，这会以花费更多时间维护队列为代价来提高召回率。&lt;/p&gt;
&lt;p&gt;参数&lt;code&gt;search_width&lt;/code&gt;定义在每次搜索迭代中为展开其子顶点而遍历的最近父顶点的数目。&lt;/p&gt;
&lt;p&gt;另一个有用的参数是 执行的迭代次数。默认情况下，该设置是自动选择的，但可以将其更改为更高或更低的值，以换取更快的搜索。&lt;/p&gt;
&lt;p&gt;CAGRA 的优化图是固定的度(fixed-degree)，使用参数&lt;code&gt;graph_degree&lt;/code&gt;进行调整。通过在搜索图时保持计算次数的一致性来更好地利用 GPU 资源。它通过计算实际的 k-NN 来构建初始 k-NN 图，例如通过使用前面解释的 IVF-PQ 来计算训练数据集中所有点的最近邻居。&lt;/p&gt;
&lt;p&gt;k-NN 可以使用参数&lt;code&gt;intermediate_graph_degree&lt;/code&gt;，来权衡最终可搜索的 CAGRA 图的质量。&lt;/p&gt;
&lt;p&gt;可以使用更大的&lt;code&gt;intermediate_graph_degree&lt;/code&gt;，这意味着最终优化的图更有可能找到产生高召回率的最近邻居。RAFT 提供了几个有用的参数来调整 CAGRA 算法。想了解更多详细信息，请参阅&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_cagra/&#34;&gt;CAGRA API 文档&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;同样，这个参数可以用来控制搜索对整个空间的覆盖程度，但这也是以必须进行更多搜索才能找到最近的邻居为代价的，这降低了搜索性能。&lt;/p&gt;
&lt;h2 id=&#34;pylibraft-入门&#34;&gt;pylibraft 入门&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.rapids.ai/api/raft/nightly/pylibraft_api&#34;&gt;Pylibraft&lt;/a&gt; 是 RAFT 的轻量级 Python 库，使您能够使用 RAFT 的 ANN 算法在 Python 中进行向量搜索。Pylibraft 可以接受任何支持 &lt;strong&gt;CUDA_array_interface&lt;/strong&gt; 的库，例如 &lt;a href=&#34;https://pytorch.org/&#34;&gt;Torch&lt;/a&gt; 或 &lt;a href=&#34;https://cupy.dev/&#34;&gt;CuPy&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;以下示例简要演示了如何使用 Pylibraft 构建和查询 RAFT CAGRA 索引。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pylibraft.neighbors&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cagra&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cupy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cp&lt;/span&gt;
 
&lt;span class=&#34;c1&#34;&gt;# On small batch sizes, using &amp;#34;multi_cta&amp;#34; algorithm is efficient&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;index_params&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cagra&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;IndexParams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph_degree&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;search_params&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cagra&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SearchParams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;algo&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;multi_cta&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;corpus_embeddings&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1500&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;96&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;float32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;query_embeddings&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;96&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;float32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;cagra_index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cagra&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;build&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;corpus_embeddings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# Find the 10 closest vectors&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;hits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cagra&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;search&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;search_params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cagra_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query_embeddings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;随着 LLM 的出现，语义搜索已经成为展示使用 RAFT 的向量相似性搜索的完美方式。在以下示例中，&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/distilbert&#34;&gt;DistilBERT&lt;/a&gt; transformer 模型与三个神经网络指标中的每一个指标相结合，用于解决一个简单的问题检索问题。简单英语维基百科数据集用于回答用户的搜索查询。&lt;/p&gt;
&lt;p&gt;语言模型首先将训练语句转换为插入 RAFT ANN 索引的向量嵌入。推理是通过对查询进行编码并使用我们训练的 ANN 索引来找到与编码的查询向量相似的向量来完成的。返回给用户的答案是简单维基百科中最近的文章，你可以使用相似性搜索中最接近的向量来获取它。&lt;/p&gt;
&lt;p&gt;可以使用 pylibraft 和本笔记本开始 RAFT 的问题检索任务：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/lowener/08eef6aca69cae5c2151224c801521b0&#34;&gt;https://gist.github.com/lowener/08eef6aca69cae5c2151224c801521b0&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;基准&#34;&gt;基准&lt;/h2&gt;
&lt;p&gt;使用 GPU 作为向量搜索应用程序的硬件加速器可以提高性能，这在大型数据集上表现得尤为明显。可以通过参考 &lt;a href=&#34;https://docs.rapids.ai/api/raft/nightly/raft_ann_benchmarks/&#34;&gt;RAFT 的端到端基准测试文档&lt;/a&gt;来完全复制这个基准。我们的基准测试假设数据已经可以用于计算，这意味着数据传输没有被考虑在内，尽管由于最近 NVIDIA 硬件的高传输速度（超过 25 GB/s），这应该不会造成显著的差异。&lt;/p&gt;
&lt;p&gt;使用在 H100 GPU 上的 &lt;a href=&#34;http://big-ann-benchmarks.com/neurips21.html&#34;&gt;DEEP-100M&lt;/a&gt; 数据集，以将 RAFT 索引与运行在 Intel Xeon Platinum 8480CL CPU 上的 HNSW 进行比较。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerating-vector-search-fine-tuning-gpu-index-algorithms/2.png&#34; alt=&#34;Bar chart compares the throughput of HNSW, the state-of-the-art on CPU, against RAFT’s ANN algorithms for a single query at a time at various levels of recall.&#34;&gt;&lt;em&gt;图 2:比较不同召回率和吞吐量的 ANN 算法，批次为 1&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;图 2 比较了单个查询在不同召回率和吞吐量下的 ANN 算法。在高召回率下，RAFT 的方法显示出比其他替代库更高的吞吐量。&lt;/p&gt;
&lt;p&gt;我们一次对单个向量的查询进行性能比较，称为&lt;em&gt;在线搜索&lt;/em&gt;。它是向量搜索的主要用例之一。与使用 CPU 或 GPU 的其他库相比，基于 RAFT 的索引提供了更高的吞吐量（以每秒查询量（QPS）衡量）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerating-vector-search-fine-tuning-gpu-index-algorithms/3.png&#34; alt=&#34;Bar chart compares throughput for HNSW, the state-of-the-art on CPU, against RAFT’s ANN algorithms for a query of 10 vectors at a time.&#34;&gt;&lt;em&gt;图 3。比较不同召回率和吞吐量的ANN算法，批次为 10&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;图 3 比较了不同召回和吞吐量级别的 ANN 算法与 10 个查询的批量大小。RAFT 的方法在所有实验中都表现出比 HNSW 更高的吞吐量。&lt;/p&gt;
&lt;p&gt;将 GPU 用于向量搜索应用程序的好处在更大的批量中最为普遍。 CPU 和 GPU 之间的性能差距很大，可以很容易地扩大规模。图 3 显示，对于批量大小为 10 的情况，当比较每秒的查询数时，只有基于 RAFT 的索引是最优的。对于 10k 的批量大小（图 4），CAGRA 迄今为止优于所有其他索引。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerating-vector-search-fine-tuning-gpu-index-algorithms/4.png&#34; alt=&#34;Bar chart compares throughput for HNSW, the state-of-the-art on CPU, against RAFT’s ANN algorithms for a query of 10k vectors at a time.&#34;&gt;&lt;em&gt;图 4。比较不同召回率和吞吐量的ANN算法，批次10k&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;图 4 将不同召回和吞吐量级别的 ANN 算法与批量大小为 10K 的查询进行了比较。RAFT 的方法在所有实验中都表现出比 HNSW 更高的吞吐量。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;每种不同的向量搜索索引类型都有优缺点，最终取决于场景需求。这篇文章概述了其中的一些优点和缺点，简要解释了每种不同算法的工作原理，以及一些最重要的参数，这些参数可以用来权衡存储成本、构建时间、搜索质量和搜索性能。在所有情况下， GPU 都可以提高索引构建和搜索性能。&lt;/p&gt;
&lt;p&gt;（&lt;strong&gt;注&lt;/strong&gt;：使用GPU的成本更高，需要衡量；如果可以满足当前召回率和吞吐性能场景，第一选择是成本低的 CPU下的HNSW索引，并且可以通过SIMD进行调优；而在一些对性能、延迟有着极高要求的场景，只通过 CPU 索引来支撑的难度越来越高，而 GPU 有着非常强大的并行处理能力，CPU+GPU可以作为构建索引时的第二选择。）&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/&#34;&gt;https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/&#34;&gt;https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rapidsai/raft/blob/branch-23.08/notebooks/tutorial_ivf_pq.ipynb&#34;&gt;https://github.com/rapidsai/raft/blob/branch-23.08/notebooks/tutorial_ivf_pq.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.rapids.ai/api/raft/nightly/raft_ann_benchmarks/&#34;&gt;https://docs.rapids.ai/api/raft/nightly/raft_ann_benchmarks/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：加速向量搜索：利用 GPU 索引的 RAPIDS RAFT</title>
      <link>https://weedge.github.io/post/gpu/1.accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/</link>
      <pubDate>Fri, 03 Nov 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/1.accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/</guid>
      
        <description>&lt;p&gt;在 2023 年的人工智能领域，向量搜索成为最热门的话题之一，因为它在&lt;a href=&#34;https://www.nvidia.cn/glossary/data-science/large-language-models/&#34;&gt;大语言模型&lt;/a&gt;（LLM）和&lt;a href=&#34;https://www.nvidia.cn/glossary/data-science/generative-ai/&#34;&gt;生成式人工智能&lt;/a&gt;中发挥了重要作用。语义向量搜索实现了一系列重要任务，如检测欺诈交易、向用户推荐产品、使用上下文信息增强全文搜索以及查找潜在安全风险的参与者。&lt;/p&gt;
&lt;p&gt;数据量持续飙升，传统的逐一比较的方法在计算上变得不可行。向量搜索方法使用近似查找，这种查找更具可扩展性，可以更有效地处理大量数据。正如我们在这篇文章中所展示的，在 GPU 上加速向量搜索不仅提供了更快的搜索时间，而且索引构建时间也可以更快。&lt;/p&gt;
&lt;p&gt;本文内容如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;向量搜索简介及流行应用综述&lt;/li&gt;
&lt;li&gt;在 GPU 上加速向量搜索的 RAFT 库综述&lt;/li&gt;
&lt;li&gt;GPU 加速向量搜索索引与 CPU 上最新技术的性能比较&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本系列的第二篇文章深入探讨了每一个 GPU 加速指数，并简要解释了算法的工作原理以及微调其行为的重要参数摘要。想要了解更多信息，请访问 &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/&#34;&gt;加速向量搜索：微调 GPU 索引算法&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;什么是向量搜索&#34;&gt;什么是向量搜索？&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/1.png&#34; alt=&#34;Diagram shows a list of vectors that may have been encoded from sources like images, documents, or videos and a query vector for which you would like to find the closest vectors from the list.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 1。向量搜索过程&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;图 1 显示了向量搜索需要创建一个向量索引，并执行查找以在索引中找到一些最接近查询向量的向量。向量可以小到激光雷达点云中的三维点，也可以是文本文档、图像或视频中的较大嵌入向量。&lt;/p&gt;
&lt;p&gt;向量搜索是查询数据库以查找最相似向量的过程。这种相似性搜索是在可以表示任何类型对象的数字向量上进行的（图 2）。这些向量通常是从多媒体（如图像、视频和文本片段）或整个文档中创建的嵌入向量，这些文档经过深度学习模型将其语义特征编码为向量形式。&lt;/p&gt;
&lt;p&gt;嵌入向量通常具有比原始文档更小的对象（维度更低）的优势，同时保持尽可能多的源信息。因此，两个相似的文档通常具有相似的嵌入向量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/2.jpg&#34; alt=&#34;Image of a 3D point cloud such as one created from LIDAR.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 2:向量表示更高维度的数据点&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;图 2 中的点是 3D 的，但它们可能是 500 维甚至更高。&lt;/p&gt;
&lt;p&gt;这使得比较对象更容易，因为嵌入向量更小，并且保留了大部分信息。当两个文档共享相似的特征时，它们的嵌入向量通常在空间上接近或相似。&lt;/p&gt;
&lt;h3 id=&#34;向量搜索的近似方法&#34;&gt;向量搜索的近似方法&lt;/h3&gt;
&lt;p&gt;为了有效地处理较大的数据集，通常使用近似最近邻（ANN）方法进行向量搜索。神经网络方法通过逼近最接近的向量来加快搜索速度。这避免了精确暴力方法通常需要的穷举距离计算，该方法需要将查询与数据库中的每个向量进行比较。&lt;/p&gt;
&lt;p&gt;除了搜索计算成本外，存储许多向量还可能消耗大量内存。为了确保快速搜索和低内存使用率，必须以高效的方式对向量进行索引。正如我们稍后所概述的，这有时可以从压缩中受益。向量索引是建立在数学模型上的一种空间有效的数据结构，用于一次有效地查询多个向量。&lt;/p&gt;
&lt;p&gt;当索引需要数小时甚至数天才能建立时，更新索引（例如插入和删除向量）可能会导致问题。事实证明，这些索引通常可以在 GPU 上更快地构建。我们稍后将在帖子中展示这一表现。&lt;/p&gt;
&lt;h2 id=&#34;llm-中的向量搜索&#34;&gt;LLM 中的向量搜索&lt;/h2&gt;
&lt;p&gt;LLM 由于捕捉和保存原始文档的语义和上下文而变得流行起来。这意味着可以使用&lt;em&gt;向量相似性搜索&lt;/em&gt;。此搜索可查找恰好包含相似单词、形状或移动对象的项目。它还发现了在上下文和语义上意味着相似事物的向量。&lt;/p&gt;
&lt;p&gt;这种语义搜索不依赖于精确的单词匹配。例如，在图像数据库中搜索术语“我想买一辆肌肉车”应该能够将句子置于上下文中，以理解以下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;买车和租车不同，所以你希望找到更接近汽车经销商和购车者评论的载体，而不是租车公司。&lt;/li&gt;
&lt;li&gt;肌肉车不同于健美运动员，所以你会期望找到道奇充电器的向量，而不是阿诺德·施瓦辛格。&lt;/li&gt;
&lt;li&gt;购买肌肉车与购买肌肉放松剂或经济型汽车不同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最近出现了基于大型语言 transformer 的模型，如&lt;a href=&#34;https://www.nvidia.cn/ai-data-science/generative-ai/nemo-framework/&#34;&gt;NeMo&lt;/a&gt;和 BERT，它们提供了重大的技术飞跃，提高了模型的上下文意识，使其更加有用，适用于更多的行业。&lt;/p&gt;
&lt;p&gt;除了创建可以存储和稍后搜索的嵌入向量外，这些新的 LLM 模型还在管道中使用语义搜索，这些管道从通过查找类似向量收集的上下文中生成新内容。如图 3 所示，这个内容生成过程称为&lt;em&gt;检索增强了生成人工智能。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;在向量数据库中使用向量搜索&#34;&gt;在向量数据库中使用向量搜索&lt;/h3&gt;
&lt;p&gt;向量数据库存储高维向量（例如，嵌入向量），并促进基于向量相似性的快速准确搜索和检索（例如，ANN 算法）。一些数据库是专门为向量搜索而构建的（例如 Milvus）。其他数据库包括向量搜索功能作为附加功能（例如 Redis）。&lt;/p&gt;
&lt;p&gt;选择要使用的向量数据库取决于工作流的要求。&lt;/p&gt;
&lt;p&gt;检索增强语言模型允许通过使用已由 LLM 编码为向量并存储在向量数据库中的附加上下文来增强搜索，从而为特定产品、服务或其他领域特定用例定制预训练的模型。&lt;/p&gt;
&lt;p&gt;更具体地说，搜索被编码为向量形式，并且在向量数据库中找到相似的向量以增强搜索。然后将向量与 LLM 一起使用，以形成适当的响应。检索增强 LLM 是生成人工智能的一种形式，它们彻底改变了聊天机器人和语义文本搜索行业。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/3.png&#34; alt=&#34;Workflow diagram shows how vector search is often combined with LLMs to perform semantic search. &#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 3。使用 RAPIDS RAFT 进行向量搜索的文本检索应用程序的示例工作流程&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;向量相似性搜索的其他应用&#34;&gt;向量相似性搜索的其他应用&lt;/h3&gt;
&lt;p&gt;除了用于生成人工智能的检索增强 LLM 之外，嵌入向量已经存在了一段时间，并在现实世界中发现了许多有用的应用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;推荐系统&lt;/strong&gt;：根据用户的兴趣或互动行为，提供个性化建议。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;财务&lt;/strong&gt;：欺诈检测模型将用户交易向量化，从而可以确定这些交易是否与典型的欺诈活动相似。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;网络安全&lt;/strong&gt;：使用嵌入向量对不良行为者和异常活动的行为进行建模和搜索。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基因组学&lt;/strong&gt;：在基因组学分析中，我们可以发现相似的基因和细胞结构，如单细胞 RNA 分析。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;化学&lt;/strong&gt;：对化学结构的分子描述符或指纹进行建模，以便比较它们或在数据库中找到类似的结构。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rapids-raft-向量搜索库&#34;&gt;RAPIDS RAFT 向量搜索库&lt;/h2&gt;
&lt;p&gt;RAFT 是一个可组合构建块库，用于加速 GPU 上的机器学习算法，例如最近邻居和向量搜索中使用的算法。ANN 算法是构成向量搜索库的核心构建块之一。最重要的是，这些算法可以极大地受益于 GPU 加速。&lt;/p&gt;
&lt;p&gt;有关 RAFT 的核心 API 及其包含的各种加速构建块的更多信息，请参阅&lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/&#34;&gt;RAPIDS RAFT 中的机器学习和数据分析的可复用计算模式&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;用于快速搜索的ann&#34;&gt;用于快速搜索的ANN&lt;/h2&gt;
&lt;p&gt;除了&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_brute_force/&#34;&gt;精确搜索的暴力法(Brute-Force)&lt;/a&gt;，RAFT 目前为 ANN 搜索提供了三种不同的算法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_ivf_flat/&#34;&gt;IVF-Flat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_ivf_pq/&#34;&gt;IVF-PQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.rapids.ai/api/raft/nightly/cpp_api/neighbors_cagra/&#34;&gt;CAGRA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;算法的选择可能取决于您的需求，因为它们各自提供不同的优势。有时，暴力甚至是更好的选择。更多内容将在即将发布的版本中添加。&lt;/p&gt;
&lt;p&gt;由于这些算法没有进行精确的搜索，可能会遗漏一些高度相似的向量。这个 &lt;a href=&#34;https://en.wikipedia.org/wiki/Precision_and_recall&#34;&gt;recall&lt;/a&gt; 度量可以用于表示结果中有多少邻居是查询的实际最近邻居。我们的大多数基准都以 85% 及更高的召回率为目标，这意味着检索到 85%（或更多）的相关向量。&lt;/p&gt;
&lt;p&gt;要针对不同的召回级别调整结果索引，请在训练近似近邻算法时使用各种设置或超参数。降低召回率通常会提高搜索速度，而提高召回则会降低搜索速度。这就是所谓的召回-速度权衡。&lt;/p&gt;
&lt;p&gt;想要了解更多信息，请访问 &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/&#34;&gt;加速向量搜索：微调 GPU 索引算法&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&#34;性能比较&#34;&gt;性能比较&lt;/h3&gt;
&lt;p&gt;GPU 擅长一次处理大量数据。当一次计算数千或数万个点的最近邻居时，刚才提到的所有算法都可以优于 CPU 上的相应算法。&lt;/p&gt;
&lt;p&gt;然而，CAGRA 是专门为在线搜索而设计的，这意味着即使一次只查询几个数据点的最近邻居，它的性能也优于 CPU 。&lt;/p&gt;
&lt;p&gt;图 4 和图 5 显示了我们通过在 100M 向量上构建索引并一次只查询 10 个向量来执行的基准测试。在图 4 中，CAGRA 在原始搜索性能方面优于 HNSW，HNSW 是 CPU 上最受欢迎的向量搜索索引之一，即使是 10 个向量的极小批量。然而，这种速度是以内存为代价的。在图 5 中，可以看到 CAGRA 的GPU内存占用比其他近邻方法略高。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/4.png&#34; alt=&#34;Bar chart compares throughput performance (queries per second) for RAFT’s GPU algorithms against HNSW on the CPU.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 4。DEEP-100M 数据集上 95%召回率的向量搜索吞吐量，批量大小为 10&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;在图 5 中，IVF-PQ 的主机内存用于可选的细化步骤。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/5.png&#34; alt=&#34;Bar chart compares memory usage for RAFT’s GPU algorithms against HNSW on the CPU.&#34;&gt;&lt;em&gt;图 5。 GPU 内存使用情况&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;图 6 显示了索引构建时间的比较，并表明索引通常可以在 GPU 上更快地构建。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/6.png&#34; alt=&#34;Bar chart compares index build time for RAFT’s GPU algorithms against HNSW on the CPU.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图 6。召回率为 95%时表现最好的索引的构建时间&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;从特征库到生成人工智能，向量相似性搜索可以应用于每个行业。 GPU 上的向量搜索以较低的延迟执行，并为在线和批量处理的每一级调用实现更高的吞吐量。&lt;/p&gt;
&lt;p&gt;RAFT 是一组可组合的构建块，可用于加速任何数据源中的向量搜索。它为 Python 和 C++预先构建了 API。Milvus、Redis 和 FAISS 的 RAFT 集成正在进行中。&lt;/p&gt;
&lt;p&gt;（&lt;strong&gt;注&lt;/strong&gt;：相关PR见reference连接）&lt;/p&gt;
&lt;p&gt;除了最先进的 ANN 算法，RAFT 还包含了如矩阵和向量运算、迭代求解器和聚类算法等其他 GPU 加速构建块。在本系列的第二篇文章中，我们深入探讨了每种 GPU 加速索引方法，并简要解释了算法的工作原理，以及总结重要的微调参数。有关详细信息，请参阅 &lt;a href=&#34;https://developer.nvidia.cn/zh-cn/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/&#34;&gt;加速向量搜索：微调 GPU 索引算法&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;了解RAPIDS RAFT中的三种不同算法：&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_ivf_flat/&#34;&gt;IVF-Flat&lt;/a&gt;，&lt;a href=&#34;https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_ivf_pq/&#34;&gt;IVF-PQ&lt;/a&gt;，&lt;a href=&#34;https://docs.rapids.ai/api/raft/nightly/cpp_api/neighbors_cagra/&#34;&gt;CAGRA&lt;/a&gt;, 并且调用RAPIDS RAFT 向量搜索库的相关接口工程实践下；&lt;/li&gt;
&lt;li&gt;借鉴RAPIDS RAFT 库在 Milvus、Redis 和 FAISS 中的集成；&lt;/li&gt;
&lt;li&gt;将开源改吧改吧，结合业务场景，某种商业目的变成闭源，然后美其名曰自研，岂不“妙哉”;)&lt;/li&gt;
&lt;li&gt;学废了，然后吐出来，当个搬运工，换个说法，看是否可以换些银子；&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/&#34;&gt;https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rapidsai/raft#what-is-raft&#34;&gt;https://github.com/rapidsai/raft#what-is-raft&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/RedisAI/VectorSimilarity/pull/413&#34;&gt;https://github.com/RedisAI/VectorSimilarity/pull/413&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/milvus-io/knowhere/pull/712&#34;&gt;https://github.com/milvus-io/knowhere/pull/712&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/milvus-io/knowhere/pull/953&#34;&gt;https://github.com/milvus-io/knowhere/pull/953&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/zilliztech/knowhere/pull/10&#34;&gt;https://github.com/zilliztech/knowhere/pull/10&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/pull/2521&#34;&gt;https://github.com/facebookresearch/faiss/pull/2521&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/pull/2707&#34;&gt;https://github.com/facebookresearch/faiss/pull/2707&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/pull/3044&#34;&gt;https://github.com/facebookresearch/faiss/pull/3044&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：利用 GPU 上的大规模并行hashmap最大限度地提高性能</title>
      <link>https://weedge.github.io/post/gpu/maximizing-performance-with-massively-parallel-hash-maps-on-gpus/</link>
      <pubDate>Thu, 02 Nov 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/gpu/maximizing-performance-with-massively-parallel-hash-maps-on-gpus/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/maximizing-performance-with-massively-parallel-hash-maps-on-gpus/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;数十年的计算机科学历史一直致力于设计有效存储和检索信息的解决方案。hashmap（或hashtable）是一种流行的信息存储数据结构，因为它们可以保证元素插入和检索的恒定时间。&lt;/p&gt;
&lt;p&gt;然而，尽管hashmap很流行，但很少在 GPU 加速计算的背景下进行讨论。虽然 GPU 以其大量线程和计算能力而闻名，但其极高的内存带宽可以加速许多数据结构（例如hashmap）。&lt;/p&gt;
&lt;p&gt;这篇文章将介绍哈hashmap的基础知识以及它们的内存访问模式如何使其非常适合 GPU 加速。我们将介绍&lt;a href=&#34;https://github.com/NVIDIA/cuCollections&#34;&gt;cuCollections&lt;/a&gt;，这是一个用于并发数据结构（包括hashmap）的新开源 CUDA C++ 库。&lt;/p&gt;
&lt;p&gt;最后，如果有兴趣在应用程序中使用 GPU 加速的哈希表，我们提供了多列关系连接算法的示例实现case。RAPIDS cuDF 集成了 GPU 哈希表，这有助于为数据科学工作负载实现令人难以置信的加速。要了解更多信息，请参阅GitHub 上的&lt;a href=&#34;https://github.com/rapidsai/cudf&#34;&gt;rapidsai/cudf&lt;/a&gt;; 以及使用示例case &lt;a href=&#34;https://medium.com/rapids-ai/accelerating-tf-idf-for-natural-language-processing-with-dask-and-rapids-6f6e416429df&#34;&gt;使用 Dask 和 RAPIDS 加速 TF-IDF 进行自然语言处理&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;还可以将 cuCollections 用于表格数据处理之外的许多用例，例如推荐系统、流压缩、图形算法、基因组学和稀疏线性代数运算。请参阅&lt;a href=&#34;https://blogs.nvidia.com/blog/2022/08/04/pinterest-gpu-acceleration-recommenders/&#34;&gt;Pinterest 通过切换推荐系统的 GPU 加速将主页订阅参与度提高 16%&lt;/a&gt;了解更多信息。&lt;/p&gt;
&lt;h2 id=&#34;hash-map基础知识&#34;&gt;Hash map基础知识&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Hash_table&#34;&gt;Hashmap&lt;/a&gt;是关联(&lt;em&gt;associative&lt;/em&gt;)容器，这意味着它们存储pair&amp;lt;key,val&amp;gt;对，其中keymap到关联val，从而可以通过查找key来检索val。例如，可以使用hashmap来实现电话簿，方法是使用个人姓名作为key，使用电话号码作为关联值。&lt;/p&gt;
&lt;p&gt;Hashmap与其他关联容器的不同之处在于，插入或检索等操作的平均成本是恒定的。&lt;a href=&#34;https://en.cppreference.com/w/cpp/container/map&#34;&gt;&lt;code&gt;std::map&lt;/code&gt;&lt;/a&gt;C++ 标准模板库中的map不是hashtable，而是通常以二叉搜索树的形式实现。&lt;a href=&#34;https://en.cppreference.com/w/cpp/container/unordered_map&#34;&gt;&lt;code&gt;std::unordered_map&lt;/code&gt;&lt;/a&gt;更类似于与此讨论相关的hashtable。就本文而言，hashtable和hashmap之间没有区别。这两个术语将在全文中互换使用。&lt;/p&gt;
&lt;h3 id=&#34;单值与多值比较&#34;&gt;单值与多值比较&lt;/h3&gt;
&lt;p&gt;讨论哈希表时的一个重要区别是是否允许重复key。单值哈希表或hashmap要求key是唯一的（例如，&lt;code&gt;std::unordered_map&lt;/code&gt;），而多值哈希表或哈希多重map允许重复的key（例如，&lt;code&gt;std::unordered_multimap&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;使用电话簿类比，后者指的是一个人可以拥有多个电话号码的情况。例如，电话簿可能具有(k=Alice, v=408-555-0148)和具有另一个值(k=Alice, v=408-555-3847) 的重复key。&lt;/p&gt;
&lt;h3 id=&#34;存储和检索&#34;&gt;存储和检索&lt;/h3&gt;
&lt;p&gt;从概念上讲，哈希表由一组桶组成，其中每个桶可以保存一个或多个key值对。为了将新的对插入到map中，对key应用哈希函数以产生哈希值。然后使用该哈希值来选择其中一个存储桶。如果存储桶可用，则该对存储在该存储桶中。&lt;/p&gt;
&lt;p&gt;例如，要插入对(Alice, 408-555-0148)，可以对keyhash(Alice)=4 进行哈希处理，以获取其哈希值，并选择位置 4 处的存储桶来存储该对。稍后，要检索与Alice关联的值，可以使用相同的哈希函数hash(Alice)再次选择位置 4 处的存储桶并检索之前存储的值。&lt;/p&gt;
&lt;h3 id=&#34;哈希冲突&#34;&gt;哈希冲突&lt;/h3&gt;
&lt;p&gt;如果表中的桶的数量等于可能的key的数量，则可以采用散列桶和key之间的一对一关系，其中每个key恰好map到表中的一个桶。&lt;/p&gt;
&lt;p&gt;然而，这在大多数情况下是不切实际的，因为事先不知道潜在key的数量，或者为每个key保留存储桶所需的存储空间将超出可用内存容量。想象一下，如果的电话簿必须为宇宙中每个可能的名字保留一个条目！&lt;/p&gt;
&lt;p&gt;因此，哈希函数通常不完善，可能会导致哈希冲突，即两个不同的keymap到相同的哈希值（图 1）。好的哈希函数会尽量减少冲突的可能性，但在大多数情况下它们是不可避免的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday//maximizing-performance-with-massively-parallel-hash-maps-on-gpus/hash-collision-diagram.png&#34; alt=&#34;显示存储桶四中的哈希冲突的图表。 灰色插槽表示已被占用的插槽。 &#34;&gt;&lt;em&gt;图 1. 两个不同的key（Alice 和 Bob)具有相同的哈希值，导致存储桶 4 处发生哈希冲突&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;开放寻址&#34;&gt;开放寻址&lt;/h3&gt;
&lt;p&gt;在文献中可以找到许多解决哈希冲突的策略，但本文重点介绍一种称为线性探测(&lt;strong&gt;linear probing&lt;/strong&gt;)的开放寻址策略。&lt;/p&gt;
&lt;p&gt;开放寻址哈希表使用内存中连续的存储桶数组。使用线性探测，如果在位置 i 遇到已占用的存储桶，则移动到下一个相邻位置i+1。如果这个存储桶也被占用，则移动到i+2，依此类推。当到达最后一个桶时，将回到起点。这种探测方案对于每个key都是确定性的（图 2）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday//maximizing-performance-with-massively-parallel-hash-maps-on-gpus/linear-probing-strategy-diagram.png&#34; alt=&#34;该图显示了两个不同key的两个哈希值相同时的线性探测策略&#34;&gt;&lt;em&gt;图 2. 开放寻址通过按确定性顺序遍历一系列替代存储桶的探测方案将冲突条目存储在不同位置&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;这种方法的缓存效率很高，因为它访问内存中的连续位置。如果负载因子（已填充的存储桶与总存储桶的比率）较高，则可能会导致性能下降 ，因为这会导致额外的内存读取。&lt;/p&gt;
&lt;p&gt;从hashmap中检索key Bob 的工作方式相同：从位置hash(Bob)=4开始遵循key的探测序列，直到在位置 6 处找到所需的存储桶。如果在给定key的探测序列中的任何点遇到空存储桶，则知道所查询的key不存在于hashmap中。&lt;/p&gt;
&lt;h2 id=&#34;随机存储器访问&#34;&gt;随机存储器访问&lt;/h2&gt;
&lt;p&gt;精心设计的散列函数通过最大化散列任意两个key产生不同散列值的可能性来最小化冲突次数。这意味着对于任何给定的两个key，它们对应的存储桶可能位于不同的内存位置。&lt;/p&gt;
&lt;p&gt;因此，大多数哈希表操作的内存访问模式实际上是随机的。要理解哈希表的性能，了解随机内存访问的性能非常重要。&lt;/p&gt;
&lt;p&gt;表 1 将理论峰值带宽与在现代 CPU 和 GPU 上 通过&lt;a href=&#34;https://icl.utk.edu/projectsfiles/hpcc/RandomAccess/&#34;&gt;GUPS 基准测试&lt;/a&gt;测量的随机 64 位读取所实现的带宽进行了比较。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;芯片（内存）&lt;/th&gt;
&lt;th&gt;理论峰值带宽（GB/s）&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;测量的随机 64 位读取带宽 (GB/s)&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/products/sku/212459/intel-xeon-platinum-8360y-processor-54m-cache-2-40-ghz/specifications.html&#34;&gt;英特尔至强铂金 8360Y&lt;/a&gt;（DDR4-3200，8 通道）&lt;/td&gt;
&lt;td&gt;204&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NVIDIA A100-80GB-SXM (HBM2e)&lt;/td&gt;
&lt;td&gt;2039&lt;/td&gt;
&lt;td&gt;141&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NVIDIA H100-80GB-SXM (HBM3)&lt;/td&gt;
&lt;td&gt;3352&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;表 1. 带宽的计算方式为访问大小乘以访问次数除以时间&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;如果有兴趣在系统上运行 GUPS GPU 基准测试，请参阅&lt;a href=&#34;https://github.com/NVIDIA-developer-blog/code-samples/tree/master/posts/gups&#34;&gt;NVIDIA 开发人员博客代码示例&lt;/a&gt;GitHub 存储库。可以在&lt;a href=&#34;https://github.com/ParRes/Kernels&#34;&gt;ParRes/Kernels&lt;/a&gt; GitHub 存储库中访问 CPU 代码。&lt;/p&gt;
&lt;p&gt;正如所看到的，随机内存访问比理论峰值带宽大约慢 10 倍。这是因为内存子系统针对顺序访问进行了优化。更重要的是，NVIDIA GPU 的随机访问吞吐量比现代 CPU 高出一个数量级。这些结果表明，性能最佳的 CPU 哈希表可能比性能最佳的 GPU 哈希表慢一个数量级。&lt;/p&gt;
&lt;h2 id=&#34;gpu哈希表实现&#34;&gt;GPU哈希表实现&lt;/h2&gt;
&lt;p&gt;随机内存访问在哈希表实现中是不可避免的，与 CPU 相比，GPU 在随机访问方面表现出色。这是有希望的，因为它暗示 GPU 应该擅长哈希表操作。为了测试这一理论，本节讨论 GPU 哈希表的实现和优化，并将性能与 CPU 实现进行比较。&lt;/p&gt;
&lt;p&gt;我们的目标不是开发标准 C++ 容器直接替代品（例如 &lt;code&gt;std::unordered_map&lt;/code&gt;)，而是专注于实现适合 GPU 加速应用程序中出现的大规模并行、高吞吐量问题的哈希表。&lt;/p&gt;
&lt;p&gt;此示例使用以下简化假设：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;表的容量是固定的——不能在初始容量之外添加额外的key值对&lt;/li&gt;
&lt;li&gt;需要将其中一些key values留作哨兵值以表示空桶&lt;/li&gt;
&lt;li&gt;key value类型的大小之和必须小于或等于八个字节&lt;/li&gt;
&lt;li&gt;Key-value对一旦插入就无法删除&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;请注意，这些不是基本限制，可以通过 cuCollections 库中提供的更高级的实现来克服。&lt;/p&gt;
&lt;p&gt;首先，示例哈希表使用开放寻址并由存储桶数组组成。每个存储桶可以容纳一个key值对，并使用key/值标记进行初始化以表示它当前为空。对于碰撞解决， 使用线性探测。&lt;/p&gt;
&lt;p&gt;GPU 加速的哈希表需要支持来自多个线程的并发更新，并且有必要采取措施避免数据竞争，例如，如果两个线程尝试在同一位置插入。为了避免昂贵的锁定，示例哈希表使用原子操作，其中使用&lt;code&gt;libcu++&lt;/code&gt; 中的 &lt;a href=&#34;https://nvidia.github.io/libcudacxx/extended_api/synchronization_primitives/atomic.html&#34;&gt;&lt;code&gt;cuda::std::atomic&lt;/code&gt;&lt;/a&gt; 函数将每个存储桶定义为&lt;code&gt;cuda::std::atomic&amp;lt;pair&amp;lt;key, value&amp;gt;&amp;gt;&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;要插入新key，该实现根据哈希值计算第一个存储桶，并执行原子比较和交换操作，期望存储桶中的key等于&lt;code&gt;empty_sentinel&lt;/code&gt;。如果是，则槽为空，插入成功。否则，它会前进到下一个桶，直到最终找到一个空桶。&lt;/p&gt;
&lt;p&gt;下面的代码显示了哈希表插入函数的简化版本。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;n&#34;&gt;__device__&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;insert&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Key&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Value&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;// get initial probing position from the hash value of the key
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hash&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;capacity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;c1&#34;&gt;// load the content of the bucket at the current probe position
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;old_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;old_v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;buckets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;memory_order_relaxed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;c1&#34;&gt;// if the bucket is empty we can attempt to insert the pair
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;old_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;empty_sentinel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;// try to atomically replace the current content of the bucket with the input pair
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;success&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;buckets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;compare_exchange_strong&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
                    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;old_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;old_v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memory_order_relaxed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;success&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
      &lt;span class=&#34;c1&#34;&gt;// store was successful
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;      &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;old_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;// input key is already present in the map
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
  &lt;span class=&#34;c1&#34;&gt;// if the bucket was already occupied move to the next (linear) probing position
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// using the modulo operator to wrap back around to the beginning if we     
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// go beyond the capacity
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;capacity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以类似的方式查找hashmap中特定key的关联value。检查key探测序列中的每个位置，直到找到包含所需key的存储桶；或空存储桶表明该key没在hashmap中。&lt;/p&gt;
&lt;p&gt;（&lt;strong&gt;注&lt;/strong&gt;：「&lt;a href=&#34;https://nosferalatu.com/SimpleGPUHashTable.html&#34;&gt;SimpleGPUHashTable&lt;/a&gt;」一样的实现，但是cuCollections hashmap 做了进一步的优化，使用Cooperative groups 在负载系数高的情况下，线性探测的优化）&lt;/p&gt;
&lt;h2 id=&#34;cooperative-groups-协作组&#34;&gt;Cooperative groups 协作组&lt;/h2&gt;
&lt;p&gt;乍一看，为每个输入元素分配一个工作线程似乎是一个合理的比例。但是，请考虑以下事项：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入中的相邻key与其在内存中的相关探测位置之间没有关系。这意味着warp中的每个线程都可能访问hashmap的完全不同的区域。在最坏的情况下，每个探测步骤都需要从全局内存中的 32 个不同位置加载每个 warp。（回想一下随机存储器访问。）&lt;/li&gt;
&lt;li&gt;通过线性探测，每个线程可以从其初始探测位置开始访问多个相邻的存储桶。这种本地访问模式允许使用单个合并负载预取多个探测位置，不幸的是，这无法通过单个线程实现。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们可以做得更好吗？是的。CUDA&lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cooperative-groups&#34;&gt;协作组&lt;/a&gt;模型可以轻松地重新配置工作分配的粒度。每个输入元素不是使用单个 CUDA 线程，而是将元素分配给同一warp内的一组连续线程。&lt;/p&gt;
&lt;p&gt;对于给定的输入key，不是按顺序遍历其关联的探测序列，而是使用单个合并负载来预取多个相邻桶的窗口。然后，该组使用高效的 &lt;code&gt;ballot&lt;/code&gt; 和 &lt;code&gt;shuffle&lt;/code&gt; 内在函数合作确定窗口内的候选存储桶。&lt;/p&gt;
&lt;p&gt;下图显示了关key Bob 的小组合作探测步骤及其中间步骤。 由四个线程组成的协作组用于将key Bob 插入哈希表中。 从由key的哈希值确定的初始探测索引开始，将桶的合并窗口加载到本地寄存器中，并使用“ballot”内在函数确定候选桶。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday//maximizing-performance-with-massively-parallel-hash-maps-on-gpus/group-cooperative-probing-diagram.png&#34; alt=&#34;&#34;&gt;&lt;em&gt;图 3. key Bob 的群体合作探测步骤及其中间步骤&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;以下代码扩展了之前引入的插入函数，以使用warp中的四个连续线程来协作插入单个key。&lt;code&gt;cg::thread_block_tile&amp;lt;4&amp;gt;&lt;/code&gt;代表子warp中的四个线程。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;k&#34;&gt;enum&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;probing_state&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SUCCESS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DUPLICATE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;CONTINUE&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;__device__&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;insert&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;thread_block_tile&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Key&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Value&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;// get initial probing position from the hash value of the key
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hash&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;thread_rank&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;capacity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;probing_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CONTINUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;c1&#34;&gt;// load the contents of the bucket at the current probe position of each rank in a coalesced manner
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;old_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;old_v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;buckets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;memory_order_relaxed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;c1&#34;&gt;// input key is already present in the map
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;any&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;old_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;c1&#34;&gt;// each rank checks if its current bucket is empty, i.e., a candidate bucket for insertion
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;empty_mask&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ballot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;old_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;empty_sentinel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
  &lt;span class=&#34;c1&#34;&gt;// it there is an empty buckets in the group&amp;#39;s current probing window
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;empty_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;// elect a candidate rank (here: thread with lowest rank in mask)
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;candidate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;__ffs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;empty_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;thread_rank&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;candidate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
      &lt;span class=&#34;c1&#34;&gt;// attempt atomically swapping the input pair into the bucket
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;      &lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;success&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;buckets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;compare_exchange_strong&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
                      &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;old_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;old_v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memory_order_relaxed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
      &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;success&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;// insertion went successful
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;probing_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SUCCESS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;old_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;// else, re-check if a duplicate key has been inserted at the current probing position
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;probing_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DUPLICATE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;// broadcast the insertion result from the candidate rank to all other ranks
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;candidate_state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shfl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;candidate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;candidate_state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;probing_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SUCCESS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;candidate_state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;probing_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DUPLICATE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;// else, move to the next (linear) probing window
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;capacity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;前面的哈希表插入函数的代码示例是 cuCollections 实际实现的简化版本&lt;code&gt;cuco::static_map&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;图 4 显示了在&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/a100/&#34;&gt;NVIDIA A100 80 GB&lt;/a&gt; GPU上测量的非合作和合作探测方法的性能，未具体化不同组大小和表占用率。&lt;/p&gt;
&lt;p&gt;下图为不同协作组大小的探测吞吐量，以及不同哈希表负载因子下的最大可实现吞吐量（GUPS 结果）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday//maximizing-performance-with-massively-parallel-hash-maps-on-gpus/cooperative-probing-throughput-graph.png&#34; alt=&#34;&#34;&gt;&lt;em&gt;图 4. 对于协作探测，吞吐量以 GB/s 为单位（越高越好)。红色虚线显示峰值 GUPS 结果，它提供了该系统上可以实现的吞吐量的上限。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;如果负载系数较低，则非合作（非 CG）表现出接近最佳性能。然而，如果负载因子增加，由于冲突次数增加和探测序列更长，吞吐量会急剧下降。这是有问题的，因为较高的表加载因子对应于更好的内存利用率。&lt;/p&gt;
&lt;p&gt;协作探测可提高此类高负载系数场景的性能。当组大小为 4 时，当负载系数较高时，与非合作方法相比，可以观察到插入吞吐量高出 13%，查找吞吐量高出 40%。&lt;/p&gt;
&lt;p&gt;长探测序列也会出现在具有高key重数的多值场景中，因为相同的key会遍历相同的桶序列。合作探测也有助于加快这些场景的速度。&lt;/p&gt;
&lt;p&gt;有关组协作哈希表探测的更多信息，请参阅&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2018-s8237/&#34;&gt;多 GPU 节点上的并行哈希&lt;/a&gt;和&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcspring21-e31204/&#34;&gt;WarpCore：GPU 上快速哈希表的库&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;现有cpu和gpu哈希表比较&#34;&gt;现有CPU和GPU哈希表比较&lt;/h2&gt;
&lt;p&gt;多年来已经提出了各种 C++ hashmap实现。其中最受欢迎的是libstdc++/libc++ &lt;code&gt;std::unordered_map&lt;/code&gt;和&lt;a href=&#34;https://abseil.io/&#34;&gt;Abseil&lt;/a&gt; &lt;code&gt;absl::flat_hash_map&lt;/code&gt;。这些是顺序实现，从多个线程使用它们需要额外的同步。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/oneapi/onetbb.html&#34;&gt;&lt;strong&gt;TBB&lt;/strong&gt;&lt;/a&gt; &lt;code&gt;tbb::concurrent_hash_map&lt;/code&gt; 和 &lt;a href=&#34;https://github.com/facebook/folly&#34;&gt;&lt;strong&gt;Folly&lt;/strong&gt;&lt;/a&gt; &lt;code&gt;folly::AtomicHashMap&lt;/code&gt;是在CPU中并发多线程场景下，常使用的hashmap库。GPU 上可用的少数实现之一来自&lt;a href=&#34;https://kokkos.github.io/&#34;&gt;Kokkos &lt;/a&gt;&lt;code&gt;kokkos::UnorderedMap&lt;/code&gt;库。&lt;/p&gt;
&lt;p&gt;将上面提供的hashmap实现的性能与 cuCollection &lt;code&gt;cuco::static_map&lt;/code&gt;进行比较。基准设置如下。&lt;/p&gt;
&lt;p&gt;首先，将 2^27 (1 GB) 个唯一的 4 字节key/4 字节值对插入到每个map中，然后查询同一组key以检索其关联值。每次运行的目标hashtable负载率为 50%。性能以内存吞吐量（GB/秒；越高越好）来衡量。&lt;/p&gt;
&lt;p&gt;结果如图 5 所示。&lt;code&gt;cuco::static_map&lt;/code&gt;在单个 NVIDIA H100-80GB-SXM 上实现了 87.5 GB/s 的插入吞吐量和 134.6 GB/s 的查找吞吐量，这意味着比最快的 CPU 单线程和多线程实现，有数量级的提升。此外，在本次测试中，cuCollections 的性能优于其他 GPU 实现，&lt;code&gt;kokkos::UnorderedMap&lt;/code&gt;插入性能分别高出 3.8 倍，查找性能分别高出 2.6 倍。&lt;/p&gt;
&lt;p&gt;请注意，在此基准测试设置中，每个操作的 I/O 向量驻留在 CPU 端实现的 CPU 内存中，以及 GPU 端实现的 GPU 内存中。如果 GPU 哈希表的数据向量需要驻留在 CPU 内存中，则需要首先将输入数据移至 GPU，然后将结果移回 CPU 内存。&lt;/p&gt;
&lt;p&gt;这可以通过显式（异步批量）复制或使用 CUDA&lt;a href=&#34;https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/&#34;&gt;统一内存&lt;/a&gt;(unified memory)概念的自动页面迁移来实现。结果表明，我们实现的吞吐量始终远高于 PCIe Gen4 的实际可用带宽，甚至高于 H100 上的 PCIe Gen5。这意味着这种方法能够使 CPU 和 GPU 之间的链路完全饱和。&lt;/p&gt;
&lt;p&gt;换句话说，cuCollections 能够以系统 PCIe 带宽的速度构建和查询哈希表，即使数据不在 GPU 内存中也是如此。此外，得益于 CPU 和 GPU 之间的快速 NVLink-C2C 互连，&lt;a href=&#34;https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/&#34;&gt;NVIDIA Grace Hopper Superchip&lt;/a&gt;可以提供额外的加速，从而释放哈希表的全部吞吐量。相比之下，与 PCIe 相比，CPU hashmap的吞吐量通常要低得多。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday//maximizing-performance-with-massively-parallel-hash-maps-on-gpus/insert-find-throughput-graph.png&#34; alt=&#34;显示批量插入和批量查找操作的各种hashmap实现的吞吐量的条形图。&#34;&gt;&lt;em&gt;图 5. 流行的 CPU 和 GPU hash map实现的性能比较&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;多列关系连接multicolumn-relational-join示例&#34;&gt;多列关系连接(multicolumn relational join)示例&lt;/h2&gt;
&lt;p&gt;本节提供一个真实示例，说明如何使用 GPU 哈希表来实现复杂算法。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rapidsai/cudf&#34;&gt;cuDF&lt;/a&gt;是一个用于数据分析的 GPU 加速库。它提供了数据操作的原语，例如加载、连接和聚合。通过利用 cuCollections 哈希表，它使用哈希联接算法来执行联接操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday//maximizing-performance-with-massively-parallel-hash-maps-on-gpus/inner-join-implementation-RAPIDS-cuDF.png&#34; alt=&#34;该图显示了三个表，说明了 cuDF 连接实现如何用于内部连接。  &#34;&gt;&lt;em&gt;图 6. RAPIDS cuDF 中内部联接实现的构建和探测阶段&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;图 6 显示了 cuDF 连接实现如何用于内部连接。cuDF 提供内置哈希函数，将任意类型的行哈希为哈希值。不同的行可以具有相同的哈希值，因此需要进行行相等检查来确定两行是否真正相同。&lt;/p&gt;
&lt;p&gt;左侧的表用于填充 一个&lt;a href=&#34;https://github.com/NVIDIA/cuCollections/blob/dev/include/cuco/static_multimap.cuh&#34;&gt;&lt;code&gt;cuco::static_multimap&lt;/code&gt;&lt;/a&gt;其中key是行的哈希值，有效负载是关联的行索引。第24行插入到第47个桶，第25行插入到第48个桶。在探测阶段，右表第200行的哈希值为47，与桶的哈希值相同（或相同的key） 47 来自哈希表。&lt;/p&gt;
&lt;p&gt;为了最终确定两行是否相等，需要右表中 {André-Marie, Ampère} 的行索引 200 和左表中 {Alessandro, Volta} 的行索引 24 ，传递给行相等函数&lt;em&gt;row_equal(200, 24)&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;最后，这两行不相同，因此左侧表的第 24 行不匹配。最终，左表的第 25 行与右表的第 200 行匹配，因为哈希值相同，并且行相等性检查 ( row_equal &lt;em&gt;(200, 25)&lt;/em&gt; ) 也通过了。&lt;/p&gt;
&lt;p&gt;考虑到大小、选择性等方面的许多选项，对连接操作进行基准测试是一个复杂的主题。有关更多详细信息，请参阅&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2018-s8289/&#34;&gt;如何充分利用 GPU 加速数据库运算符&lt;/a&gt;和&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2019-s9557/&#34;&gt;有效、可扩展的多 GPU 连接&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;（&lt;strong&gt;注&lt;/strong&gt;： 类似的场景很多，比如判断两个特征向量是否相似，合并两个特征向量等等，可以引入GPU hashmap对应实现库来加速，一般是模型训练算力加速；在web业务服务场景下，很少使用，主要是cpu服务场景已经满足，没必要进一步优化，而且gpu计算服务成本高）&lt;/p&gt;
&lt;h2 id=&#34;如何在代码中使用-gpu-哈希表&#34;&gt;如何在代码中使用 GPU 哈希表&lt;/h2&gt;
&lt;p&gt;GPU 非常适合hashmap等并发数据结构。这一切都始于高带宽内存架构，对于许多小型随机读取和原子更新来说，高带宽内存架构比 CPU 快一个数量级(order-of-magnitude)。这直接转化为 GPU 上高效的哈希表插入和探测性能。&lt;/p&gt;
&lt;p&gt;本文介绍了设计大规模并行hashmap时的一些重要注意事项：&lt;/p&gt;
&lt;p&gt;1）具有开放寻址的哈希桶的平坦内存布局，以解决冲突；&lt;/p&gt;
&lt;p&gt;2）线程在相邻哈希桶上进行协作以进行插入和探测，以提高高负载因子场景中的性能。&lt;/p&gt;
&lt;p&gt;可以在 GitHub 上找到快速灵活的hashmap实现，作为&lt;a href=&#34;https://github.com/NVIDIA/cuCollections#data-structures&#34;&gt;cuCollections&lt;/a&gt;库的一部分。&lt;/p&gt;
&lt;p&gt;如果高性能数据存储和检索对的应用程序很重要，那么 GPU 加速的哈希表可以成为的首选数据结构。尝试一下&lt;a href=&#34;https://github.com/NVIDIA/cuCollections&#34;&gt;cuCollections&lt;/a&gt;库，亲自体验 GPU 的强大功能。&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;注&lt;/strong&gt;：还有另外一个库&lt;a href=&#34;https://github.com/stotko/stdgpu&#34;&gt;stdgpu&lt;/a&gt;，提供在GPU场景下 类似c++ STL相关容器操作；两者代码结构都是标准规范的c++工程开发结构，都有example,test,benchmark，使用起来非常友好~。&lt;a href=&#34;https://github.com/NVIDIA&#34;&gt;NVIDIA&lt;/a&gt;官方库 &lt;a href=&#34;https://github.com/NVIDIA/cuCollections&#34;&gt;cuCollections &lt;/a&gt;比较新, 优化支持更好，如果感兴趣可以贡献一波)&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/maximizing-performance-with-massively-parallel-hash-maps-on-gpus/&#34;&gt;https://developer.nvidia.com/blog/maximizing-performance-with-massively-parallel-hash-maps-on-gpus/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/&#34;&gt;https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2018-s8289/&#34;&gt;https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2018-s8289/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2019-s9557/&#34;&gt;https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2019-s9557/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/rapids-ai/accelerating-tf-idf-for-natural-language-processing-with-dask-and-rapids-6f6e416429df&#34;&gt;https://medium.com/rapids-ai/accelerating-tf-idf-for-natural-language-processing-with-dask-and-rapids-6f6e416429df&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Hash_table&#34;&gt;https://en.wikipedia.org/wiki/Hash_table&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/class/ee380/Abstracts/070221_LockFreeHash.pdf&#34;&gt;https://web.stanford.edu/class/ee380/Abstracts/070221_LockFreeHash.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://oneapi-src.github.io/oneTBB/main/tbb_userguide/concurrent_hash_map.html&#34;&gt;https://oneapi-src.github.io/oneTBB/main/tbb_userguide/concurrent_hash_map.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebook/folly/blob/main/folly/concurrency/ConcurrentHashMap.h&#34;&gt;https://github.com/facebook/folly/blob/main/folly/concurrency/ConcurrentHashMap.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stotko.github.io/stdgpu/doxygen/classstdgpu_1_1unordered__map.html&#34;&gt;https://stotko.github.io/stdgpu/doxygen/classstdgpu_1_1unordered__map.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：相似性搜索，第 7 部分：LSH 组合</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/7.lsh-compositions/</link>
      <pubDate>Tue, 26 Sep 2023 23:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/7.lsh-compositions/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;深入研究 LSH 函数的组合以保证更可靠的搜索&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;在本系列文章的最后两部分中，我们深入研究了 LSH —— 一种&lt;em&gt;将输入向量转换为低维散列值，同时保留有关其相似性的信息&lt;/em&gt;的算法。特别是，我们已经研究了两种适用于不同距离度量的算法：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://weedge.github.io/post/oneday/similarity-search/5.locality-sensitive-hashing/&#34;&gt;相似性搜索，第 5 部分：局部敏感哈希 (LSH)&lt;/a&gt;: 经典的LSH算法构造反映向量&lt;strong&gt;Jaccard系数&lt;/strong&gt;信息的签名。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://weedge.github.io/post/oneday/similarity-search/6.random-projections-with-lsh-forest/&#34;&gt;相似性搜索，第 6 部分：LSH 森林的随机投影&lt;/a&gt;: 随机投影方法构建了保持向量&lt;strong&gt;余弦相似性&lt;/strong&gt;的超平面森林。&lt;/p&gt;
&lt;p&gt;事实上，LSH 算法也适用于其他距离度量。尽管每种方法都有其独特的部分，但每种方法中都出现了许多共同的概念和公式。为了促进未来新方法的学习过程，我们将更多地关注理论并提供一些经常出现在高级 LSH 文献中的基本定义和定理。在本文结束时，我们将能够通过简单地将基本方案组合为乐高积木来构建更复杂的 LSH 方案。&lt;/p&gt;
&lt;p&gt;最后我们将了解如何将&lt;strong&gt;欧几里得距离&lt;/strong&gt;纳入 LSH 中。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意&lt;/em&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;作为主要先决条件，您应该已经熟悉本系列文章的第 5 部分和第 6 部分。如果没有，强烈建议先阅读它们。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cosine_similarity&#34;&gt;余弦距离&lt;/a&gt;定义在 [0, 2] 范围内。为简单起见，我们将其映射到区间 [0, 1]，其中 0 和 1 分别表示最低和最高可能的相似度。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;lsh-的正式定义&#34;&gt;LSH 的正式定义&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;给定距离度量 d，如果对于随机选择的对象 x 和 y，满足以下条件，则 H 被称为 (d₁, d2, p₁, p2) 敏感的 LSH 函数：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;如果 d(x, y) ≤ d₁，则 p(H(x) = H(y)) ≥ p₁，即 H(x) = H(y) 的概率至少为 p₁。&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;如果 d(x, y) ≥ d2，则 p(H(x) = H(y)) ≤ p2，即 H(x) = H(y) 的概率至多为 p2。&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;让我们了解这些陈述的含义。当两个向量相似时，它们之间的距离较小。基本上，第一个语句确保将它们散列到同一存储桶的概率高于某个阈值。这样，就消除了一些&lt;em&gt;FN 假阴性 漏报&lt;/em&gt;：如果两个向量之间的距离大于&lt;em&gt;d₁&lt;/em&gt;，那么它们散列到同一桶的概率始终小于&lt;em&gt;p₁&lt;/em&gt;。相反，第二条语句控制&lt;em&gt;FP 假阳性 误报&lt;/em&gt;：如果两个向量不相似并且它们之间的距离大于&lt;em&gt;d2&lt;/em&gt; ，则它们出现在同一桶中的概率上限为&lt;em&gt;p2阈值。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;考虑到上面的陈述，我们通常希望系统中满足以下陈述：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;p₁&lt;/em&gt;应尽可能接近 1，以减少 &lt;em&gt;FN 假阴性 漏报&lt;/em&gt; 的数量。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;p2&lt;/em&gt;应尽可能接近 0，以减少 &lt;em&gt;FP 假阳性 误报&lt;/em&gt; 的数量。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;d₁&lt;/em&gt;和&lt;em&gt;d2&lt;/em&gt;之间的差距应尽可能小，以减少无法对数据进行概率估计的区间。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;左图显示了一条典型曲线，展示了 (d1, d2, p1, p2) 符号的 LSH 参数之间的关系。右侧的曲线展示了阈值d₁和d2之间没有间隙的理想情况&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;有时，上面的陈述是通过使用相似度&lt;em&gt;s&lt;/em&gt;而不是距离&lt;em&gt;d 来引入的：&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;给定相似性度量 s，如果对于随机选择的对象 x 和 y，满足以下条件，则 H 被称为 (s₁, s2, p₁, p2) 敏感的 LSH 函数：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;如果 s(x, y) ≥ s₁，则 p(H(x) = H(y)) ≥ p₁，即 H(x) = H(y) 的概率至少为 p₁。&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;如果 s(x, y) ≤ s2，则 p(H(x) = H(y)) ≤ p2，即 H(x) = H(y) 的概率至多为 p2。&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;左图显示了一条典型曲线，展示了 ( &lt;em&gt;s1, s2, p1, p2&lt;/em&gt; ) 符号的 LSH 参数之间的关系。右侧的曲线展示了阈值&lt;em&gt;s₁&lt;/em&gt;和&lt;em&gt;s2&lt;/em&gt;之间没有间隙的理想情况。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意&lt;/em&gt;： 在本文中，将使用两种符号 &lt;em&gt;(d1、d2、p1、p2)&lt;/em&gt; 和 &lt;em&gt;(s1、s2、p1、p2)&lt;/em&gt;。根据文本中符号中使用的字母，应该清楚是暗示距离 &lt;em&gt;d&lt;/em&gt; 还是相似度 &lt;em&gt;s&lt;/em&gt; 。默认情况下，使用符号 &lt;em&gt;(d₁, d2, p₁, p2)&lt;/em&gt;。&lt;/p&gt;
&lt;h2 id=&#34;lsh-示例&#34;&gt;LSH 示例&lt;/h2&gt;
&lt;p&gt;为了让事情更清楚，让我们证明以下陈述：&lt;/p&gt;
&lt;p&gt;如果距离度量&lt;em&gt;s&lt;/em&gt;是 Jaccard 系数，则&lt;em&gt;H&lt;/em&gt;是一个 &lt;em&gt;(0.6, 0.6, 0.4, 0.4)&lt;/em&gt; 敏感的 LSH 函数。基本上，必须证明等价的陈述：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;如果 d(x, y) ≤ 0.6，则 p(H(x) = H(y)) ≥ 0.4&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;如果 d(x, y) ≥ 0.6，则 p(H(x) = H(y)) ≤ 0.4&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从本系列文章的第 5 部分中，我们知道&lt;em&gt;两个二进制向量获得相等哈希值的概率等于 Jaccard 相似度&lt;/em&gt;。因此，如果两个向量相似度至少为 40%，那么就保证获得相等哈希值的概率也至少为 40%。同时，至少 40% 的 Jaccard 相似度相当于最多 60% 的 Jaccard 系数。至此，第一个命题得证。对于第二个语句可以进行类似的思考。&lt;/p&gt;
&lt;p&gt;这个例子可以推广为定理：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：如果 d 是Jaccard 系数，则 H 是 LSH 函数的 (d₁, d₂, 1 — d₁, 1 — d₂) 族。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;同样，根据第6部分得到的结果，可以证明另一个定理：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：如果 s 是余弦相似度（在 -1 和 1 之间），则 H 是 LSH 函数的 (s₁, s2, 1 — arccos(s₁) / 180, 1 — arccos(d₂) / 180) 系列。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;组合-lsh-函数combining-lsh-functions&#34;&gt;组合 LSH 函数(Combining LSH functions)&lt;/h1&gt;
&lt;p&gt;让我们参考一下之前在 LSH 中学到的有用概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;回到关于 minhashing 的第 5 部分，每个向量都被分成几个band，每个band包含一组行。为了将一对向量视为候选向量，必须存在&lt;strong&gt;至少一个所有&lt;/strong&gt;向量行都相等的band。&lt;/li&gt;
&lt;li&gt;关于随机投影的第 6 部分，仅当存在至少一棵树且所有随机投影均未分离初始向量时，才将两个向量视为候选向量。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们可以注意到，这两种方法在底层有相似的范例。仅当n 个配置向量中&lt;strong&gt;至少有一次&lt;/strong&gt; &lt;em&gt;k&lt;/em&gt;次具有相同的哈希值时，它们才将一对向量视为候选向量。使用布尔代数符号，可以写成这样：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;基于这个例子，让我们介绍逻辑运算符&lt;em&gt;OR&lt;/em&gt;和&lt;em&gt;AND&lt;/em&gt;，它们允许聚合一组哈希函数。&lt;em&gt;然后我们将估计它们如何影响两个候选向量的输出概率以及假阴性&lt;/em&gt;和&lt;em&gt;假阳性&lt;/em&gt;错误率。&lt;/p&gt;
&lt;h2 id=&#34;与运算符and-operator&#34;&gt;与运算符(AND operator)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;给定 n 个独立的 LSH 函数 H₁、H2、…Hₙ，仅当两个向量的&lt;strong&gt;所有n 个对应哈希值都相等时，&lt;/strong&gt; &lt;strong&gt;AND&lt;/strong&gt;运算符才会将两个向量视为候选对。否则，向量不被视为候选向量。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;如果通过AND&lt;/em&gt;运算符聚合两个高度不同的向量的哈希值，那么它们成为候选者的概率随着使用的哈希函数数量的增加而降低。因此，&lt;em&gt;假阳性&lt;/em&gt;的数量减少了。&lt;/p&gt;
&lt;p&gt;同时，两个相似的向量可能会偶然产生一对不同的哈希值。因此，算法不会认为此类向量相似。这方面导致&lt;em&gt;假阴性率&lt;/em&gt;较高。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：考虑 r 独立 (s₁, s2, p₁, p2) 敏感的 LSH 函数。将这些 r LSH 函数与 AND 运算符组合会产生一个新的 LSH 函数，其参数为&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;使用几个独立事件的概率公式很容易证明这个说法，该公式乘以所有事件的概率来估计所有事件发生的概率。&lt;/p&gt;
&lt;h2 id=&#34;或运算符or-operator&#34;&gt;或运算符(OR operator)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;给定 n 个独立的 LSH 函数 H₁、H2、…Hₙ，仅当两个向量的 n 个对应哈希值中&lt;strong&gt;至少有一个相等时，&lt;/strong&gt; &lt;strong&gt;OR&lt;/strong&gt;运算符才会将两个向量视为候选对。否则，向量不被视为候选向量。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;与&lt;em&gt;AND&lt;/em&gt;运算符相反，&lt;em&gt;OR&lt;/em&gt;运算符增加了任意两个向量成为候选向量的概率。对于任何向量对，至少有一个对应的哈希值相等就足够了。因此，OR 聚合会减少&lt;em&gt;假阴性&lt;/em&gt;的数量并增加&lt;em&gt;假阳性&lt;/em&gt;的数量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：考虑&lt;em&gt;b 个&lt;/em&gt;独立的 (d₁, d2, p₁, p2) 族 LSH 函数。将这&lt;em&gt;b 个&lt;/em&gt;LSH 函数与 AND 运算符组合会产生一个新的 LSH 函数，其参数为&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们不会证明这个定理，因为本系列文章的第 5 部分已经获得并解释了由此产生的类似概率公式。&lt;/p&gt;
&lt;h2 id=&#34;组合composition&#34;&gt;组合(Composition)&lt;/h2&gt;
&lt;p&gt;通过&lt;em&gt;AND&lt;/em&gt;和&lt;em&gt;OR&lt;/em&gt;运算，可以通过各种方式将它们组合在一起，以更好地控制&lt;em&gt;误报率&lt;/em&gt;和&lt;em&gt;漏报率&lt;/em&gt;。让我们想象一下&lt;em&gt;AND&lt;/em&gt;组合器使用&lt;em&gt;r&lt;/em&gt; LSH 函数，&lt;em&gt;OR&lt;/em&gt;组合器使用&lt;em&gt;b&lt;/em&gt; LSH 函数。使用这些基本组合器可以构建两种不同的组合：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/6.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;AND-OR 和 OR-AND 是可以使用 AND 和 OR 运算符构建的两种组合类型。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;前两篇文章中描述的算法使用&lt;em&gt;AND-OR&lt;/em&gt;组合。事实上，没有什么可以阻止我们基于&lt;em&gt;AND&lt;/em&gt;和&lt;em&gt;OR&lt;/em&gt;运算构建更复杂的组合。&lt;/p&gt;
&lt;h2 id=&#34;构图示例&#34;&gt;构图示例&lt;/h2&gt;
&lt;p&gt;让我们研究一个示例，了解&lt;em&gt;AND&lt;/em&gt;和&lt;em&gt;OR&lt;/em&gt;的组合如何显着提高性能。假设参数 &lt;em&gt;b&lt;/em&gt; = 4 和 &lt;em&gt;r&lt;/em&gt; = 8 的 &lt;em&gt;OR-AND&lt;/em&gt; 组合。根据上面的相应公式，我们可以估计两个候选向量在组合后的初始概率如何变化：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/7.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;通过应用参数 b = 4 和 r = 8 的 OR-AND 组合来改变概率。第一行显示初始概率，第二行显示转换后的概率。&lt;/p&gt;
&lt;p&gt;例如，如果对于两个向量之间的某个相似度值，单个 LSH 函数在 40% 的情况下将它们散列到同一个存储桶，那么在 OR- &lt;em&gt;AND&lt;/em&gt;组合之后，在 32.9% 的情况下它们将被散列。&lt;/p&gt;
&lt;p&gt;要了解组合的特殊之处，请考虑 &lt;em&gt;(0.4, 1.7, 0.8, 0.2)&lt;/em&gt; 敏感的 LSH 函数。经过&lt;em&gt;OR-AND&lt;/em&gt;转换后，LSH 函数转换为 &lt;em&gt;(0.4, 1.7, 0.0148, 0.987)&lt;/em&gt; 敏感格式。&lt;/p&gt;
&lt;p&gt;本质上，如果最初两个向量非常相似并且距离小于 0.4，那么在 80% 的情况下它们将被视为候选向量。然而，通过应用组合，它们现在是 98.7% 场景的候选者，从而导致&lt;em&gt;假阴性&lt;/em&gt;错误大大减少！&lt;/p&gt;
&lt;p&gt;类似地，如果两个向量彼此非常不同并且距离大于 1.7，那么现在仅在 1.48% 的情况下将它们视为候选向量（之前为 20%）。这样，&lt;em&gt;误报错误&lt;/em&gt;的频率减少了 13.5 倍！这是一个巨大的进步！&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/8.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;显示不同组合后初始概率如何转换的曲线&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一般来说，通过使用*(d₁, d2, p₁, p2)&lt;em&gt;敏感的 LSH 函数，可以将其转换为&lt;/em&gt;(d₁, d2, p&#39;₁, p&#39;2)&lt;em&gt;格式，其中&lt;/em&gt;p&#39;₁&lt;em&gt;接近 1 p&#39;2接近 0。越接近 1 和 0 通常需要使用更多的组合&lt;/em&gt;。*&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;其他距离指标的-lsh&#34;&gt;其他距离指标的 LSH&lt;/h1&gt;
&lt;p&gt;我们已经深入研究了 LSH 方案，用于保存有关 Jaccard 系数和余弦距离的信息。自然出现的问题是是否可以将 LSH 用于其他距离度量。不幸的是，对于大多数指标，没有相应的 LSH 算法。&lt;/p&gt;
&lt;p&gt;然而，LSH 模式适用于欧几里得距离——机器学习中最常用的指标之一。由于它经常使用，我们将研究如何获取欧氏距离的哈希值。通过上面介绍的理论符号，我们将证明该指标的一个重要的 LSH 属性。&lt;/p&gt;
&lt;h2 id=&#34;欧几里得euclidean距离的-lsh&#34;&gt;欧几里得(Euclidean)距离的 LSH&lt;/h2&gt;
&lt;p&gt;欧几里得空间中点的散列机制包括将它们投影到随机线上。算法假设&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果两个点彼此相对接近，那么它们的投影也应该接近。&lt;/li&gt;
&lt;li&gt;如果两点彼此相距很远，那么它们的投影也应该很远。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了测量两个投影的接近程度，可以将一条线分为几个相等的段（桶），每个段的大小为&lt;em&gt;a&lt;/em&gt;。每条线段对应一个特定的哈希值。如果两个点投影到同一条线段，则它们具有相同的哈希值。否则，哈希值是不同的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/9.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;在随机线上投影点&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;尽管该方法乍一看似乎很强大，但它仍然可以将彼此相距较远的点投影到同一段。当连接两点的线几乎垂直于初始投影线时，尤其会发生这种情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/lsh-compositions/10.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;尽管两个点彼此相距较远，但它们仍然有可能被散列到同一个桶中。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;为了降低错误率，强烈建议使用随机投影线的组合，如上所述。&lt;/p&gt;
&lt;p&gt;几何上可以证明，如果&lt;em&gt;a&lt;/em&gt;是欧氏空间中单条线段的长度，则&lt;em&gt;H&lt;/em&gt;是 &lt;em&gt;(a/2,2a,1/2,1/3)&lt;/em&gt; 敏感的LSH函数。&lt;/p&gt;
&lt;h1 id=&#34;结论&#34;&gt;结论&lt;/h1&gt;
&lt;p&gt;在本章中，我们积累了一般 LSH 表示法的知识，这有助于我们正式引入组合操作，从而显着降低错误率。值得注意的是，LSH 仅适用于一小部分机器学习指标，但至少适用于最流行的指标，如欧氏距离、余弦距离和杰卡德指数。当处理测量向量之间相似性的另一个度量时，建议选择另一种相似性搜索方法。&lt;/p&gt;
&lt;p&gt;作为参考，本文中介绍的陈述的正式证明可以在&lt;a href=&#34;https://web.lums.edu.pk/~imdad/pdfs/CS5312_Notes/CS5312_Notes-14-LSH.pdf&#34;&gt;这些注释&lt;/a&gt;中找到。&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://web.lums.edu.pk/~imdad/pdfs/CS5312_Notes/CS5312_Notes-14-LSH.pdf&#34;&gt;&lt;strong&gt;Locality Sensitive Hashing | Lecture Notes for Big Data Analytics | Nimrah Mustafa&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cosine_similarity&#34;&gt;Cosine distance | Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;除非另有说明，所有图片均由作者提供&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;原文地址： &lt;a href=&#34;https://towardsdatascience.com/similarity-search-part-7-lsh-compositions-1b2ae8239aca&#34;&gt;https://towardsdatascience.com/similarity-search-part-7-lsh-compositions-1b2ae8239aca&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;附操作笔记&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_locality_sensitive_hashing_random_projection.ipynb&#34;&gt;&lt;strong&gt;faiss_locality_sensitive_hashing_random_projection.ipynb&lt;/strong&gt;&lt;/a&gt; 学习笔记&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>译：相似性搜索，第 6 部分：LSH 森林的随机投影</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/6.random-projections-with-lsh-forest/</link>
      <pubDate>Tue, 26 Sep 2023 21:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/6.random-projections-with-lsh-forest/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;了解如何对数据进行哈希处理并通过构造随机超平面来反映其相似性&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;在&lt;a href=&#34;https://medium.com/towards-data-science/similarity-search-part-5-locality-sensitive-hashing-lsh-76ae4b388203&#34;&gt;上一部分&lt;/a&gt;中，我们研究了 LSH 的主要范例，即将&lt;em&gt;输入向量转换为低维hash值，同时保留有关其相似性的信息&lt;/em&gt;。为了获取hash值（签名），使用了 minhash 函数。在本文中，我们将随机投影输入数据以获得类似的二进制向量。&lt;/p&gt;
&lt;h1 id=&#34;idea&#34;&gt;Idea&lt;/h1&gt;
&lt;p&gt;考虑高维空间中的一组点。可以构造一个随机超平面(hyperplane)，充当墙并将每个点分为两个子组之一：正子组positive和负子组negative。正侧的各点被赋予“1”值，负侧的各点被赋予“0”值。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;3D 空间中分隔两点的超平面示例&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;如何确定某个向量的超平面的边？通过使用内积！回到线性代数的本质，给定向量与超平面法向量之间的点积的符号决定了该向量位于哪一侧。这样，每个数据集向量都可以分为两侧之一。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;计算向量与超平面法向量的内积，并与0比较，可以知道向量相对于超平面位于哪一侧&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;显然，用一个二进制值对每个数据集向量进行编码是不够的。也就是说，应该构造几个随机超平面，因此每个向量都可以根据其与特定超平面的相对位置用许多 0 和 1 值进行编码。如果两个向量具有完全相同的二进制代码，则表明所构造的超平面都无法将它们分成不同的区域。因此，他们在现实中很可能非常接近。&lt;/p&gt;
&lt;p&gt;为了找到给定查询的最近邻居，通过检查其与所有超平面的相对位置以相同的方式用 0 和 1 编码查询就足够了。可以将找到的查询二元向量与为数据集向量获得的所有其他二元向量进行比较。这可以通过使用汉明距离线性完成。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;两个向量之间的汉明距离(Hamming distance)&lt;/strong&gt; 是它们的值不同的位置的数量。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;计算汉明距离的示例。左边的一对向量彼此更相似，因为它们的汉明距离更小。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;与查询的汉明距离最小的二元向量被视为候选向量，然后与初始查询进行充分比较。&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;注&lt;/strong&gt;： &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/lsh_hyperplanes.ipynb&#34;&gt;lsh_hyperplanes.ipynb&lt;/a&gt; 实操笔记)&lt;/p&gt;
&lt;h2 id=&#34;为什么超平面是随机构建的&#34;&gt;&lt;strong&gt;为什么超平面是随机构建的？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;在当前阶段，要求为什么超平面以随机方式构建而不是确定性的，意味着可以自定义规则，定义用于分离数据集点，似乎是合乎逻辑的。这背后有两个主要原因：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先，确定性方法无法推广算法并可能导致过度拟合。&lt;/li&gt;
&lt;li&gt;其次，随机性允许对算法的性能做出不依赖于输入数据的概率陈述。对于确定性方法来说，这是行不通的，因为它可能对一个数据表现良好，但对另一数据表现不佳。一个很好的类比是确定性&lt;a href=&#34;https://medium.com/@slavahead/quick-sort-explained-and-visualised-866cae28308e&#34;&gt;快速排序&lt;/a&gt;算法，其平均运行时间为&lt;em&gt;O(n * log n)&lt;/em&gt;。然而，它适用于O(n²)最坏情况下排序数组上的时间。如果有人了解算法的工作流程，那么该信息可以通过始终提供最坏情况的数据来明显损害系统的效率。这就是为什么随机快速排序更受欢迎的原因。随机超平面也会发生绝对类似的情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;为什么-lsh-随机投影也称为树&#34;&gt;为什么 LSH 随机投影也称为“树”？&lt;/h2&gt;
&lt;p&gt;随机投影方法有时称为&lt;strong&gt;LSH 树&lt;/strong&gt;。这是因为hash码分配的过程可以用决策树的形式表示，其中每个节点包含向量是否位于当前超平面的负侧或正侧的条件。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;第一个节点检查向量相对于红色超平面位于哪一侧。第二层的节点检查相同的条件，但相对于绿色超平面。最后，第三级检查蓝色超平面的相对位置。基于这 3 个条件，为向量分配一个 3 位hash值。&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;超平面森林&#34;&gt;超平面森林&lt;/h1&gt;
&lt;p&gt;超平面是随机构造的。这可能会导致如下图所示的数据集点分离不佳的情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;构造 4 个超平面以将数据集点表示为 4 长度的二进制向量。尽管 D 点和 E 点具有相同的hash码，但它们彼此相距相对较远（FP）。相反的情况发生在一对位于不同区域但彼此非常接近的点 E 和 F (FN) 上。考虑到汉明距离，该算法通常预测 D 点比 F 点更接近 E。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;从技术上讲，当两个点具有相同的hash码但彼此相距很远时，这并不是什么大问题。在算法的下一步中，这些点将被作为候选点并进行充分比较——这样算法就可以消除 &lt;em&gt;假阳性(false positive)&lt;/em&gt; 误报情况。&lt;em&gt;假阴性(false negatives)&lt;/em&gt; 漏报情况更加复杂：当两个点具有不同的hash码但实际上彼此接近时。&lt;/p&gt;
&lt;p&gt;为什么不使用与经典机器学习中的决策树相同的方法，将其组合成随机森林来提高整体预测质量？&lt;em&gt;如果一个估计器犯了错误，其他估计器可以产生更好的预测并减轻最终的预测误差&lt;/em&gt;。利用这个想法，构建随机超平面的过程可以独立重复。生成的hash值可以聚合为一对向量，其方式与上一章中的 minhash 值类似：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;如果查询与另一个向量至少有一次相同的hash码，则它们被视为候选者&lt;/em&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;使用这种机制可以减少&lt;em&gt;假阴性&lt;/em&gt;的数量。&lt;/p&gt;
&lt;h1 id=&#34;质量与速度的权衡quality-vs-speed-trade-off&#34;&gt;质量与速度的权衡(Quality vs speed trade-off)&lt;/h1&gt;
&lt;p&gt;选择适当数量的超平面在数据集上运行非常重要。选择的超平面越多来划分数据集点，冲突就越少，计算hash码所需的时间就越多，存储它们的内存也就越多。确切地说，如果一个数据集由&lt;em&gt;n 个&lt;/em&gt;向量组成，并且我们将其分割为&lt;em&gt;k 个&lt;/em&gt;超平面，那么平均每个可能的hash码将被分配给&lt;em&gt;n / 2ᵏ&lt;/em&gt;个向量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/6.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;k = 3 产生 2³ = 8 个桶&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;复杂complexity&#34;&gt;复杂(Complexity)&lt;/h1&gt;
&lt;h2 id=&#34;训练training&#34;&gt;训练(Training)&lt;/h2&gt;
&lt;p&gt;LSH Forest 训练阶段由两部分组成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;k个超平面的生成&lt;/em&gt;。这是一个相对较快的过程，因为&lt;em&gt;d&lt;/em&gt;维空间中的单个超平面可以在 &lt;em&gt;O(d)&lt;/em&gt; 时间内生成。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;为所有数据集向量分配hash码&lt;/em&gt;。此步骤可能需要时间，尤其是对于大型数据集。获取单个hash码需要 &lt;em&gt;O(dk)&lt;/em&gt; 的时间。如果数据集由 n个向量组成，则总复杂度变为&lt;em&gt;O(ndk)&lt;/em&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于森林中的每棵树，上述过程都会重复几次。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/7.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;训练复杂性&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;推理查询inference&#34;&gt;推理查询(Inference)&lt;/h2&gt;
&lt;p&gt;LSH 森林的优点之一是其快速推理查询，包括两个步骤：(k 超平面个数，d 维数，n 数据集向量数)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;获取查询的hash码&lt;/em&gt;。这相当于计算&lt;em&gt;k&lt;/em&gt; 个标量积，结果为 &lt;em&gt;O(dk)&lt;/em&gt; 时间。&lt;/li&gt;
&lt;li&gt;通过计算到候选者的精确距离，在同一存储桶（具有相同hash码的向量）中查找距查询最近的邻居。对于O(d) ，距离计算呈线性进行。每个桶平均包含n / 2ᵏ个向量。因此，计算到所有潜在候选者的距离需要O(dn / 2ᵏ)的时间。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总复杂度为 &lt;em&gt;O(dk + dn / 2ᵏ)&lt;/em&gt; 。&lt;/p&gt;
&lt;p&gt;像往常一样，对于森林中的每棵树，上述过程都会重复几次。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/8.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;推理查询复杂度&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;当超平面&lt;em&gt;k&lt;/em&gt;的数量选择为&lt;em&gt;n ~ 2ᵏ&lt;/em&gt;（在大多数情况下是可能的）时，总推理复杂度变为 &lt;em&gt;O(ldk)&lt;/em&gt;  （ &lt;em&gt;l&lt;/em&gt; 是树的数量）。基本上，这意味着 &lt;strong&gt;计算时间不取决于数据集大小！&lt;/strong&gt; 这种微妙之处导致了对数百万甚至数十亿向量的相似性搜索的有效可扩展性。&lt;/p&gt;
&lt;h1 id=&#34;错误率error-rate&#34;&gt;错误率(Error rate)&lt;/h1&gt;
&lt;p&gt;在有关 LSH 的文章的前一部分中，我们讨论了如何根据两个向量的签名相似性来确定两个向量被选为候选向量的概率。在这里，我们将使用几乎相同的逻辑来找到 LSH 森林的公式。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/9.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;令 s 为两个向量在其hash值的相同位置具有相同位的概率（稍后将估计 s）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/10.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;两个向量的长度为 k 的hash码相等的概率&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/11.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;两个向量的长度为 k 的hash码不同（或至少一位不同）的概率&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/12.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;两个向量的所有 l 个hash码（对于 l 个超平面）都不同的概率&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/13.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;两个向量的 l 个hash码中至少有一个相等的概率，因此向量将成为候选向量&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;到目前为止，我们几乎已经得到了估计两个向量成为候选向量的概率的公式。剩下的唯一事情就是估计方程中变量&lt;em&gt;s&lt;/em&gt;的值。在经典的LSH算法中，&lt;em&gt;s&lt;/em&gt;等于两个向量的Jaccard系数或签名相似度。另一方面，为了估计LSH 森林的&lt;em&gt;s&lt;/em&gt;，将使用线性代数理论。&lt;/p&gt;
&lt;p&gt;坦率地说，&lt;em&gt;s&lt;/em&gt;是两个向量&lt;em&gt;a&lt;/em&gt;和&lt;em&gt;b&lt;/em&gt;具有相同位的概率。该概率相当于随机超平面将这些向量分离到同一侧的概率。让我们想象一下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/14.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;向量 a 和 b 由蓝色超平面分隔。绿色超平面没有将它们分开。&lt;/p&gt;
&lt;p&gt;从图中可以清楚地看出，只有当向量&lt;em&gt;a&lt;/em&gt;和&lt;em&gt;b从向量 a 和 b 之间经过时，超平面才会将它们分成两个不同的边。&lt;em&gt;这种概率&lt;/em&gt;q&lt;/em&gt;与向量之间的角度成正比，可以轻松计算：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/15.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;随机超平面分隔两个向量的概率（即它们具有不同的位）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/16.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;随机超平面不分离两个向量的概率（即，它们具有相同的位）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;将这个方程代入之前获得的方程得出最终公式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/17.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;基于超平面数 k 和 LSH 树数 l 两个向量具有至少一个对应hash值（即成为候选者）的概率&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;可视化visualisation&#34;&gt;可视化(Visualisation)&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;注意&lt;/em&gt;：余弦相似度正式定义在范围 [-1, 1] 中。为简单起见，我们将此区间映射到 [0, 1]，其中 0 和 1 分别表示最低和最高可能的相似度。&lt;/p&gt;
&lt;p&gt;利用最后获得的公式，让我们根据不同数量的超平面 &lt;em&gt;k&lt;/em&gt; 和树 &lt;em&gt;l&lt;/em&gt; 的余弦相似度来可视化两个向量作为候选向量的概率。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/18.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;调整树的数量 l&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/19.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;调整超平面数量k&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;根据这些图，可以进行一些有用的观察：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;余弦相似度为 1 的一对向量总是成为候选向量。&lt;/li&gt;
&lt;li&gt;余弦相似度为 0 的一对向量永远不会成为候选向量。&lt;/li&gt;
&lt;li&gt;当超平面 &lt;em&gt;k&lt;/em&gt; 的数量减少或 LSH 树 &lt;em&gt;l&lt;/em&gt; 的数量增加时，两个向量成为候选的概率 &lt;em&gt;P&lt;/em&gt; 增加（即更多 &lt;em&gt;假阳性 false positives&lt;/em&gt; 误报）。相反的说法是正确的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;综上所述，LSH是一种非常灵活的算法：可以根据给定的问题调整不同的 &lt;em&gt;k&lt;/em&gt; 和 &lt;em&gt;l&lt;/em&gt; 值，得到满足问题要求的概率曲线。&lt;/p&gt;
&lt;h2 id=&#34;例子&#34;&gt;例子&lt;/h2&gt;
&lt;p&gt;让我们看下面的例子。想象一下&lt;em&gt;l = 5&lt;/em&gt;棵树， 用&lt;em&gt;k = 10 个&lt;/em&gt;超平面构建的树。除此之外，还有两个向量的余弦相似度为0.8。在大多数系统中，这种余弦相似性表明向量确实彼此非常接近。根据前面几节的结果，这个概率只有 2.5%！显然，对于如此高的余弦相似度来说，这是一个非常低的结果。使用&lt;em&gt;l = 5&lt;/em&gt;和&lt;em&gt;k = 10&lt;/em&gt;这些参数会 导致大量 &lt;em&gt;假阴性 false negatives&lt;/em&gt; 漏报！下面的绿线代表这种情况下的概率。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/20.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;基于两个向量余弦相似度的概率曲线&lt;/p&gt;
&lt;p&gt;这个问题可以通过调整&lt;em&gt;k&lt;/em&gt;和&lt;em&gt;l&lt;/em&gt;的更好值来将曲线向左移动来解决。&lt;/p&gt;
&lt;p&gt;例如，如果&lt;em&gt;k&lt;/em&gt;减小到 3（红线），则余弦相似度 0.8 的相同值将对应于 68% 的概率，这比以前更好。乍一看，红线似乎比绿线更适合，但请务必记住，使用较小的&lt;em&gt;k&lt;/em&gt;值（如红线的情况）会导致大量碰撞。这就是为什么有时最好调整第二个参数，即树的数量&lt;em&gt;l&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;与&lt;em&gt;k&lt;/em&gt;不同，它通常需要非常多的树 &lt;em&gt;l&lt;/em&gt; 才能获得相似的线形状。在图中，蓝线是通过将 &lt;em&gt;l&lt;/em&gt; 值从 10 更改为 500 从绿线获得的。蓝线显然比绿线拟合得更好，但仍远非完美：因为余弦之间的斜率很高相似度值为0.6和0.8时，余弦相似度在0.3-0.5附近的概率几乎等于0，这是不利的。文档相似度为 0.3-0.5 的小概率在现实生活中通常应该更高。&lt;/p&gt;
&lt;p&gt;根据最后一个例子，很明显，即使树的数量非常多（需要大量计算），仍然会导致许多&lt;em&gt;漏报&lt;/em&gt;！这是随机投影方法的主要缺点：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;尽管有可能获得完美的概率曲线，但它要么需要大量计算，要么会导致大量冲突。否则，会导致较高的假阴性漏报率。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;faiss实现&#34;&gt;FAISS实现&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://faiss.ai/&#34;&gt;根据Faiss文档&lt;/a&gt;中的信息，我们将了解如何构建LSH索引。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;随机投影&lt;/em&gt;算法在 Faiss 中的IndexLSH类中实现。尽管 Faiss 作者使用了另一种称为“随机旋转”的技术，但它仍然与本文中描述的技术有相似之处。&lt;strong&gt;该类仅实现一个 LSH 树。如果我们想使用 LSH 森林，那么只需创建几个 LSH 树并聚合它们的结果就足够了&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;IndexLSH&lt;/em&gt;类的构造函数有两个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;d&lt;/strong&gt;：维数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nbits&lt;/strong&gt;：编码单个向量所需的位数（可能的桶数等于&lt;em&gt;2ⁿᵇᶦᵗˢ&lt;/em&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;search() 方法返回的距离是到查询向量的汉明距离。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/21.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;IndexLSH 的 Faiss 实现&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;此外，Faiss 允许通过调用*faiss.vector_to_array(index.codes)*方法检查每个数据集向量的编码hash值。&lt;/p&gt;
&lt;p&gt;由于每个数据集向量均由&lt;em&gt;nbits&lt;/em&gt;二进制值编码，因此存储单个向量所需的字节数等于：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/random-projections-with-lsh-forest/22.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;johnson-lindenstrauss-lemma&#34;&gt;Johnson-Lindenstrauss lemma&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://cs.stanford.edu/people/mmahoney/cs369m/Lectures/lecture1.pdf&#34;&gt;Johnson-Lindenstrauss lemma&lt;/a&gt;是一个与降维相关的引理。虽然可能很难完全理解其原始陈述，但可以用简单的语言表述：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;选择随机子集并将原始数据投影到其上可以保留点之间相应的成对距离。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;更准确地说，拥有&lt;em&gt;n 个&lt;/em&gt;点的数据集，可以在&lt;em&gt;O(logn)&lt;em&gt;维的新空间中表示它们，从而几乎保留点之间的相对距离。如果向量在 LSH 方法中由&lt;/em&gt;~logn&lt;/em&gt;二进制值编码，则可以应用引理。此外，LSH 完全按照引理要求以随机方式创建超平面。&lt;/p&gt;
&lt;p&gt;Johnson-Lindenstrauss 引理的另一个令人难以置信的事实是，&lt;strong&gt;新数据集的维数不依赖于原始数据集的维数&lt;/strong&gt;！实际上，这个引理对于非常小的维度来说效果不佳。&lt;/p&gt;
&lt;h1 id=&#34;结论&#34;&gt;结论&lt;/h1&gt;
&lt;p&gt;我们已经使用了强大的相似性搜索算法。基于通过随机超平面分离点的简单想法，它通常在大型数据集上表现良好并且可扩展。此外，它具有良好的灵活性，允许选择适当数量的超平面和树。&lt;/p&gt;
&lt;p&gt;Johnson-Lindenstrauss 引理的理论结果强化了随机预测方法的使用。&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Random_projection&#34;&gt;https://en.wikipedia.org/wiki/Random_projection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://infolab.stanford.edu/~bawa/Pub/similarity.pdf&#34;&gt;LSH Forest: Self-Tuning Indexes for Similarity Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs.stanford.edu/people/mmahoney/cs369m/Lectures/lecture1.pdf&#34;&gt;The Johnson-Lindenstrauss Lemma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://faiss.ai/&#34;&gt;Faiss documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&#34;&gt;Summary of Faiss indexes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;除非另有说明，所有图片均由作者提供&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;原文地址： &lt;a href=&#34;https://towardsdatascience.com/similarity-search-part-6-random-projections-with-lsh-forest-f2e9b31dcc47&#34;&gt;https://towardsdatascience.com/similarity-search-part-6-random-projections-with-lsh-forest-f2e9b31dcc47&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;附操作笔记&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/lsh_hyperplanes.ipynb&#34;&gt;&lt;strong&gt;lsh_hyperplanes.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/lsh_random_projection_simp.ipynb&#34;&gt;&lt;strong&gt;lsh_random_projection_simp.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/lsh_sparse_implementation.ipynb&#34;&gt;&lt;strong&gt;lsh_sparse_implementation.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_lsh_implementation.ipynb&#34;&gt;&lt;strong&gt;faiss_lsh_implementation.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_locality_sensitive_hashing_random_projection.ipynb&#34;&gt;&lt;strong&gt;faiss_locality_sensitive_hashing_random_projection.ipynb&lt;/strong&gt;&lt;/a&gt; 学习笔记&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>译：相似性搜索，第 5 部分：局部敏感哈希 (LSH)</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/5.locality-sensitive-hashing/</link>
      <pubDate>Tue, 26 Sep 2023 15:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/5.locality-sensitive-hashing/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;探索如何将相似性信息合并到哈希函数中&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;在本系列文章的前几部分中，我们讨论了倒排文件索引、产品量化和 HNSW 以及如何将它们结合使用来提高搜索质量。在本章中，我们将研究一种主要不同的方法，该方法可以保持高搜索速度和质量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;局部敏感哈希&lt;/strong&gt;（LSH）是一组方法，用于通过将数据向量转换为哈希值来缩小搜索范围，同时保留有关其相似性的信息。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们将讨论传统方法，该方法包括三个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Shingling&lt;/strong&gt;：将原始文本编码为向量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MinHashing&lt;/strong&gt; ：将向量转换为称为&lt;strong&gt;签名&lt;/strong&gt;的特殊表示，可用于比较它们之间的相似性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LSH函数&lt;/strong&gt;：将签名块散列到不同的桶中。如果一对向量的签名至少一次落入同一个桶中，则它们被视为候选向量。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们将在整篇文章中逐步深入探讨每个步骤的细节。&lt;/p&gt;
&lt;h1 id=&#34;shingling&#34;&gt;Shingling&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Shingling&lt;/strong&gt; 是收集给定文本的k元模型的过程。k-gram是一组&lt;em&gt;k&lt;/em&gt;个连续标记(token)。根据上下文，标记(token)可以是单词或符号。shingling 的最终目标是使用收集的&lt;em&gt;k&lt;/em&gt;元模型对每个文档进行编码。我们将为此使用 one-hot 编码。然而，也可以应用其他编码方法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;为“学习数据科学令人着迷(learning data science is fascinating)”这句话收集长度为 k = 3 的独特碎片(shingles)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;首先，收集每个文档的唯一&lt;em&gt;k&lt;/em&gt;元组。其次，为了对每个文档进行编码，需要一个词汇表来表示所有文档中一组唯一的&lt;em&gt;k&lt;/em&gt;元组。然后，对于每个文档，创建一个长度等于词汇表大小的零向量。对于文档中出现的每个 k-gram，都会识别其在词汇表中的位置，并在文档向量的相应位置放置“1” 。即使相同的&lt;em&gt;k&lt;/em&gt;元模型在文档中出现多次，也没关系：向量中的值将始终为 1。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;one-hot编码&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;最小散列法minhashing&#34;&gt;最小散列法(MinHashing)&lt;/h1&gt;
&lt;p&gt;在此阶段，初始文本已被向量化。向量的相似度可以通过&lt;strong&gt;Jaccard系数&lt;/strong&gt;进行比较。请记住，两个集合的Jaccard系数定义为两个集合中公共元素的数量除以所有元素的长度。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;Jaccard系数定义为两个集合的并集的交集&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;如果采用一对编码向量，则 Jaccard 系数公式中的交集是同时包含 1 的行数（即&lt;em&gt;k&lt;/em&gt; -gram 出现在两个向量中），并集是至少包含一个 1 的行数（&lt;em&gt;k&lt;/em&gt; -gram 至少出现在向量之一中）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;两个向量的Jaccard系数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;使用上述公式计算两个向量的Jaccard系数的示例&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;目前的问题是编码向量的稀疏性。计算两个 one-hot 编码向量之间的相似度得分将花费大量时间。将它们转换为密集格式将使以后的操作更加高效。最终，目标是设计这样一个函数，将这些向量转换为更小的维度，保留有关它们相似性的信息。构造这样一个函数的方法称为 MinHashing。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;MinHashing&lt;/strong&gt;是一个哈希函数，它对输入向量的分量进行排列，然后返回排列后的向量分量等于 1 的第一个索引。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/6.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;计算给定向量和排列的 minhash 值的示例&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;为了获得由n 个数字组成的向量的密集表示，可以使用n 个minhash 函数来获取形成签名的n个minhash值。&lt;/p&gt;
&lt;p&gt;乍一听起来可能并不明显，但可以使用几个 minhash 值来近似向量之间的 Jaccard 相似度。事实上，使用的 minhash 值越多，近似值就越准确。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/7.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;签名矩阵的计算以及如何使用它来计算向量之间的相似度。使用 Jaccard 相似度和签名计算的相似度通常应大致相等。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;这只是一个有用的观察。事实证明，幕后有一个完整的定理。我们来了解一下为什么可以使用签名来计算Jaccard指数。&lt;/p&gt;
&lt;h2 id=&#34;证明statement-proof&#34;&gt;证明(Statement proof)&lt;/h2&gt;
&lt;p&gt;让我们假设给定的向量对仅包含类型为&lt;em&gt;01&lt;/em&gt;、&lt;em&gt;10&lt;/em&gt;和&lt;em&gt;11&lt;/em&gt;的行。然后对这些向量进行随机排列。由于所有行中至少存在一个 1，因此在计算两个哈希值时，这两个哈希值计算过程中至少有一个会停在对应哈希值等于 1 的向量的第一行。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/8.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;第二个哈希值等于第一个哈希值的概率是多少？显然，只有当第二个哈希值也等于 1 时才会发生这种情况。这意味着第一行必须是类型&lt;em&gt;11&lt;/em&gt;。由于排列是随机进行的，因此此类事件的概率等于&lt;em&gt;P&lt;/em&gt; &lt;em&gt;=&lt;/em&gt; &lt;em&gt;count(11) / (count(01) + count(10) + count(11) )&lt;/em&gt;。这个表达式与杰卡德指数公式完全相同。所以：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;基于随机行排列的两个二进制向量获得相等哈希值的概率等于 Jaccard 系数&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;然而，通过证明上面的陈述，我们假设初始向量不包含类型00的行。很明显，类型00的行不会更改 Jaccard 索引的值。同样，获得包含类型00的行的相同哈希值的概率也不会对其产生影响。例如，如果第一个排列行是00，那么 minhash 算法会忽略它并切换到下一行，直到一行中至少存在一个 1。当然，类型 00 的行可能会产生与没有类型 00 的行不同的哈希值，但获得相同哈希值的概率保持不变。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/9.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们已经证明了一个重要的说法。但是如何估计获得相同 minhash 值的概率呢？当然，可以生成向量的所有可能排列，然后计算所有最小哈希值以找到所需的概率。出于显而易见的原因，这效率不高，因为大小为&lt;em&gt;n&lt;/em&gt;的向量的可能排列数量等于&lt;em&gt;n!&lt;/em&gt;。不过，可以近似地评估概率：让我们使用许多哈希函数来生成那么多哈希值。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;两个二进制向量的 Jaccard 系数大约等于其签名中对应值的数量。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/10.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;很容易注意到，采用更长的签名会导致计算更准确。&lt;/p&gt;
&lt;h1 id=&#34;lsh-函数&#34;&gt;LSH 函数&lt;/h1&gt;
&lt;p&gt;目前，我们可以将原始文本转换为等长度的密集签名，保留有关相似性的信息。然而，在实践中，如此密集的签名通常仍然具有很高的维度，直接比较它们的效率很低。&lt;/p&gt;
&lt;p&gt;考虑&lt;em&gt;n = 10⁶&lt;/em&gt;文档，其签名长度为 100。假设单个签名需要 4 个字节来存储，那么整个签名将需要 400 个字节。为了存储&lt;em&gt;n = 10⁶&lt;/em&gt;文档，需要 400 MB 的空间，这在现实中是可行的。但是以强力方式比较每个文档将需要大约 5 * 10^1 比较，这太多了，尤其是当&lt;em&gt;n&lt;/em&gt;更大时。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/11.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;为了避免这个问题，可以构建一个哈希表来加速搜索性能，但即使两个签名非常相似并且仅在 1 个位置上不同，它们仍然可能具有不同的哈希值（因为向量余数可能不同）。然而，我们通常希望它们落入同一个桶中。这正是LSH能够发挥作用的地方。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;LSH&lt;/strong&gt;机制构建了一个由多个部分组成的哈希表，如果一对签名至少有一个对应的部分，则将它们放入同一个桶中。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;LSH 采用签名矩阵并将其水平划分为相等的&lt;em&gt;b&lt;/em&gt;部分，称为&lt;strong&gt;band&lt;/strong&gt;，每个部分包含&lt;em&gt;r&lt;/em&gt; &lt;strong&gt;rows&lt;/strong&gt;。不是将整个签名插入单个哈希函数，而是将签名分为&lt;em&gt;b&lt;/em&gt;个部分，并且每个子签名由哈希函数独立处理。因此，每个子签名都会落入不同的桶中。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/12.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;使用 LSH 的示例：长度为 9 的两个签名被分为 b = 3 个带，每个带包含 r = 3 行。每个子向量被散列到 k 个可能的桶之一中。由于第二个带中存在匹配（两个子向量具有相同的哈希值），因此我们将这些签名中的一对视为最近邻居的候选者。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;如果两个不同签名的相应子向量之间至少存在一次冲突，则这些签名被视为候选。正如我们所看到的，这个条件更加灵活，因为将向量视为候选向量不需要绝对相等。然而，这增加了误报的数量：一对不同的签名可以具有单个对应部分，但总体上完全不同。根据问题的不同，优化参数&lt;em&gt;b&lt;/em&gt;、&lt;em&gt;r&lt;/em&gt;和 k。&lt;/p&gt;
&lt;h1 id=&#34;错误率error-rate&#34;&gt;错误率(Error rate)&lt;/h1&gt;
&lt;p&gt;使用 LSH，可以估计具有相似性&lt;em&gt;s&lt;/em&gt;的两个签名将被视为候选的概率，给定多个频带&lt;em&gt;b&lt;/em&gt;和每个频带中的行数&lt;em&gt;r&lt;/em&gt;。让我们分几步找到它的公式。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/13.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;两个签名的随机行相等的概率&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/14.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;具有 r 行的一个随机带相等的概率&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/15.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;具有 r 行的一个随机带不同的概率&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/16.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;表中所有b波段不同的概率&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/17.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;b 个波段中至少有一个相等的概率，因此两个签名是候选者&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;请注意，该公式没有考虑不同子向量意外散列到同一存储桶中时的冲突。因此，签名成为候选人的真实概率可能差别不大。&lt;/p&gt;
&lt;h2 id=&#34;例子&#34;&gt;例子&lt;/h2&gt;
&lt;p&gt;为了更好地理解我们刚刚获得的公式，让我们考虑一个简单的例子。考虑两个长度为 35 个符号的签名，它们被平均分为 5 个带，每个带 7 行。下表表示基于 Jaccard 相似度至少有一个相等带的概率：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/18.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;根据两个签名的相似度 s 获得至少一个相应频带的概率 P&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;我们注意到，如果两个相似的签名的 Jaccard 相似度为 80%，那么它们在 93.8% 的情况下具有相应的band（&lt;em&gt;真阳性 true positives TP&lt;/em&gt;）。在其余 6.2% 的场景中，这样的一对签名是&lt;em&gt;漏报的&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;现在让我们采取两个不同的签名。例如，它们的相似度只有 20%。因此，在 0.224% 的情况下，它们是&lt;em&gt;假阳性 false positive FP&lt;/em&gt;候选者。在其他 99.776% 的情况下，它们没有类似的band，因此它们是&lt;em&gt;真阴性 true negatives TN&lt;/em&gt;。&lt;/p&gt;
&lt;h2 id=&#34;可视化visualisation&#34;&gt;可视化(Visualisation)&lt;/h2&gt;
&lt;p&gt;现在让我们可视化相似性&lt;em&gt;s&lt;/em&gt;和两个签名成为候选者的概率&lt;em&gt;P&lt;/em&gt;之间的联系。通常，签名相似度越高*，*签名成为候选的概率就越高。理想情况下，它看起来像下面这样：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/19.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;理想的场景。仅当一对签名的相似度大于某个阈值 t 时，才将其视为候选签名&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;根据上面获得的概率公式，一条典型的线路如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/20.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;一条典型的线，在开始和结束时缓慢增加，并在图中近似概率公式给出的阈值 t 处具有陡峭的斜率&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;可以改变band &lt;em&gt;b&lt;/em&gt;的数量来将图中的线向左或向右移动。增加&lt;em&gt;b&lt;/em&gt;将线向左移动并导致更多&lt;em&gt;FP&lt;/em&gt;，减少 - 将其向右移动并导致更多&lt;em&gt;FN&lt;/em&gt;。根据问题的不同，找到良好的平衡点很重要。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/21.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;band数量较多时，该线向左移动，band数量较少时，则向右移动&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/22.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;向左移动阈值会增加 FP，向右移动会增加 FN&lt;/p&gt;
&lt;h2 id=&#34;不同数量的band和行的实验&#34;&gt;不同数量的band和行的实验&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;下面针对b&lt;/em&gt;和&lt;em&gt;r&lt;/em&gt;的不同值构建了几个线图。最好根据特定任务调整这些参数，以成功检索所有相似文档对并忽略具有不同签名的文档。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/23.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;调整频段数量&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/locality-sensitive-hashing/24.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;调整行数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（注&lt;/strong&gt;：&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/testing_lsh.ipynb&#34;&gt;&lt;strong&gt;testing_lsh.ipynb&lt;/strong&gt;&lt;/a&gt; 实操笔记）&lt;/p&gt;
&lt;h1 id=&#34;结论&#34;&gt;结论&lt;/h1&gt;
&lt;p&gt;我们已经完成了 LSH 方法的经典实现。LSH 通过使用较低维的签名表示和快速哈希机制来缩小候选者的搜索范围，从而显着优化搜索速度。同时，这是以搜索准确性为代价的，但在实践中，差异通常是微不足道的。&lt;/p&gt;
&lt;p&gt;然而，LSH 容易受到高维数据的影响：更多维度需要更长的签名长度和更多的计算才能保持良好的搜索质量。在这种情况下，建议使用其他索引。&lt;/p&gt;
&lt;p&gt;事实上，LSH 存在不同的实现，但它们都基于相同的范例，即将&lt;em&gt;输入向量转换为哈希值，同时保留有关其相似性的信息&lt;/em&gt;。基本上，其他算法只是定义了获取这些哈希值的其他方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;随机投影&lt;/strong&gt;是另一种 LSH 方法，将在下一章中介绍，它在&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss&lt;/a&gt;库中作为 LSH 索引实现，用于相似性搜索。&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cse.iitkgp.ac.in/~animeshm/algoml/lsh.pdf&#34;&gt;Locality Sensitive Hashing | Andrew Wylie | December 2, 2013&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.inf.u-szeged.hu/~berendg/docs/dm/DM_lsh_en_pf.pdf&#34;&gt;Data Mining | Locality Sensitive Hashing | University of Szeged&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/&#34;&gt;https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;除非另有说明，所有图片均由作者提供&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;原文地址： &lt;a href=&#34;https://medium.com/towards-data-science/similarity-search-part-5-locality-sensitive-hashing-lsh-76ae4b388203&#34;&gt;https://medium.com/towards-data-science/similarity-search-part-5-locality-sensitive-hashing-lsh-76ae4b388203&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;附操作笔记&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/testing_traditional_lsh.ipynb&#34;&gt;&lt;strong&gt;testing_traditional_lsh.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_lsh.ipynb&#34;&gt;&lt;strong&gt;faiss_lsh.ipynb&lt;/strong&gt;&lt;/a&gt; 学习笔记&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>译：相似性搜索，第 4 部分：分层可导航小世界 (HNSW)</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/4.hierarchical-navigable-small-world-hnsw/</link>
      <pubDate>Mon, 25 Sep 2023 15:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/4.hierarchical-navigable-small-world-hnsw/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;了解如何构建高效的多层图以提高海量数据的搜索速度&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1603.09320.pdf&#34;&gt;&lt;strong&gt;分层可导航小世界&lt;/strong&gt;&lt;/a&gt;(HNSW) 是一种用于近似搜索最近邻居的最先进算法。在底层，HNSW 构建了优化的图结构，使其与本系列文章前面部分中讨论的其他方法非常不同。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;HNSW 的主要思想是构建这样一个图，其中任何一对顶点之间的路径都可以通过少量步骤遍历。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;著名的&lt;a href=&#34;https://en.wikipedia.org/wiki/Six_degrees_of_separation&#34;&gt;六次握手规则&lt;/a&gt;的一个著名类比与此方法有关：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所有人之间的社会关系距离不超过 6 个。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在继续讨论 HNSW 的内部工作之前，让我们首先讨论跳跃列表和可导航小世界——HNSW 实现中使用的关键数据结构。&lt;/p&gt;
&lt;h1 id=&#34;跳跃列表skip-list&#34;&gt;跳跃列表(Skip List)&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Skip_list&#34;&gt;skip list&lt;/a&gt;是一种概率数据结构，允许在排序列表中插入和搜索元素*O(logn)*一般。跳跃列表由多层链表构成。最底层有原始链表，其中包含所有元素。当移动到更高级别时，跳过的元素数量会增加，从而减少连接数量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在跳跃列表中查找元素 20&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;某个值的搜索过程从最高级别开始，并将其下一个元素与该值进行比较。如果该值小于或等于该元素，则算法将继续处理下一个元素。否则，搜索过程下降到具有更多连接的较低层并重复相同的过程。最后，算法下降到最低层并找到所需的节点。&lt;/p&gt;
&lt;p&gt;根据&lt;a href=&#34;https://en.wikipedia.org/wiki/Skip_list&#34;&gt;维基百科&lt;/a&gt;的信息，跳跃列表的主要参数&lt;em&gt;p&lt;/em&gt;定义了一个元素出现在多个列表中的概率。如果某个元素出现在&lt;em&gt;第 i层&lt;/em&gt;，那么它出现在第&lt;em&gt;i+1&lt;/em&gt;层的概率等于&lt;em&gt;p&lt;/em&gt;（&lt;em&gt;p&lt;/em&gt;通常设置为 0.5 或 0.25 ）。平均而言，每个元素都以*1 / (1 - p)*列表的形式呈现。&lt;/p&gt;
&lt;p&gt;我们可以看到，这个过程比链表中普通的线性搜索要快得多。事实上，HNSW 继承了相同的思想，但它使用的不是链表，而是图。&lt;/p&gt;
&lt;h1 id=&#34;可航行的小世界navigable-small-world&#34;&gt;可航行的小世界(Navigable Small World)&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Small-world_network&#34;&gt;&lt;strong&gt;可导航小世界&lt;/strong&gt;&lt;/a&gt;是一个具有多对数*T = O(logᵏn)*搜索复杂度的图，它使用贪婪路由。**路由(Routing)**是指从低度数顶点开始搜索过程，以高度数顶点结束的过程。由于低度顶点的连接很少，因此该算法可以在它们之间快速移动，以有效地导航到最近邻居可能所在的区域。然后算法逐渐放大并切换到高度顶点，以找到该区域顶点中的最近邻居。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;顶点&lt;em&gt;Vertex&lt;/em&gt;有时也称为&lt;strong&gt;节点Node&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;搜索&#34;&gt;搜索&lt;/h2&gt;
&lt;p&gt;首先，通过选择入口点来进行搜索。为了确定算法要移动到的下一个顶点（或多个顶点），它会计算从查询向量到当前顶点的邻居的距离，并移动到最近的顶点。在某些时候，当算法无法找到比当前节点本身更接近查询的邻居节点时，算法会终止搜索过程。该节点作为查询的响应返回。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;可导航的小世界中的贪婪搜索过程。节点 A 用作入口点。它有两个邻居 B 和 D。节点 D 比 B 更接近查询。因此，我们移动到 D。节点 D 有三个邻居 C、E 和 F。E 是距离查询最近的邻居，因此我们移动到 D。最后，搜索过程将导致节点 L。由于 L 的所有邻居都比 L 本身距离查询更远，因此我们停止算法并返回 L 作为查询的答案&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;这种贪婪策略不能保证它会找到精确的最近邻居，因为该方法仅使用当前步骤的本地信息来做出决策。&lt;strong&gt;提前停止Early stopping&lt;/strong&gt;是该算法的问题之一。它尤其发生在搜索过程开始时，当没有比当前节点更好的邻居节点时。在大多数情况下，当起始区域具有太多低度顶点时，可能会发生这种情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;提前停止，当前节点的两个邻居距离查询都较远。因此，算法返回当前节点作为响应，尽管存在距离查询更近的节点&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;通过&lt;strong&gt;使用多个入口点&lt;/strong&gt;可以提高搜索准确性。&lt;/p&gt;
&lt;h2 id=&#34;建造&#34;&gt;建造&lt;/h2&gt;
&lt;p&gt;NSW 图是通过打乱数据集点并将它们一一插入到当前图中来构建的。当插入一个新节点时，它会通过边链接到距离它最近的&lt;em&gt;M 个顶点。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;按 M = 2 顺序插入节点（从左到右）。每次迭代时，都会将一个新顶点添加到图中，并链接到其 M = 2 个最近邻居。蓝线表示与新插入节点的连接边。&lt;/p&gt;
&lt;p&gt;在大多数情况下，远程边可能会在图构建的开始阶段创建。它们在图形导航中发挥着重要作用。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在构造开始时插入的元素的最近邻居的链接后来成为网络集线器之间的桥梁，保持整体图的连接性并允许在贪婪路由期间对跳数进行对数缩放。——于。A.马尔科夫，DA Yashunin&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;从上图中的例子我们可以看出一开始添加的远程边&lt;em&gt;AB&lt;/em&gt;的重要性。想象一下，一个查询需要从相对较远的节点&lt;em&gt;A&lt;/em&gt;和 I 遍历一条路径。有了边&lt;em&gt;AB&lt;/em&gt;，可以通过直接从图的一侧导航到另一侧来快速完成此操作。&lt;/p&gt;
&lt;p&gt;随着图中顶点数量的增加，新连接到新节点的边的长度变小的可能性也随之增加。&lt;/p&gt;
&lt;h1 id=&#34;hnsw&#34;&gt;HNSW&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1603.09320.pdf&#34;&gt;&lt;strong&gt;HNSW&lt;/strong&gt;&lt;/a&gt;基于与skiplist 调表和navigable small world (NSW) 可导航小世界相同的原理。它的结构代表了一个多层图，顶层的连接较少，底层的区域更密集。&lt;/p&gt;
&lt;h1 id=&#34;搜索search&#34;&gt;搜索(Search)&lt;/h1&gt;
&lt;p&gt;搜索从最高层开始，每次在各层节点中贪婪地找到局部最近邻时，都会向下一级进行搜索。最终，在最低层找到的最近邻居就是查询的答案。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在HNSW中搜索&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;与 NSW 类似，HNSW 的搜索质量可以通过使用多个入口点来提高。不是在每一层上只找到一个最近邻居，而是找到与查询向量最接近的&lt;em&gt;efSearch&lt;/em&gt;（超参数） ，并将这些邻居中的每一个用作下一层的入口点。&lt;/p&gt;
&lt;h2 id=&#34;复杂度&#34;&gt;复杂度&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1603.09320.pdf&#34;&gt;原论文&lt;/a&gt;中，在任何层上找到最近邻居所需的操作数量都受到一个常数的限制。考虑到图中所有层的数量是对数的，我们得到的总搜索复杂度为&lt;em&gt;O(logn)&lt;/em&gt;。&lt;/p&gt;
&lt;h1 id=&#34;建造construction&#34;&gt;建造(Construction)&lt;/h1&gt;
&lt;h2 id=&#34;选择最大层数&#34;&gt;选择最大层数&lt;/h2&gt;
&lt;p&gt;HNSW中的节点是按顺序一一插入的。每个节点都被随机分配一个整数&lt;em&gt;l&lt;/em&gt;，表示该节点可以出现在图中的最大层。例如，如果&lt;em&gt;l = 1&lt;/em&gt;，则只能在第 0 层和第 1 层上找到该节点。作者为每个节点随机选择&lt;em&gt;l&lt;/em&gt; ，其具有由非零乘数 mL 归一化的&lt;strong&gt;指数衰减概率分布(exponentially decaying probability distribution)&lt;/strong&gt;（&lt;em&gt;mL= 0&lt;/em&gt;导致HNSW 中的单层和未优化的搜索复杂度）。通常，大多数&lt;em&gt;l&lt;/em&gt;值应等于 0，因此大多数节点仅存在于最低级别。&lt;em&gt;mL&lt;/em&gt;的较大值增加节点出现在较高层的概率。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/6.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	每个节点的层数 &lt;em&gt;l&lt;/em&gt; 是按指数衰减概率分布随机选择的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/7.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;基于归一化因子 mL 的层数分布。水平轴表示均匀(0, 1) 分布的值&lt;/em&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;为了实现可控层次结构的最佳性能优势，不同层上的邻居之间的重叠（即也属于其他层的元素邻居的百分比）必须很小。— Yu. A. Malkov, D. A. Yashunin。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;减少重叠的方法之一是减少&lt;em&gt;mL&lt;/em&gt;。但重要的是要记住，减少&lt;em&gt;mL&lt;/em&gt;平均也会导致在每层的贪婪搜索期间进行更多的遍历。这就是为什么必须选择一个能够平衡重叠和遍历次数的&lt;em&gt;mL值。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;该论文的作者建议选择&lt;em&gt;mL&lt;/em&gt;的最佳值，即等于&lt;strong&gt;1 / ln(M)&lt;/strong&gt;。该值对应于跳跃列表的参数&lt;em&gt;p = 1 / M&lt;/em&gt;，即层之间的平均单个元素重叠。&lt;/p&gt;
&lt;h2 id=&#34;插入insertion&#34;&gt;插入(Insertion)&lt;/h2&gt;
&lt;p&gt;为节点分配值&lt;em&gt;l&lt;/em&gt;后，其插入有两个阶段：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;该算法从上层开始，贪婪地寻找最近的节点。然后找到的节点将用作下一层的入口点，并且搜索过程将继续。一旦到达第&lt;em&gt;l&lt;/em&gt;层*，*插入就进入第二步。&lt;/li&gt;
&lt;li&gt;从第&lt;em&gt;l&lt;/em&gt;层开始，算法在当前层插入新节点。然后它的行为与之前步骤 1 中的相同，但不是只查找一个最近邻居，而是贪婪地搜索&lt;em&gt;efConstruction&lt;/em&gt;（超参数）最近邻居。然后从&lt;em&gt;efConstruction&lt;/em&gt;邻居中选择&lt;em&gt;M&lt;/em&gt; 个，并构建从插入节点到它们的边。之后，算法下降到下一层，找到的每个&lt;em&gt;efConstruction&lt;/em&gt;节点都充当入口点。当新节点及其边被插入到最低层 0 后，算法终止。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/8.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在 HNSW 中插入节点（蓝色）。新节点的最大层被随机选择为 l = 2。因此，该节点将被插入到第 2、1 和 0 层。在这些层的每一层上，该节点将连接到其 M = 2 个最近邻居&lt;/em&gt;。&lt;/p&gt;
&lt;h2 id=&#34;选择构造参数值choosing-values-for-construction-parameters&#34;&gt;选择构造参数值(Choosing values for construction parameters)&lt;/h2&gt;
&lt;p&gt;原始论文提供了关于如何选择超参数的一些有用的见解：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据模拟，&lt;em&gt;M&lt;/em&gt;的最佳值在 5 到 48 之间。较小的&lt;em&gt;M&lt;/em&gt;值往往更适合较低召回率或低维数据，而较高的 M 值更适合高召回率或高维数据。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;efConstruction&lt;/em&gt;的值越高，意味着随着探索的候选者越多，搜索就越深入。然而，它需要更多的计算。作者建议选择这样一个&lt;em&gt;efConstruction&lt;/em&gt;值，使训练期间的召回率接近&lt;em&gt;0.95-1&lt;/em&gt;。&lt;/li&gt;
&lt;li&gt;此外，还有一个重要参数&lt;em&gt;Mₘₐₓ&lt;/em&gt;——一个顶点可以拥有的最大边数。除此之外，还存在相同的参数&lt;em&gt;Mₘₐₓ₀&lt;/em&gt;，但对于最低层Lay 0是单独的。建议为&lt;em&gt;Mₘₐₓ&lt;/em&gt;选择接近&lt;em&gt;2 * M&lt;/em&gt;的值。大于&lt;em&gt;2 * M&lt;/em&gt;的值可能会导致性能下降和内存使用过多。同时，&lt;em&gt;Mₘₐₓ = M&lt;/em&gt;导致在高召回率下表现不佳。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;候选者启发式选择candidate-selection-heuristic&#34;&gt;候选者启发式选择(Candidate selection heuristic)&lt;/h2&gt;
&lt;p&gt;上面注意到，在节点插入期间，选择&lt;em&gt;efConstruction&lt;/em&gt;候选者中的&lt;em&gt;M&lt;/em&gt;个来为它们构建边。让我们讨论选择这&lt;em&gt;M&lt;/em&gt; 个节点的可能方法。&lt;/p&gt;
&lt;p&gt;朴素方法采用&lt;em&gt;M&lt;/em&gt;个最接近的候选者。然而，它并不总是最佳选择。下面是一个演示它的例子。&lt;/p&gt;
&lt;p&gt;想象一个具有下图结构的图表。如您所见，共有三个区域，其中两个区域未相互连接（左侧和顶部）。因此，例如，从&lt;em&gt;A&lt;/em&gt;点到&lt;em&gt;B&lt;/em&gt;点需要穿过另一个区域的很长的路径。以某种方式连接这两个区域以实现更好的导航是合乎逻辑的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/9.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;节点 X 被插入到图中。目标是将其最佳地连接到其他 M = 2 点&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;然后将节点&lt;em&gt;X&lt;/em&gt;插入到图中，并且需要链接到&lt;em&gt;M&lt;/em&gt; &lt;em&gt;= 2 个&lt;/em&gt;其他 顶点。&lt;/p&gt;
&lt;p&gt;在这种情况下，朴素方法直接采用&lt;em&gt;M = 2 个&lt;/em&gt;最近邻居（&lt;em&gt;B&lt;/em&gt;和&lt;em&gt;C&lt;/em&gt;）并将&lt;em&gt;X&lt;/em&gt;连接到它们。尽管&lt;em&gt;X&lt;/em&gt;与其真正的最近邻居相连，但这并不能解决问题。让我们看看作者发明的启发式方法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;启发式不仅考虑节点之间的最近距离，还考虑图上不同区域的连通性。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;启发式选择第一个最近邻居（在我们的例子中为&lt;em&gt;B&lt;/em&gt;）并将插入的节点 (&lt;em&gt;X&lt;/em&gt;) 连接到它。然后，该算法按顺序选取另一个最接近的最近邻居 (&lt;em&gt;C&lt;/em&gt;)，并仅当该邻居到新节点 (&lt;em&gt;X&lt;/em&gt;) 的距离小于从该邻居到所有已连接顶点的任何距离时，才为其构建一条边(&lt;em&gt;B&lt;/em&gt;) 到新节点 (&lt;em&gt;X&lt;/em&gt;)。之后，算法继续到下一个最近邻，直到建立&lt;em&gt;M&lt;/em&gt;条边&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;回到示例，启发式过程如下图所示。启发式选择&lt;em&gt;B&lt;/em&gt;作为 X 的最近邻并构建边&lt;em&gt;BX&lt;/em&gt;。然后算法选择&lt;em&gt;C&lt;/em&gt;作为下一个最近的最近邻。然而，这次&lt;em&gt;BC &amp;lt; CX&lt;/em&gt;。这表明将边&lt;em&gt;CX&lt;/em&gt;添加到图中并不是最佳的，因为已经存在边&lt;em&gt;BX&lt;/em&gt;并且节点&lt;em&gt;B&lt;/em&gt;和&lt;em&gt;C&lt;/em&gt;彼此非常接近。对于节点&lt;em&gt;D&lt;/em&gt;和&lt;em&gt;E&lt;/em&gt;进行相同的类比。之后，算法检查节点&lt;em&gt;A&lt;/em&gt;。&lt;em&gt;这次，它满足BA&lt;/em&gt; &lt;em&gt;&amp;gt; AX 的&lt;/em&gt;条件。结果，新边&lt;em&gt;AX&lt;/em&gt;和两个初始区域彼此连接。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/10.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;左边的例子使用了朴素方法。右侧的示例使用启发式选择，这会导致两个初始不相交区域相互连接&lt;/em&gt;。&lt;/p&gt;
&lt;h2 id=&#34;复杂度-1&#34;&gt;复杂度&lt;/h2&gt;
&lt;p&gt;与搜索过程相比，插入过程的工作方式非常相似，没有任何可能需要非常数操作的显着差异。因此，插入单个顶点需要&lt;em&gt;O(logn)&lt;em&gt;的时间。为了估计总复杂度，应考虑给定数据集中所有插入节点&lt;/em&gt;n&lt;/em&gt;的数量。最终，HNSW 构建需要O(n * logn)时间。&lt;/p&gt;
&lt;h1 id=&#34;将-hnsw-与其他方法相结合&#34;&gt;将 HNSW 与其他方法相结合&lt;/h1&gt;
&lt;p&gt;HNSW 可以与其他相似性搜索方法一起使用，以提供更好的性能。最流行的方法之一是将其与倒排文件索引和乘积量化 ( &lt;em&gt;IndexIVFPQ&lt;/em&gt; ) 结合起来，这在本系列文章的其他部分中进行了描述。&lt;/p&gt;
&lt;p&gt;在此范例中，HNSW 扮演IndexIVFPQ 粗量化器(&lt;strong&gt;coarse quantizer&lt;/strong&gt;)的角色，这意味着它将负责查找最近的 Voronoi 分区，因此可以缩小搜索范围。为此，必须在所有 Voronoi 质心上构建 HNSW 索引。当给出查询时，HNSW 用于查找最近的 Voronoi 质心（而不是像以前那样通过比较到每个质心的距离进行强力搜索）。之后，查询向量在相应的 Voronoi 分区内进行量化，并使用 PQ 码计算距离。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/11.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;通过查找建立在 Voronoi 质心之上的 HNSW 中的最近邻居来选择最近的 Voronoi 质心&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;当仅使用倒排文件索引时，最好将 Voronoi 分区的数量设置得不要太大（例如 256 或 1024），因为会执行强力搜索来找到最近的质心。通过选择少量的 Voronoi 分区，每个分区内的候选者数量会变得相对较多。因此，该算法可以快速识别查询的最近质心，并且其大部分运行时间都集中在查找 Voronoi 分区内的最近邻居上。&lt;/p&gt;
&lt;p&gt;然而，将 HNSW 引入工作流程需要进行调整。考虑仅在少量质心（256 或 1024）上运行 HNSW：HNSW 不会带来任何显著的好处，因为在向量数量较少的情况下，HNSW 在执行时间方面与朴素的暴力搜索相对相同。此外，HNSW 需要更多内存来存储图结构。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这就是为什么在合并 HNSW 和倒排文件索引时，建议将 Voronoi 质心的数量设置得比平常大得多。通过这样做，每个 Voronoi 分区内的候选者数量变得少得多。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这种范式的转变导致了以下设置：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;HNSW 在对数时间内快速识别最近的 Voronoi 质心。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;之后，在各个 Voronoi 分区内执行穷举搜索。这应该不成问题，因为潜在候选人的数量很少。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;： 这种HNSW+IndexIVFPQ方式是最常用的方式，常结合场景数据结合硬件训练，调优，&lt;a href=&#34;https://github.com/harsha-simhadri/big-ann-benchmarks&#34;&gt;刷榜&lt;/a&gt;。来处理B级别以上的embedding向量数据；需要关注 &lt;strong&gt;recall 召回率&lt;/strong&gt;， &lt;strong&gt;内存使用率&lt;/strong&gt; 和 &lt;strong&gt;搜索速度&lt;/strong&gt;， 根据场景对这三方面进行取舍。（详情见： faiss复合索引 &lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_composite_indexes.ipynb&#34;&gt;&lt;strong&gt;faiss_composite_indexes.ipynb&lt;/strong&gt;&lt;/a&gt;）&lt;/p&gt;
&lt;h1 id=&#34;faiss实现&#34;&gt;FAISS实现&lt;/h1&gt;
&lt;p&gt;根据&lt;a href=&#34;https://faiss.ai/&#34;&gt;Faiss 文档&lt;/a&gt;中的信息，我们将了解如何利用 HNSW 并将其与倒排文件索引和乘积量化合并在一起。&lt;/p&gt;
&lt;h2 id=&#34;indexhnswflat&#34;&gt;IndexHNSWFlat&lt;/h2&gt;
&lt;p&gt;FAISS 有一个实现 HNSW 结构的&lt;em&gt;IndexHNSWFlat&lt;/em&gt;类。与往常一样，后缀“ &lt;em&gt;Flat&lt;/em&gt; ”表示数据集向量完全存储在索引中。构造函数接受 2 个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;d&lt;/strong&gt;：数据维度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M&lt;/strong&gt;：插入时每个新节点需要添加的边数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外，通过&lt;strong&gt;hnsw&lt;/strong&gt;字段，&lt;em&gt;IndexHNSWFlat&lt;/em&gt;提供了几个有用的属性（可以修改）和方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;hnsw.efConstruction&lt;/strong&gt;：构建过程中要探索的最近邻居的数量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hnsw.efSearch&lt;/strong&gt;：搜索期间要探索的最近邻居的数量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hnsw.max_level&lt;/strong&gt;：返回最大层。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hnsw.entry_point&lt;/strong&gt;：返回入口点。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;faiss.vector_to_array(index.hnsw.levels)&lt;/strong&gt;：返回每个向量的最大层列表&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hnsw.set_default_probas(M: int, level_mult: float)&lt;/strong&gt;：允许分别设置&lt;em&gt;M&lt;/em&gt;和&lt;em&gt;mL&lt;/em&gt;值。默认情况下，&lt;em&gt;level_mult&lt;/em&gt;设置为&lt;em&gt;1 / ln(M)&lt;/em&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/12.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;IndexHNSWFlat 的 Faiss 实现&lt;/em&gt; (注：图中的efConstruction 和 efSearch 注释弄反了)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;IndexHNSWFlat&lt;/em&gt;设置&lt;em&gt;Mₘₐₓ = M&lt;/em&gt;和&lt;em&gt;Mₘₐₓ₀ = 2 * M&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;indexhnswflat--indexivfpq&#34;&gt;IndexHNSWFlat + IndexIVFPQ&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;IndexHNSWFlat&lt;/em&gt;也可以与其他索引组合。其中一个示例是上一部分中描述的&lt;em&gt;IndexIVFPQ&lt;/em&gt; 。该综合索引的创建分两个步骤进行：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;IndexHNSWFlat&lt;/em&gt;被初始化为粗量化器。&lt;/li&gt;
&lt;li&gt;量化器作为参数传递给&lt;em&gt;IndexIVFPQ&lt;/em&gt;的构造函数。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;训练和添加可以通过使用不同或相同的数据来完成。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/hierarchical-navigable-small-world-hnsw/13.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;IndexHNSWFlat + IndexIVFPQ 的 FAISS 实现&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;结论&#34;&gt;结论&lt;/h1&gt;
&lt;p&gt;在本文中，我们研究了一种鲁棒算法，该算法特别适用于大型数据集向量。通过使用多层图表示和候选启发式选择，其搜索速度可以有效扩展，同时保持良好的预测精度。还值得注意的是，HNSW 可以与其他相似性搜索算法结合使用，使其非常灵活。&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Six_degrees_of_separation&#34;&gt;Six degrees of separation | Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Skip_list&#34;&gt;Skip List | Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1603.09320.pdf&#34;&gt;&lt;strong&gt;Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs&lt;/strong&gt;. Yu. A. Malkov, D. A. Yashunin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://faiss.ai/&#34;&gt;Faiss documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&#34;&gt;Summary of Faiss indexes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;除非另有说明，所有图片均由作者提供&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;原文地址：&lt;a href=&#34;https://towardsdatascience.com/similarity-search-part-4-hierarchical-navigable-small-world-hnsw-2aad4fe87d37&#34;&gt;https://towardsdatascience.com/similarity-search-part-4-hierarchical-navigable-small-world-hnsw-2aad4fe87d37&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;附操作学习笔记&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_hnsw.ipynb&#34;&gt;&lt;strong&gt;faiss_hnsw.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_vector_indexes.ipynb&#34;&gt;&lt;strong&gt;faiss_vector_indexes.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_composite_indexes.ipynb&#34;&gt;&lt;strong&gt;faiss_composite_indexes.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>译：相似性搜索，第 3 部分：混合倒排文件索引和乘积量化</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/3.blending-inverted-file-index-and-product-quantization/</link>
      <pubDate>Mon, 25 Sep 2023 11:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/3.blending-inverted-file-index-and-product-quantization/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;了解如何结合两个基本的相似性搜索索引以发挥两者的优势&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;在本系列的前两部分中，我们讨论了信息检索中的两种基本算法：&lt;strong&gt;倒排文件索引&lt;/strong&gt;和&lt;strong&gt;乘积量化&lt;/strong&gt;。它们都优化了搜索性能，但侧重于不同的方面：第一个加速了搜索速度，而后者将向量压缩为更小的、节省内存的表示形式。&lt;/p&gt;
&lt;p&gt;由于两种算法侧重于不同的方面，自然出现的问题是是否可以将这两种算法合并为一种新算法&lt;/p&gt;
&lt;p&gt;在本文中，我们将结合这两种方法的优点来产生快速且内存高效的算法。作为参考，大多数讨论的想法都将基于&lt;a href=&#34;https://inria.hal.science/inria-00514462v2/document&#34;&gt;本文&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;在深入研究细节之前，有必要了解残差向量(residual vectors)是什么，并对它们的有用属性有一个简单的直觉。稍后我们将在设计算法时使用它们。&lt;/p&gt;
&lt;h1 id=&#34;残差向量residual-vectors&#34;&gt;残差向量(Residual vectors)&lt;/h1&gt;
&lt;p&gt;想象一下执行了一个聚类算法并产生了多个聚类。每个簇都有一个质心和与其关联的点。&lt;strong&gt;残差residual&lt;/strong&gt;是点(向量)与其质心的偏移量。基本上，要找到特定向量的残差，必须从向量的质心中减去该向量。&lt;/p&gt;
&lt;p&gt;如果聚类是通过 k-means 算法构建的，则聚类质心是属于该聚类的所有点的平均值。因此，从任意点找到残差相当于从中减去簇的平均值。通过从属于特定簇的所有点中减去平均值，这些点将以 0 为中心。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;原始点簇显示在左侧。然后从所有聚类点中减去聚类质心。生成的残差向量显示在右侧。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;我们可以观察到一个有用的事实：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;用残差替换原始向量不会改变它们之间的相对位置。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也就是说，向量之间的距离始终保持不变。让我们简单地看一下下面的两个方程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;减去平均值不会改变相对距离&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;第一个方程是一对向量之间的欧氏距离公式。在第二个方程中，从两个向量中减去簇平均值。我们可以看到均值项简单地被抵消了——整个表达式变得与第一个方程中的欧几里得距离相同！&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我们通过使用 L2 度量（欧氏距离）公式证明了这一说法。重要的是要记住，此规则可能不适用于其他度量方法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;因此，如果对于给定的查询，目标是找到最近的邻居，则可以仅从查询中减去簇均值，然后继续在残差中进行正常的搜索过程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;从查询中减去平均值不会改变其与其他向量的相对位置&lt;/p&gt;
&lt;p&gt;现在让我们看一下下图中的另一个示例，其中有两个簇，其中每个簇向量的残差是单独计算的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;从簇的每个向量中减去相应质心的平均值将使所有数据集向量以 0 为中心&lt;/em&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这是一个有用的观察结果，将来会用到。此外，对于给定的查询，我们可以计算所有集群的查询残差。查询残差允许我们计算到簇的原始残差的距离。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;从每个簇中减去平均值后，所有点都以 0 为中心。查询和查询残差与相应簇的其他点的相对位置不会改变。&lt;/p&gt;
&lt;h1 id=&#34;训练training&#34;&gt;训练(Training)&lt;/h1&gt;
&lt;p&gt;考虑到上一节中有用的观察结果后，我们可以开始设计算法。&lt;/p&gt;
&lt;p&gt;给定一个向量数据库，构建一个倒排文件索引，将向量集划分为&lt;em&gt;n&lt;/em&gt;个Voronoi 分区，从而减少推理查询过程中的搜索范围。&lt;/p&gt;
&lt;p&gt;在每个 Voronoi 分区内，从每个向量中减去质心的坐标。结果，来自所有分区的向量变得彼此更接近并且以 0 为中心。此时，不需要原始向量，因为我们存储它们的残差。&lt;/p&gt;
&lt;p&gt;之后，对来自所有分区的向量运行乘积量化算法。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;重要的方面&lt;/em&gt;：乘积量化不是&lt;strong&gt;针对&lt;/strong&gt;每个分区单独执行的——这会降低效率，因为分区的数量通常很高，这可能会导致需要大量内存来存储所有代码本。相反，&lt;strong&gt;该算法是同时对所有分区的所有残差执行的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;实际上，现在每个子空间都包含来自不同 Voronoi 分区的子向量。然后，对于每个子空间，像往常一样执行聚类算法并构建&lt;em&gt;k&lt;/em&gt;个聚类及其质心。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;训练流程&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;用向量的残差替换向量的重要性是什么&lt;/em&gt;？如果向量不被它们的残差替换，那么每个子空间将包含更多不同的子向量（因为子空间将存储来自不同的非相交 Voronoi 分区的子向量，这些分区在空间中可能彼此相距很远）。现在，来自不同分区的向量（残差）相互重叠。由于现在每个子空间的多样性较少，因此有效表示向量所需的再现值较少。换句话说：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在与之前使用的相同长度的PQ码的情况下，向量可以更准确地表示，因为它们具有较小的方差。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;推理查询inference&#34;&gt;推理查询(Inference)&lt;/h1&gt;
&lt;p&gt;对于给定的查询，找到 Voronoi 分区的&lt;em&gt;k&lt;/em&gt;个最近的质心。这些区域内的所有点都被视为候选点。由于原始向量最初被每个 Voronoi 区域中的残差替换，因此还需要计算查询向量的残差。在这种情况下，需要为每个 Voronoi 分区单独计算查询残差（因为每个区域具有不同的质心）。只有所选 Voronoi 分区的残差才会分配给候选者。&lt;/p&gt;
&lt;p&gt;然后将查询残差分割成子向量。与原始乘积量化算法一样，对于每个子空间，计算包含从子空间质心到查询子向量的距离的距离矩阵&lt;em&gt;d&lt;/em&gt;。 必须记住，每个 Voronoi 分区的查询残差都是不同的。基本上，这意味着需要为每个查询残差单独计算距离矩阵&lt;em&gt;d&lt;/em&gt;。这是所需优化的计算成本。&lt;/p&gt;
&lt;p&gt;最后，像之前在乘积量化算法中所做的那样，对部分距离进行求和。&lt;/p&gt;
&lt;h2 id=&#34;排序结果sorting-results&#34;&gt;排序结果(Sorting results)&lt;/h2&gt;
&lt;p&gt;计算完所有距离后，需要选择&lt;em&gt;k&lt;/em&gt;个最近邻。为了有效地做到这一点，作者建议维护&lt;a href=&#34;https://medium.com/@slavahead/heapify-with-heap-sort-5df23b5764c1&#34;&gt;MaxHeap&lt;/a&gt; (&lt;strong&gt;注&lt;/strong&gt;：容量为k 的大顶堆用于查最小最近的k个值，小顶堆则相反)数据结构。它的容量有限为&lt;em&gt;k&lt;/em&gt;，并且在每一步中，它存储&lt;em&gt;k&lt;/em&gt;个当前最小距离。每当计算新距离时，仅当计算值小于 MaxHeap 中的最大值时，才会将其值添加到 MaxHeap 中。计算完所有距离后，查询的答案已存储在 MaxHeap 中。使用 MaxHeap 的优点是其快速构建时间为线性时间&lt;strong&gt;O(n)&lt;/strong&gt; 。（&lt;strong&gt;注&lt;/strong&gt;：高度 h=logn；2^h -h -1 = n - logn -1 =&amp;gt; O(n)）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/6.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;推理查询过程&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;性能performance&#34;&gt;性能(Performance)&lt;/h1&gt;
&lt;p&gt;该算法利用了倒排文件索引和乘积量化的优点。根据推理查询过程中 Voronoi 分区探测的数量，需要计算相同数量的子向量到质心矩阵&lt;em&gt;d&lt;/em&gt;并将其存储在内存中。这可能看起来像是一个缺点，但与整体优势相比，这是一个相当不错的权衡。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/7.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;该算法继承了倒排文件索引良好的搜索速度和乘积量化的压缩效率&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;faiss实现&#34;&gt;FAISS实现&lt;/h1&gt;
&lt;p&gt;根据&lt;a href=&#34;https://faiss.ai/&#34;&gt;Faiss 文档&lt;/a&gt;中的信息，我们将看到如何将倒排文件和产品量化索引组合在一起形成新的索引。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Faiss 在IndexIVFPQ&lt;/em&gt;类中实现所描述的算法，该类接受以下参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;quantizer&lt;/strong&gt;：指定如何计算向量之间的距离。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;d&lt;/strong&gt;：数据维度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nlist&lt;/strong&gt;：Voronoi 分区的数量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M&lt;/strong&gt;：子空间的数量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nbits&lt;/strong&gt;：编码单个簇 ID 所需的位数。这意味着单个子空间中的总簇数将等于&lt;em&gt;k = 2^nbits&lt;/em&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外，还可以调整&lt;strong&gt;nprobe&lt;/strong&gt;属性，该属性指定在推理查询期间必须使用多少个 Voronoi 分区来搜索候选者。更改此参数不需要重新训练。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/8.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;em&gt;IndexIVFPQ 的 Faiss 实现&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;存储单个向量所需的内存与原始乘积量化方法相同，只是现在我们添加了 8 个字节来存储有关倒排文件索引中向量的信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/blending-inverted-file-index-and-product-quantization/9.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;结论&#34;&gt;结论&lt;/h1&gt;
&lt;p&gt;利用前面文章部分的知识，我们已经完成了最先进的算法的实现，该算法实现了高内存压缩和加速的搜索速度。该算法广泛应用于处理海量数据的信息检索系统中。&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://inria.hal.science/inria-00514462v2/document&#34;&gt;paper: &lt;strong&gt;Product quantization for nearest neighbour search&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://faiss.ai/&#34;&gt;Faiss document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki&#34;&gt;&lt;strong&gt;Faiss wiki&lt;/strong&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&#34;&gt;Summary of Faiss indexes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;除非另有说明，所有图片均由作者提供&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;原文地址：&lt;a href=&#34;https://towardsdatascience.com/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa&#34;&gt;https://towardsdatascience.com/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;附操作学习笔记&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_vector_indexes.ipynb&#34;&gt;&lt;strong&gt;faiss_vector_indexes.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_composite_indexes.ipynb&#34;&gt;&lt;strong&gt;faiss_composite_indexes.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>译：相似性搜索，第 2 部分：乘积量化</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/2.product-quantization/</link>
      <pubDate>Mon, 25 Sep 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/2.product-quantization/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/0.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习有效压缩大数据的强大技术&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。有多种不同的方法可以提高海量数据的搜索性能。&lt;/p&gt;
&lt;p&gt;在本系列文章的&lt;a href=&#34;https://weedge.github.io/post/oneday/similarity-search/1.knn-inverted-file-index/&#34;&gt;第一部分&lt;/a&gt;中，我们研究了用于执行相似性搜索的 kNN 和倒排文件索引结构。正如我们所知，kNN 是最直接的方法，而倒排文件索引则在其之上发挥作用，建议在速度加速和准确性之间进行权衡。然而，这两种方法都不使用数据压缩技术，这可能会导致内存问题，特别是在数据集较大且 RAM 有限的情况下。在本文中，我们将尝试通过研究另一种称为“&lt;strong&gt;乘积量化(Product Quantization)&lt;/strong&gt;”的方法来解决此问题。&lt;/p&gt;
&lt;h1 id=&#34;定义&#34;&gt;定义&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;乘积量化&lt;/strong&gt;是将每个数据集向量转换为短的内存高效表示形式（称为&lt;strong&gt;PQ code&lt;/strong&gt;）的过程。不是完全保留所有向量，而是存储它们的短表示。同时，乘积量化是一种有损压缩方法，预测精度较低，但在实践中，该算法效果很好。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一般来说，量化是将无限值映射到离散值的过程。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;训练training&#34;&gt;训练(Training)&lt;/h1&gt;
&lt;p&gt;首先，该算法将每个向量分为几个相等的部分——&lt;strong&gt;子向量&lt;/strong&gt;。所有数据集向量的各个部分形成独立的&lt;strong&gt;子空间&lt;/strong&gt;并单独处理。然后对每个向量子空间执行聚类算法。通过这样做，在每个子空间中创建了几个质心。每个子向量都用它所属的质心的 ID 进行编码。此外，所有质心的坐标都会被存储以供以后使用。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;子空间质心也称为&lt;strong&gt;量化向量(quantized vectors)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在乘积量化中，簇ID通常被称为&lt;strong&gt;再现值(reproduction value)&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note: 在下图中，矩形表示包含多个值的向量，而正方形表示单个数字。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​											&lt;em&gt;使用量化进行编码&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;因此，如果一个原始向量被分为&lt;em&gt;n&lt;/em&gt;个部分，那么它可以由&lt;em&gt;n&lt;/em&gt;个数字——每个子向量各自质心的 ID 来编码。通常，为了更有效地使用内存，创建的质心&lt;em&gt;k&lt;/em&gt;的数量通常选择为 2 的幂。这样，存储编码向量所需的内存为n * log(k)位。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;子空间内所有质心的集合称为&lt;strong&gt;码本&lt;/strong&gt;。对所有子空间运行 n 个聚类算法会产生 n 个独立的码本。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;压缩示例compression-example&#34;&gt;压缩示例(Compression example)&lt;/h2&gt;
&lt;p&gt;想象一下，存储浮点数（32 位）的大小为 1024 的原始向量被分为&lt;em&gt;n = 8&lt;/em&gt;个子向量，其中每个子向量由&lt;em&gt;k = 256&lt;/em&gt;个簇之一进行编码。因此，对单个簇的 ID 进行编码需要&lt;em&gt;log(256) = 8&lt;/em&gt;位。让我们比较两种情况下向量表示的内存大小：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原始向量：1024 * 32 位 = 4096 字节。&lt;/li&gt;
&lt;li&gt;编码向量：8 * 8 位 = 8 字节。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最终压缩512倍！这就是产品量化的真正力量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​					&lt;em&gt;量化示例(向量中的数字显示它存储了多少个数字)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;以下是一些重要的注意事项：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该算法可以在一个向量子集上进行训练（例如，以创建聚类）并用于另一个向量子集：训练完算法后，将传递另一个向量数据集，其中通过使用每个子空间已构造的质心对新向量进行编码。&lt;/li&gt;
&lt;li&gt;通常，选择 k-means 作为聚类算法。它的优点之一是簇的数量&lt;em&gt;k&lt;/em&gt;是一个超参数，可以根据内存使用需求手动定义。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;推理查询inference&#34;&gt;推理查询(Inference)&lt;/h1&gt;
&lt;p&gt;为了更好地理解，让我们首先看一下几种简单的方法并找出它们的缺点。这也将帮助我们了解为什么它们不应该被正常使用。&lt;/p&gt;
&lt;h2 id=&#34;朴素方法naive-approaches&#34;&gt;朴素方法(Naive approaches)&lt;/h2&gt;
&lt;p&gt;第一种简单方法包括通过连接每个向量相应的质心来解压缩所有向量。之后，可以计算从查询向量到所有数据集向量的&lt;em&gt;L2&lt;/em&gt;距离（或其他度量）。显然，这种方法是可行的，但是非常耗时，因为要对高维解压向量进行暴力搜索和距离计算。&lt;/p&gt;
&lt;p&gt;另一种可能的方式是将查询向量分割成子向量，并基于其PQ码计算从每个查询子向量到数据库向量的相应量化向量的距离之和。因此，再次使用暴力搜索技术，并且这里的距离计算仍然需要原始向量维数的线性时间，如前一情况所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​						&lt;em&gt;使用朴素方法计算近似距离（该示例以欧氏距离作为度量）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;另一种可能的方法是将查询向量编码成PQ码。然后直接利用该 PQ 代码来计算到所有其他 PQ 代码的距离。然后，具有最短距离的相应 PQ 代码的数据集向量被视为查询的最近邻居。这种方法比前两种方法更快，因为距离始终是在低维 PQ 代码之间计算的。然而，PQ码由簇ID组成，没有太多语义，可以被视为明确用作实数变量的分类变量。显然，这是一种不好的做法，并且这种方法可能会导致预测质量较差。&lt;/p&gt;
&lt;h2 id=&#34;优化方法&#34;&gt;优化方法&lt;/h2&gt;
&lt;p&gt;查询向量被划分为子向量。对于每个子向量，计算到相应子空间的所有质心的距离。最终，该信息存储在表&lt;em&gt;d&lt;/em&gt;中。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​							&lt;em&gt;获取存储部分查询子向量到质心距离的表 d&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;计算出的子向量到质心的距离通常称为&lt;strong&gt;部分距离(partial distances)&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通过使用这个子向量到质心距离表&lt;em&gt;d&lt;/em&gt;，可以通过其 PQ 代码轻松获得从查询到任何数据库向量的近似距离：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于数据库向量的每个子向量，找到最近的质心&lt;em&gt;j&lt;/em&gt;（通过使用 PQ 代码的映射值）以及从该质心到查询子向量&lt;em&gt;i&lt;/em&gt;的部分距离&lt;em&gt;d[i][j]&lt;/em&gt;（通过使用计算的矩阵&lt;em&gt;d&lt;/em&gt;）被获取。&lt;/li&gt;
&lt;li&gt;所有部分距离均被平方并求和。通过对该值求平方根，即可获得近似的欧氏距离。如果还想了解如何获得其他度量的近似结果，见&lt;a href=&#34;http://weedge.github.io/post/oneday/similarity-search/2.product-quantization/#%E5%85%B6%E4%BB%96%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E7%9A%84%E8%BF%91%E4%BC%BC%E5%80%BCapproximation-of-other-distance-metrics&#34;&gt;&lt;em&gt;其他距离度量的近似值&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​						&lt;em&gt;使用 PQ 代码和距离表计算从查询到数据库向量的距离&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;使用此方法计算近似距离假设：部分距离&lt;strong&gt;d&lt;/strong&gt;非常接近查询和数据库子向量之间的实际距离&lt;strong&gt;a 。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;然而，这个条件可能不被满足，特别是当数据库子向量与其质心之间的距离*c很大时。*在这种情况下，计算会导致精度降低。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/6.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;左边的例子展示了当实际距离非常接近部分距离（c 很小）时的近似情况。在右侧，我们可以观察到一个糟糕的场景，因为部分距离比实际距离长得多（c很大）。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;在获得所有数据库行的近似距离后，我们搜索具有最小值的向量。这些向量将是查询的最近邻。&lt;/p&gt;
&lt;h2 id=&#34;其他距离度量的近似值approximation-of-other-distance-metrics&#34;&gt;其他距离度量的近似值（Approximation of other distance metrics）&lt;/h2&gt;
&lt;p&gt;到目前为止，我们已经了解了如何使用部分距离来近似欧氏距离。让我们也将这一规则推广到其他度量方式。&lt;/p&gt;
&lt;p&gt;想象一下我们想要计算一对向量之间的距离度量。如果我们知道度量的公式，我们可以直接应用它来得到结果。但有时我们可以通过以下方式分部分完成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;两个向量都分为&lt;em&gt;n&lt;/em&gt;个子向量。&lt;/li&gt;
&lt;li&gt;对于每对相应的子向量，计算距离度量。&lt;/li&gt;
&lt;li&gt;然后将计算出的&lt;em&gt;n 个&lt;/em&gt;度量组合起来，生成原始向量之间的实际距离。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/7.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;该图显示了计算度量的两种方法。在左侧，度量公式直接应用于两个向量。在右侧，计算每对相应子向量的部分距离。然后使用聚合函数 h、g 和 f 将它们组合起来。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;欧几里德距离是可以按部分计算度量的。根据上图，我们可以选择聚合函数为&lt;em&gt;h(z) = z²&lt;/em&gt;、&lt;em&gt;g(z₀, z₁, …, zₙ) = sum(z₀, z₁, …, zₙ)&lt;em&gt;和&lt;/em&gt;f(z) = √z&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/8.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​								&lt;em&gt;欧氏距离可以分部分计算&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;内积(IP-Inner product )是此类度量的另一个示例，具有聚合函数&lt;em&gt;h(z) = z、g(z₀, z₁, …, zₙ) = sum(z₀, z₁, …, zₙ) 和 f(z) = z&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;在乘积量化的背景下，这是一个非常重要的属性，因为在推理查询过程中，算法按部分计算距离。这意味着使用不具有此属性的度量进行乘积量化会出现更多问题。余弦距离是此类度量的一个示例。&lt;/p&gt;
&lt;p&gt;如果仍然需要使用没有此属性的度量，则需要应用额外的启发式方法来聚合具有一定误差的部分距离。&lt;/p&gt;
&lt;h2 id=&#34;性能performance&#34;&gt;性能(Performance)&lt;/h2&gt;
&lt;p&gt;乘积量化的主要优点是对存储为短 PQ 代码的数据库向量进行大规模压缩。对于某些应用来说，这样的压缩率甚至可能高于95%！然而，除了PQ码之外，还需要存储包含每个子空间的量化向量的大小为&lt;em&gt;k x n&lt;/em&gt;的矩阵&lt;em&gt;d&lt;/em&gt; 。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;乘积量化是一种有损压缩方法，因此压缩率越高，预测精度就越有可能下降。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;构建有效表示的系统需要训练多种聚类算法。除此之外，在推理查询过程中，需要以暴力方式计算&lt;em&gt;k * n&lt;/em&gt;个部分距离，并对每个数据库向量求和，这可能需要一些时间。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/9.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​											&lt;em&gt;产品量化性能&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;faiss-实现&#34;&gt;FAISS 实现&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;&lt;strong&gt;Faiss&lt;/strong&gt;&lt;/a&gt;（Facebook AI Search Comparison）是一个用 C++ 编写, binding Python使用的faiss库, 提供c api 库方便FFI交互，用于优化相似性搜索。该库提供了不同类型的索引，这些索引是用于有效存储数据和执行查询的数据结构。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;根据&lt;a href=&#34;https://faiss.ai/&#34;&gt;Faiss 文档&lt;/a&gt;中的信息，我们将了解如何利用乘积量化。&lt;/p&gt;
&lt;p&gt;乘积量化在&lt;em&gt;IndexPQ&lt;/em&gt;类中实现。为了初始化，我们需要为其提供 3 个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;d&lt;/strong&gt;：数据的维度数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M&lt;/strong&gt; ：每个向量的分割数（与上面使用的&lt;em&gt;n&lt;/em&gt;参数相同）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nbits&lt;/strong&gt;：编码单个簇 ID 所需的位数。这意味着单个子空间中的总簇数将等于&lt;em&gt;k = 2^nbits&lt;/em&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于相等的子空间维度分裂，参数&lt;em&gt;d&lt;/em&gt;必须能被&lt;em&gt;M&lt;/em&gt;整除。&lt;/p&gt;
&lt;p&gt;存储单个向量所需的总字节数等于：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/10.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;正如我们在上面的公式中看到的，为了更有效地使用内存，M * nbits 的值应该能被&lt;em&gt;8&lt;/em&gt;整除。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/product-quantization/11.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;​												&lt;em&gt;IndexPQ 的 Faiss 实现&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;结论&#34;&gt;结论&lt;/h1&gt;
&lt;p&gt;我们研究了信息检索系统中非常流行的算法，该算法可以有效地压缩大量数据。它的主要缺点是推理查询速度慢。尽管如此，该算法仍广泛应用于现代大数据应用中，特别是与其他相似性搜索技术结合使用。&lt;/p&gt;
&lt;p&gt;在本系列文章的第一部分中，我们描述了倒排文件索引的工作流程。事实上，我们可以将这两种算法合并成一种更高效的算法，从而兼具两者的优点！这正是我们在本系列的下一部分中要做的事情。&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://inria.hal.science/inria-00514462v2/document&#34;&gt;paper: &lt;strong&gt;Product quantization for nearest neighbour search&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://faiss.ai/&#34;&gt;Faiss document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki&#34;&gt;&lt;strong&gt;Faiss wiki&lt;/strong&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&#34;&gt;Summary of Faiss indexes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/series/faiss/product-quantization/&#34;&gt;https://www.pinecone.io/learn/series/faiss/product-quantization/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;原文地址： &lt;a href=&#34;https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701&#34;&gt;https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;附操作学习笔记&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_product_quantization.ipynb&#34;&gt;&lt;strong&gt;faiss_product_quantization.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>译：相似性搜索，第 1 部分：kNN 和倒排文件索引</title>
      <link>https://weedge.github.io/post/oneday/similarity-search/1.knn-inverted-file-index/</link>
      <pubDate>Sun, 24 Sep 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/similarity-search/1.knn-inverted-file-index/</guid>
      
        <description>&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;相似性搜索&lt;/strong&gt;(similarity-search)是给定一个查询，目标是在所有数据库文档中找到与其最相似的文档。本章介绍 kNN 的相似性搜索及其使用倒排文件的加速。&lt;/p&gt;
&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在数据科学中，相似性搜索经常出现在 NLP 领域、搜索引擎或推荐系统中，其中需要检索最相关的文档或项目以进行查询。通常，文档或项目以文本或图像的形式表示。然而，机器学习算法不能直接处理原始文本或图像，这就是为什么文档和项目通常被预处理并存储为数字向量的原因。&lt;/p&gt;
&lt;p&gt;有时向量的每个分量都可以存储语义。在这种情况下，这些表示也称为&lt;strong&gt;嵌入&lt;/strong&gt;。这样的嵌入可以有数百个维度，数量可以达到数百万个！由于数量如此庞大，任何信息检索系统都必须能够快速检测相关文档。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在机器学习中，向量也称为&lt;strong&gt;对象&lt;/strong&gt;或&lt;strong&gt;点&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;指数&#34;&gt;指数&lt;/h1&gt;
&lt;p&gt;为了加速搜索性能，在数据集嵌入之上构建了特殊的数据结构。这样的数据结构称为&lt;strong&gt;索引&lt;/strong&gt;。该领域已经进行了大量的研究，并演化出了多种类型的指标。在选择索引来应用特定任务之前，有必要了解它的幕后运作方式，因为每个索引都有不同的目的，并且都有自己的优点和缺点。&lt;/p&gt;
&lt;p&gt;在本文中，我们将看看最简单的方法:&lt;strong&gt;kNN&lt;/strong&gt;。基于 kNN，我们将切换到&lt;strong&gt;倒排文件&lt;/strong&gt;:一种用于更具可扩展性的搜索的索引，可以将搜索过程加速数倍。&lt;/p&gt;
&lt;h1 id=&#34;knn&#34;&gt;kNN&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;kNN&lt;/strong&gt;是最简单、最朴素的相似性搜索算法。考虑向量数据集和新的查询向量&lt;em&gt;Q&lt;/em&gt;。我们希望找到与&lt;em&gt;Q&lt;/em&gt;最相似的前&lt;em&gt;k 个&lt;/em&gt;数据集向量。首先要考虑的方面是如何测量两个向量之间的相似性（距离）。事实上，有几个相似性指标可以做到这一点。其中一些如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;相似度指标&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;训练&#34;&gt;训练&lt;/h2&gt;
&lt;p&gt;kNN 是机器学习中少数不需要训练阶段的算法之一。选择合适的指标后，我们可以直接进行预测。&lt;/p&gt;
&lt;h2 id=&#34;推理&#34;&gt;推理&lt;/h2&gt;
&lt;p&gt;对于一个新对象，该算法会详尽地计算到所有其他对象的距离。之后，它找到距离最小的&lt;em&gt;k&lt;/em&gt;个对象并将它们作为响应返回。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;kNN工作流程&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;显然，通过检查到所有数据集向量的距离，kNN 保证了 100% 准确的结果。然而，这种蛮力方法在时间性能方面非常低效。如果数据集由&lt;em&gt;m&lt;/em&gt;个维度的&lt;em&gt;n&lt;/em&gt;个向量组成，则对于每个&lt;em&gt;n&lt;/em&gt;个向量，需要&lt;em&gt;O(m)&lt;em&gt;时间来计算从查询&lt;/em&gt;Q&lt;/em&gt;到它的距离，这导致总时间复杂度为&lt;em&gt;O(mn)&lt;/em&gt;。正如我们稍后将看到的，存在更有效的方法。&lt;/p&gt;
&lt;p&gt;而且，原始向量没有压缩机制。想象一个包含数十亿个对象的数据集。将它们全部存储在 RAM 中是不可能的！&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;kNN 性能。具有 100% 的准确度且无需训练阶段，可在向量的推理和无内存压缩期间进行详尽的搜索。注：此类图显示了不同算法的相对比较。根据情况和选择的超参数，性能可能会有所不同。&lt;/p&gt;
&lt;h2 id=&#34;应用&#34;&gt;应用&lt;/h2&gt;
&lt;p&gt;kNN 的应用范围有限，仅应在以下场景之一使用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据集大小或嵌入维数相对较小。这方面确保了算法仍然能够快速执行。&lt;/li&gt;
&lt;li&gt;算法要求的准确度必须是100%。在准确性方面，没有其他最近邻算法可以超越 kNN。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据指纹检测人员就是需要 100% 准确度的问题的一个例子。如果此人犯罪并留下了指纹，则仅检索正确的结果至关重要。否则，如果系统不是 100% 可靠，那么另一个人可能会被判有罪，这是一个非常严重的错误。&lt;/p&gt;
&lt;p&gt;基本上，改进 kNN 的方法主要有两种（我们稍后会讨论）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缩小搜索范围。&lt;/li&gt;
&lt;li&gt;降低向量的维数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当使用这两种方法之一时，我们将不会再次执行详尽的搜索。此类算法称为**近似最近邻 (ANN)，**因为它们不能保证 100% 准确的结果。&lt;/p&gt;
&lt;h1 id=&#34;倒排文件索引&#34;&gt;倒排文件索引&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;“倒排索引&lt;/strong&gt;（也称为&lt;strong&gt;倒排列表&lt;/strong&gt;postings list/file、&lt;strong&gt;倒排文件&lt;/strong&gt;inverted file）是一种数据库索引，存储从内容（例如单词或数字）到其在表、文档或一组数据中的位置的映射”——维基百科&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;执行查询时，会计算查询的哈希函数并从哈希表中获取映射值。这些映射值中的每一个都包含其自己的一组潜在候选值，然后根据条件对其进行全面检查，使其成为查询的最近邻居。通过这样做，缩小了所有数据库向量的搜索范围。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;倒排文件索引工作流程&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;根据哈希函数的计算方式，该索引有不同的实现方式。我们将要看到的实现是使用&lt;strong&gt;Voronoi 图&lt;/strong&gt;（或&lt;strong&gt;Dirichlet tessellation&lt;/strong&gt;）的实现。&lt;/p&gt;
&lt;h2 id=&#34;训练-1&#34;&gt;训练&lt;/h2&gt;
&lt;p&gt;该算法的思想是创建每个数据集点所属的几个不相交的区域。每个区域都有自己的质心，指向该区域的中心。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;有时&lt;strong&gt;Voronoi 区域&lt;/strong&gt;被称为&lt;strong&gt;单元&lt;/strong&gt;或&lt;strong&gt;分区&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;Voronoi 图的示例。白点是包含一组候选的各个分区的中心。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Voronoi 图的主要属性是从一个质心到其区域内任意点的距离小于从该点到另一个质心的距离。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;推理-1&#34;&gt;推理&lt;/h2&gt;
&lt;p&gt;当给定一个新对象时，将计算到所有 Voronoi 分区质心的距离。然后选择距离最小的质心，并将该分区中包含的向量作为候选向量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/6.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;通过给定的查询，我们搜索最近的质心（位于绿色区域）&lt;/p&gt;
&lt;p&gt;最终，通过计算到候选者的距离并选择其中最接近的前&lt;em&gt;k&lt;/em&gt;个，返回最终答案。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/7.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;查找所选区域中的最近邻居&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;正如您所看到的，这种方法比前一种方法快得多，因为我们不必查看所有数据集向量。&lt;/p&gt;
&lt;h2 id=&#34;边缘问题&#34;&gt;边缘问题&lt;/h2&gt;
&lt;p&gt;随着搜索速度的提高，倒排文件也有一个缺点：它不能保证找到的对象始终是最近的。&lt;/p&gt;
&lt;p&gt;在下图中，我们可以看到这样的场景：实际的最近邻居位于红色区域，但我们仅从绿色区域中选择候选者。这种情况称为&lt;strong&gt;边缘问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/8.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;边缘问题&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;当查询的对象位于与另一个区域的边界附近时，通常会发生这种情况。为了减少这种情况下的错误数量，我们可以扩大搜索范围，并根据与对象最接近的前&lt;em&gt;m个质心选择几个区域来搜索候选区域。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/9.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;搜索多个区域内的最近邻居 (m = 3)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;探索的区域越多，结果就越准确，计算结果所需的时间也就越多。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：这个类似geohash搜索中的边缘问题&lt;/p&gt;
&lt;h2 id=&#34;应用-1&#34;&gt;应用&lt;/h2&gt;
&lt;p&gt;尽管存在边缘问题，基于训练的倒排文件在实践中仍显示出不错的结果。当我们想要以稍微降低精度来实现速度数倍增长的情况下，它是完美的选择。&lt;/p&gt;
&lt;p&gt;用例示例之一是基于内容的推荐系统。想象一下，它根据用户过去看过的其他电影向他推荐一部电影。该数据库包含一百万部电影可供选择。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过使用 kNN，系统确实为用户选择了最相关的电影并推荐。然而，执行查询所需的时间非常长。&lt;/li&gt;
&lt;li&gt;让我们假设通过倒排文件索引，系统推荐第五个最相关的电影，这可能是现实生活中的情况。搜索时间比 kNN 快 20 倍。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从用户体验来看，很难区分这两个推荐的质量结果：第一个和第五个最相关的结果都是来自一百万个可能的电影的良好推荐。用户可能会对这些建议中的任何一个感到满意。从时间角度来看，倒排文件显然是赢家。这就是为什么在这种情况下最好使用后一种方法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/10.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;倒排文件索引性能。这里我们稍微降低精度以在推理过程中获得更高的速度。&lt;/p&gt;
&lt;h1 id=&#34;faiss实现&#34;&gt;FAISS实现&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;&lt;strong&gt;Faiss&lt;/strong&gt;&lt;/a&gt;（Facebook AI Search Comparison）是一个用 C++ 编写, binding Python使用的faiss库, 提供c api 库方便FFI交互，用于优化相似性搜索。该库提供了不同类型的索引，这些索引是用于有效存储数据和执行查询的数据结构。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://faiss.ai/&#34;&gt;根据Faiss 文档&lt;/a&gt;中的信息，我们将了解索引是如何创建和参数化的。&lt;/p&gt;
&lt;h2 id=&#34;knn-1&#34;&gt;kNN&lt;/h2&gt;
&lt;p&gt;实现 kNN 方法的索引在 Faiss 中被称为&lt;strong&gt;扁平索引(flat index)&lt;/strong&gt;，因为它们不压缩任何信息。它们是保证正确搜索结果的唯一索引。实际上Faiss中存在两种类型的扁平索引：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;IndexFlatL2&lt;/em&gt;。相似度计算为欧几里得距离。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;IndexFlatIP&lt;/em&gt;。相似度计算为内积。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这两个索引在其构造函数中都需要一个参数&lt;strong&gt;d&lt;/strong&gt;：数据维度。这些索引没有任何可调参数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/11.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;IndexFlatL2 和 IndexFlatIP 的 Faiss 实现&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;存储向量的单个分量需要 4 个字节。因此，要存储维度为 d 的&lt;strong&gt;单个向量&lt;/strong&gt;，需要&lt;em&gt;4 * d字节。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/12.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;倒排文件索引-1&#34;&gt;倒排文件索引&lt;/h2&gt;
&lt;p&gt;对于所描述的倒排文件，Faiss 实现了类&lt;em&gt;IndexIVFFlat&lt;/em&gt;。与 kNN 的情况一样，“ &lt;em&gt;Flat&lt;/em&gt; ”一词表示原始向量没有解压缩并且它们被完全存储。&lt;/p&gt;
&lt;p&gt;要创建此索引，我们首先需要传递一个量化器 - 一个将确定如何存储和比较数据库向量的对象。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;IndexIVFFlat&lt;/em&gt;有2个重要参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;nlist&lt;/strong&gt;：定义在训练期间创建的多个区域（Voronoi 单元）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nprobe&lt;/strong&gt;：确定要搜索候选区域的区域数。更改 nprobe 参数不需要重新训练。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/13.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;IndexIVFFlat 的 Faiss 实现&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;与前面的情况一样，我们需要&lt;em&gt;4 * d&lt;/em&gt;字节来存储单个向量。但现在我们还必须存储数据集向量所属的 Voronoi 区域的信息。在 Faiss 实现中，此信息每个向量占用 8 个字节(可调整)。因此，存储&lt;strong&gt;单个向量&lt;/strong&gt;所需的内存为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/knn-inverted-file-index/14.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;结论&#34;&gt;结论&lt;/h1&gt;
&lt;p&gt;我们已经了解了相似性搜索中的两种基本算法。实际上，朴素 kNN 几乎不应该用于机器学习应用，因为除了特定情况外，它的可扩展性很差。另一方面，倒排文件为加速搜索提供了良好的启发式方法，其质量可以通过调整其超参数来提高。搜索性能仍然可以从不同的角度来提高。在本系列文章的下一部分中，我们将了解一种旨在压缩数据集向量的方法。&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Inverted_index&#34;&gt;Inverted Index | Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Voronoi_diagram&#34;&gt;Voronoi diagram | Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://faiss.ai/&#34;&gt;Faiss documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&#34;&gt;Summary of Faiss indexes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index&#34;&gt;Guideline for choosing an index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/series/faiss/faiss-tutorial/&#34;&gt;faiss-tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/cn/what-is/hyperparameter-tuning/&#34;&gt;https://aws.amazon.com/cn/what-is/hyperparameter-tuning/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注：&lt;em&gt;除非另有说明，所有图片均由原作者提供。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;原文：https://towardsdatascience.com/similarity-search-knn-inverted-file-index-7cab80cc0e79&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;附操作学习笔记&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_tutorial.ipynb&#34;&gt;&lt;strong&gt;faiss_tutorial.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/weedge/doraemon-nb/blob/main/faiss_vector_indexes.ipynb&#34;&gt;&lt;strong&gt;faiss_vector_indexes.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>译：FANN：200行Rust实现的向量搜索</title>
      <link>https://weedge.github.io/post/oneday/vector-search-in-200-lines-of-rust/</link>
      <pubDate>Wed, 20 Sep 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/vector-search-in-200-lines-of-rust/</guid>
      
        <description>&lt;p&gt;由于 AI/ML 采用的快速进展，向量数据库无处不在。虽然它们支持复杂的人工智能/机器学习应用，但向量搜索本身从概念上来说并不难。在这篇文章中，我们将描述向量数据库如何工作，并用不到 200 行 Rust 代码构建一个简单的向量搜索库。&lt;a href=&#34;https://github.com/fennel-ai/fann&#34;&gt;所有代码都可以在此 Github 存储库&lt;/a&gt;中找到。我们这里使用的方法基于流行库Spotify &lt;a href=&#34;https://github.com/spotify/annoy&#34;&gt;annoy&lt;/a&gt;中使用的一系列称为“&lt;a href=&#34;https://en.wikipedia.org/wiki/Locality-sensitive_hashing&#34;&gt;局部敏感散列(Locality-sensitive_hashing)&lt;/a&gt;”的算法。本文的目标不是介绍新的算法库，而是描述向量搜索如何使用真实的代码片段工作。首先了解下什么是向量搜索。&lt;/p&gt;
&lt;h2 id=&#34;向量简介又名嵌入embedding&#34;&gt;向量简介（又名嵌入embedding）&lt;/h2&gt;
&lt;p&gt;文档、图像、视频等复杂的非结构化数据很难在传统数据库中表示和查询，特别是如果查询意图是查找“相似”项目。那么 Youtube 如何才能选择接下来播放的最佳视频呢？或者Spotify根据您当前的歌曲自定义音乐队列？&lt;/p&gt;
&lt;p&gt;2010 年代初人工智能的进步（从&lt;a href=&#34;https://en.wikipedia.org/wiki/Word2vec&#34;&gt;Word2Vec&lt;/a&gt; 和 &lt;a href=&#34;https://en.wikipedia.org/wiki/GloVe&#34;&gt;GloVe&lt;/a&gt; &lt;a href=&#34;https://github.com/stanfordnlp/GloVe&#34;&gt;stanfordnlp-GloVe&lt;/a&gt; 开始）使我们能够构建这些对象的语义表示，其中它们被表示为笛卡尔空间中的点。假设一个视频映射到点 [0.1, -5.1, 7.55]，另一个视频映射到点 [5.3, -0.1, 2.7]。这些机器学习算法的神奇之处在于，这些表示的选择能够维护语义信息——两个视频越相似，它们的向量之间的距离就越小。&lt;/p&gt;
&lt;p&gt;请注意，这些向量（或更专业地称为嵌入）不必是 3 维的 - 它们可以并且通常位于更高维的空间中（例如 128 维或 750 维）。而且距离也不需要是欧几里德距离 - 其他形式的距离（例如点积）也可以。无论哪种方式，重要的是它们之间的距离与其相似性相对应。&lt;/p&gt;
&lt;p&gt;现在想象一下，我们可以访问所有 Youtube 视频的此类向量。我们如何找到与给定起始视频最相似的视频？简单的。循环遍历所有视频，计算它们之间的距离并选择距离最小的视频 - 也称为查找查询视频的“最近邻居”。这实际上会起作用。不过，正如您所猜测的，线性 O(N) 扫描的成本可能太高。因此，我们需要一种更快的亚线性方法来找到任何查询视频的最近邻居。这通常是不可能的——必须付出一些代价。&lt;/p&gt;
&lt;p&gt;事实证明，在实际情况中，我们不需要找到最近的视频- 找到足够近的视频也可以。这就是近似最近邻搜索算法（也称为向量搜索）的用武之地。目标是亚线性（理想情况下以对数时间）找到空间中任何点的足够近的最近邻。那么如何解决呢？&lt;/p&gt;
&lt;h2 id=&#34;如何找到近似最近邻居&#34;&gt;如何找到近似最近邻居？&lt;/h2&gt;
&lt;p&gt;所有向量搜索算法背后的基本思想都是相同的——进行一些预处理来识别彼此足够接近的点（有点类似于构建索引）。在查询时，使用这个“索引”来排除大片点。并在不被排除的少量点内进行线性扫描。&lt;/p&gt;
&lt;p&gt;然而，有很多方法可以实现这个简单的想法。存在几种最先进的向量搜索算法，例如&lt;a href=&#34;https://github.com/nmslib/hnswlib&#34;&gt;HNSW&lt;/a&gt;（一种连接邻近顶点并通过固定入口点维护长距离边的分层图，类似skiplist）。目前存在诸如 Facebook 的&lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;FAISS&lt;/a&gt;之类的开源项目，以及诸如&lt;a href=&#34;https://www.pinecone.io/&#34;&gt;Pinecone&lt;/a&gt;，&lt;a href=&#34;https://weaviate.io/&#34;&gt;Weaviate&lt;/a&gt;，&lt;a href=&#34;https://github.com/zilliztech/knowhere&#34;&gt;zilliz-Milvus-knowhere&lt;/a&gt;等高可用性向量数据库的 PaaS 产品中。&lt;/p&gt;
&lt;p&gt;在这篇文章中，我们将在给定的“N”点上构建一个简化的向量搜索索引，如下所示：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;随机取 2 个任意可用向量 A 和 B。&lt;/li&gt;
&lt;li&gt;计算这两个向量之间的中点，称为 C。&lt;/li&gt;
&lt;li&gt;构建一个穿过 C 并垂直于连接 A 和 B 的线段的超平面（类似于高维中的“线”）。&lt;/li&gt;
&lt;li&gt;将所有向量分类为超平面“上方”或“下方”，将可用向量分为 2 组。&lt;/li&gt;
&lt;li&gt;对于两个组中的每一个：如果组的大小高于可配置参数“最大节点大小”，则在该组上递归调用此过程以构建子树。否则，使用所有向量（或其唯一的 ID）构建单个叶节点&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因此，我们使用这个随机过程来构建一棵树，其中每个内部节点都是超平面定义，左子树是超平面“下方”的所有向量，右子树是超平面“上方”的所有向量。向量集被连续递归地分割，直到叶节点包含不超过“最大节点大小”向量。考虑下图的例子，有五点：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/vector-search-in-200-lines-of-rust/1.png&#34; alt=&#34;img&#34;&gt;图 1：用随机超平面分割空间&lt;/p&gt;
&lt;p&gt;我们随机选择向量A1=(4,2)，B1=(5,7)。它们的中点是 (4.5,4.5)，我们通过中点构建一条垂直于线 (A1, B1) 的线。该线是 x + 5y=27（用蓝色绘制），这给了我们一组 2 个向量和一组 4 个向量。假设“最大节点大小”配置为 2。我们不进一步拆分第一组，而是选择后者构建新的（A2，B2）红色超平面等等。对大型数据集进行重复分割会将超空间分割成几个不同的区域，如下所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/vector-search-in-200-lines-of-rust/2.png&#34; alt=&#34;img&#34;&gt;图 2：许多超平面后的分段空间（来自 &lt;a href=&#34;https://t.co/K0Xlht8GwQ&#34;&gt;https://t.co/K0Xlht8GwQ&lt;/a&gt;，作者：&lt;a href=&#34;https://twitter.com/bernhardsson&#34;&gt;&lt;strong&gt;Erik Bernhardsson&lt;/strong&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;这里的每个区域代表一个叶节点，并且这里的直觉是足够接近的点很可能最终出现在同一个叶节点中。因此，给定一个查询点，我们可以在对数时间内遍历树以找到它所属的叶子，并对该叶子中的所有（少量）点运行线性扫描。这显然不是万无一失的——实际上足够近的点完全有可能被超平面分开并最终彼此相距很远。但是这个问题可以通过构建不是一棵而是许多独立的树来解决 - 这样，如果两个点足够接近，它们更有可能位于至少某些树中的同一叶节点中。在查询时，我们遍历所有树以找到相关的叶节点，对所有叶节点的所有候选节点进行并集，&lt;/p&gt;
&lt;p&gt;好吧，理论已经足够了。让我们开始编写一些代码，首先为下面的 Rust 中的 Vector 类型定义一些实用程序，用于点积、平均、散列和平方 L2 距离。感谢 Rust 良好的类型系统，我们传播泛型类型参数 N 来强制索引中的所有向量在编译时具有相同的维度。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#[derive(Eq, PartialEq, Hash)]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;HashKey&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;u32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;#[derive(Copy, Clone)]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;f32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;impl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;subtract_from&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mapped&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;).&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;coords&lt;/span&gt;: &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;f32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mapped&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;collect&lt;/span&gt;::&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;try_into&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unwrap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;coords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;avg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mapped&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;).&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;coords&lt;/span&gt;: &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;f32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mapped&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;collect&lt;/span&gt;::&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;try_into&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unwrap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;coords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;dot_product&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;kt&#34;&gt;f32&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zipped_iter&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zipped_iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;).&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;::&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;f32&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;to_hashkey&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;HashKey&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// f32 in Rust doesn&amp;#39;t implement hash. We use bytes to dedup. While it
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// can&amp;#39;t differentiate ~16M ways NaN is written, it&amp;#39;s safe for us
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bit_iter&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_bits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;: &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;u32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bit_iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;collect&lt;/span&gt;::&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;try_into&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unwrap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;HashKey&lt;/span&gt;::&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sq_euc_dis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;kt&#34;&gt;f32&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zipped_iter&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zipped_iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;).&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;powi&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)).&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;构建完这些核心实用程序后，我们还可以定义超平面的外观：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;HyperPlane&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;coefficients&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;f32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;impl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;HyperPlane&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;point_is_above&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;point&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;coefficients&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot_product&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;point&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.0&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;接下来，让我们重点关注生成随机超平面并构建最近邻树森林。我们应该如何表示树中的点？&lt;/p&gt;
&lt;p&gt;我们可以直接将 D 维向量存储在叶节点内。但这会显着增加大 D 的内存碎片（主要性能损失），并且当多棵树引用相同的向量时，还会在森林中创建重复的内存。相反，我们将向量存储在全局连续位置，并在叶节点处保存“usize”大小的索引（在 64 位系统上为 8 字节，而不是 4D，其中 f32 占用 4 字节）。以下是用于表示树的内部节点和叶节点的数据类型。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;k&#34;&gt;enum&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;Node&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Inner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Box&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;InnerNode&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Leaf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Box&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LeafNode&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;LeafNode&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;InnerNode&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hyperplane&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;HyperPlane&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;left_node&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Node&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;right_node&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Node&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;ANNIndex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;trees&lt;/span&gt;: &lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Node&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ids&lt;/span&gt;: &lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;: &lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们如何真正找到正确的超平面？&lt;/p&gt;
&lt;p&gt;我们对向量 A 和 B 的两个唯一索引进行采样，计算 n = A - B，并找到 A 和 B 的中点 (point_on_plane)。超平面通过系数（向量 n）和常数（n 和 point_on_plane 的点积）结构有效存储为 n(x-x0) = nx - nx0。我们可以在任何向量和 n 之间执行点积，并减去常数以将向量放置在超平面“上方”或“下方”。由于树中的内部节点保存超平面定义，而叶节点保存向量 ID，因此我们可以使用 ADT 对树进行类型检查：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;k&#34;&gt;impl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ANNIndex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;build_hyperplane&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;indexes&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_vecs&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;HyperPlane&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;: &lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;indexes&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choose_multiple&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rand&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;thread_rng&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;collect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// cartesian eq for hyperplane n * (x - x_0) = 0
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// n (normal vector) is the coefs x_1 to x_n
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;coefficients&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subtract_from&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;point_on_plane&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;avg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;coefficients&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot_product&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;point_on_plane&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hyperplane&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;HyperPlane&lt;/span&gt;::&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;coefficients&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;above&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;below&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;vec!&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[],&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;vec!&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[]);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;in&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;indexes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hyperplane&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;point_is_above&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;above&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;push&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;below&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;push&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hyperplane&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;above&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;below&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;因此，我们可以定义递归过程来基于索引时间“最大节点大小”构建树：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;k&#34;&gt;impl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ANNIndex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;build_a_tree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_size&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;indexes&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_vecs&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;Node&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;indexes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_size&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;as&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Node&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;Leaf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Box&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;new&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LeafNode&lt;/span&gt;::&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;indexes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;clone&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())));&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plane&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;above&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;below&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;build_hyperplane&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;indexes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;node_above&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;build_a_tree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;above&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;node_below&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;build_a_tree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;below&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Node&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;Inner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Box&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;new&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;InnerNode&lt;/span&gt;::&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hyperplane&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;plane&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;left_node&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;node_below&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;right_node&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;node_above&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}));&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;   
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;请注意，在两点之间构建超平面要求这两个点是唯一的 - 即我们必须在索引之前对向量集进行重复数据删除，因为该算法不允许重复。&lt;/p&gt;
&lt;p&gt;因此整个索引（树木的森林）可以这样构建：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;k&#34;&gt;impl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ANNIndex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;deduplicate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vectors&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ids&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dedup_vectors&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ids_of_dedup_vectors&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hashes_seen&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;HashSet&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;new&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;in&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;..&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vectors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hash_key&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vectors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_hashkey&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hashes_seen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hash_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hashes_seen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;insert&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hash_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dedup_vectors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;push&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vectors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ids_of_dedup_vectors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;push&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;build_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_trees&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_size&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vecs&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vec_ids&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;ANNIndex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unique_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;vec!&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[],&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;vec!&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[]);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;deduplicate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vec_ids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unique_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// Trees hold an index into the [unique_vecs] list which is not
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// necessarily its id, if duplicates existed
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_indexes&lt;/span&gt;: &lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;..&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unique_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()).&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;collect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;trees&lt;/span&gt;: &lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;..&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_trees&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;build_a_tree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_indexes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unique_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;collect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ANNIndex&lt;/span&gt;::&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;trees&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;unique_vecs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;查询时间&#34;&gt;查询时间&lt;/h3&gt;
&lt;p&gt;一旦建立了索引，我们如何使用它来搜索单个树上输入向量的 K 个近似最近邻？在非叶节点，我们存储超平面，因此我们可以从树的根开始并询问：“这个向量是在这个超平面之上还是之下？”。这可以通过 O(D) 和点积来计算。根据响应，我们可以递归搜索左子树或右子树，直到找到叶节点。请记住，叶节点最多存储“最大节点大小”向量，这些向量位于输入向量的近似邻域中（因为它们落在所有超平面下的超空间的同一分区中，见图 1(b)）。如果该叶节点处的向量索引数量超过 K，我们现在可以按与输入向量的 L2 距离对所有这些向量进行排序，并返回最接近的 K！&lt;/p&gt;
&lt;p&gt;假设我们的索引导致平衡树，对于维度 D、向量数量 N 和最大节点大小 M &amp;laquo; N，搜索需要 O(Dlog(N) + DM + Mlog(M)) - 这构成了平均最差情况 log(N)  次比较超平面以查找叶节点（即树的高度）；其中每次比较都会花费 O(D) 点积，计算 O(DM) 中叶节点中所有候选向量的 L2 度量；最后对它们进行排序以返回 O(Mlog(M)) 中的前 K 个。&lt;/p&gt;
&lt;p&gt;但是，如果我们找到的叶节点的向量少于 K 个，会发生什么情况？如果最大节点大小太小或者超平面分割相对不均匀，子树中留下的向量很少，则这是可能的。为了解决这个问题，我们可以在树搜索中添加一些简单的回溯功能。例如，如果返回的候选数不够，我们可以在内部节点进行额外的递归调用来访问另一个分支。它可能是这样的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;k&#34;&gt;impl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ANNIndex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;tree_result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tree&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Node&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;candidates&lt;/span&gt;: &lt;span class=&#34;kp&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;HashSet&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// take everything in node, if still needed, take from alternate subtree
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;match&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tree&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Node&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;Leaf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;box_leaf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;leaf_values&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;box_leaf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_candidates_found&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;as&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;leaf_values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;in&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;..&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_candidates_found&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;candidates&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;insert&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;leaf_values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_candidates_found&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;as&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Node&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;Inner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;above&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;).&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hyperplane&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;point_is_above&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;match&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;above&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                    &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;right_node&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;left_node&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)),&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                    &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;left_node&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;right_node&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)),&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;match&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;tree_result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;candidates&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;tree_result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;candidates&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;请注意，我们可以通过在子树中存储向量总数，以及直接指向每个内部节点的所有叶节点的指针列表来进一步优化递归调用，但为了简单起见，这里不这样做。&lt;/p&gt;
&lt;p&gt;将此搜索扩展到树木森林很简单 - 只需从所有树中独立收集前 K 个候选者，按距离对它们进行排序，然后返回总体前 K 个匹配项。请注意，更多数量的树将具有线性高的内存占用和线性缩放的搜索时间，但可以导致更好的“更接近”的邻居，因为我们跨不同的树收集候选者。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;k&#34;&gt;impl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ANNIndex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;pub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;search_approximate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Vector&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;top_k&lt;/span&gt;: &lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;i32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;f32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;candidates&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;HashSet&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;new&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tree&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;in&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;trees&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;tree_result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;top_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;candidates&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;candidates&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;into_iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sq_euc_dis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sorted_by&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;partial_cmp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;).&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unwrap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;take&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;top_k&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;as&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;usize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;collect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这为我们提供了 200 行 Rust 的简单向量搜索索引！&lt;/p&gt;
&lt;p&gt;为了说明的目的，这个实现相当简单——事实上，它是如此简单，以至于我们怀疑它一定比最先进的算法差得多（尽管在更广泛的方法中是相似的）。让我们做一些基准测试来证实我们的怀疑。&lt;/p&gt;
&lt;p&gt;可以评估算法的延迟和质量。质量通常通过召回率来衡量 - 实际最近邻（从线性扫描获得）的百分比，也是通过近似最近邻搜索获得的。有时，返回的结果在技术上并不在前 K 中，但它们非常接近实际的前 K，因此并不重要 - 为了量化这一点，我们还可以查看邻居的平均欧几里德距离，并将其与暴力平均距离进行比较强制搜索。&lt;/p&gt;
&lt;p&gt;测量延迟很简单 - 我们可以查看执行查询所需的时间（我们通常对索引构建延迟不太感兴趣）。&lt;/p&gt;
&lt;p&gt;所有基准测试结果均在配备 2.3 GHz 四核 Intel Core i5 处理器的单设备 CPU 上运行，使用 999,994 个 Wiki 数据 FastText 嵌入 ( &lt;a href=&#34;https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip&#34;&gt;https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news -300d-1M.vec.zip&lt;/a&gt; ) 300 维。我们将所有查询的“top K”设置为 20。&lt;/p&gt;
&lt;p&gt;作为参考，我们将 FAISS HNSW 索引 (ef_search=16、ef_construction=40、max_node_size=15) 与 Rust 索引的小版本 (num_trees=3、max_node_size=15) 进行比较。我们在 Rust 中实现了详尽的搜索，而 FAISS 库有 HNSW 的 C++ 源代码。原始延迟数低，增强了近似搜索的优势：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;算法&lt;/th&gt;
&lt;th&gt;延迟 Latency&lt;/th&gt;
&lt;th&gt;QPS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Exhaustive Search&lt;/td&gt;
&lt;td&gt;675.25ms&lt;/td&gt;
&lt;td&gt;1.48&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FAISS HNSW Index&lt;/td&gt;
&lt;td&gt;355.36μs&lt;/td&gt;
&lt;td&gt;2814.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Custom Rust Index&lt;/td&gt;
&lt;td&gt;112.02μs&lt;/td&gt;
&lt;td&gt;8926.98&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;两种近似最近邻方法的速度都快了三个数量级，这很好。与 HNSW 相比，我们的 Rust 实现在这个微基准测试中速度快了 3 倍。分析质量时，直观地考虑 prompt “river” 返回的前 10 个最近邻。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Exhaustive Search&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;FAISS HNSW Index&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Custom Rust Index&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Euclidean Distance&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Euclidean Distance&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Euclidean Distance&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;river&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;river&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;river&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;River&lt;/td&gt;
&lt;td&gt;1.39122&lt;/td&gt;
&lt;td&gt;River&lt;/td&gt;
&lt;td&gt;1.39122&lt;/td&gt;
&lt;td&gt;creek&lt;/td&gt;
&lt;td&gt;1.63744&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;rivers&lt;/td&gt;
&lt;td&gt;1.47646&lt;/td&gt;
&lt;td&gt;river-&lt;/td&gt;
&lt;td&gt;1.58342&lt;/td&gt;
&lt;td&gt;river.&lt;/td&gt;
&lt;td&gt;1.73224&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;river-&lt;/td&gt;
&lt;td&gt;1.58342&lt;/td&gt;
&lt;td&gt;swift-flowing&lt;/td&gt;
&lt;td&gt;1.62413&lt;/td&gt;
&lt;td&gt;lake&lt;/td&gt;
&lt;td&gt;1.75655&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;swift-flowing&lt;/td&gt;
&lt;td&gt;1.62413&lt;/td&gt;
&lt;td&gt;flood-swollen&lt;/td&gt;
&lt;td&gt;1.63798&lt;/td&gt;
&lt;td&gt;sea&lt;/td&gt;
&lt;td&gt;1.87368&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;creek&lt;/td&gt;
&lt;td&gt;1.63744&lt;/td&gt;
&lt;td&gt;river.The&lt;/td&gt;
&lt;td&gt;1.68156&lt;/td&gt;
&lt;td&gt;up-river&lt;/td&gt;
&lt;td&gt;1.92088&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;flood-swollen&lt;/td&gt;
&lt;td&gt;1.63798&lt;/td&gt;
&lt;td&gt;river-bed&lt;/td&gt;
&lt;td&gt;1.68510&lt;/td&gt;
&lt;td&gt;shore&lt;/td&gt;
&lt;td&gt;1.92266&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;river.The&lt;/td&gt;
&lt;td&gt;1.68156&lt;/td&gt;
&lt;td&gt;unfordable&lt;/td&gt;
&lt;td&gt;1.69245&lt;/td&gt;
&lt;td&gt;brook&lt;/td&gt;
&lt;td&gt;2.01973&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;river-bed&lt;/td&gt;
&lt;td&gt;1.68510&lt;/td&gt;
&lt;td&gt;River-&lt;/td&gt;
&lt;td&gt;1.69512&lt;/td&gt;
&lt;td&gt;hill&lt;/td&gt;
&lt;td&gt;2.03419&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;unfordable&lt;/td&gt;
&lt;td&gt;1.69245&lt;/td&gt;
&lt;td&gt;River.The&lt;/td&gt;
&lt;td&gt;1.69539&lt;/td&gt;
&lt;td&gt;pond&lt;/td&gt;
&lt;td&gt;2.04376&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;或者，考虑一下prompt  “war”。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Exhaustive Search&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;FAISS HNSW Index&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Custom Rust Index&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Euclidean Distance&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Euclidean Distance&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Euclidean Distance&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;war&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;war&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;war&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;war&amp;ndash;&lt;/td&gt;
&lt;td&gt;1.38416&lt;/td&gt;
&lt;td&gt;war&amp;ndash;&lt;/td&gt;
&lt;td&gt;1.38416&lt;/td&gt;
&lt;td&gt;war&amp;ndash;&lt;/td&gt;
&lt;td&gt;1.38416&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;war&amp;ndash;a&lt;/td&gt;
&lt;td&gt;1.44906&lt;/td&gt;
&lt;td&gt;war&amp;ndash;a&lt;/td&gt;
&lt;td&gt;1.44906&lt;/td&gt;
&lt;td&gt;wars&lt;/td&gt;
&lt;td&gt;1.45859&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wars&lt;/td&gt;
&lt;td&gt;1.45859&lt;/td&gt;
&lt;td&gt;wars&lt;/td&gt;
&lt;td&gt;1.45859&lt;/td&gt;
&lt;td&gt;quasi-war&lt;/td&gt;
&lt;td&gt;1.59712&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;war&amp;ndash;and&lt;/td&gt;
&lt;td&gt;1.45907&lt;/td&gt;
&lt;td&gt;war&amp;ndash;and&lt;/td&gt;
&lt;td&gt;1.45907&lt;/td&gt;
&lt;td&gt;war-footing&lt;/td&gt;
&lt;td&gt;1.69175&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;war.It&lt;/td&gt;
&lt;td&gt;1.46991&lt;/td&gt;
&lt;td&gt;war.It&lt;/td&gt;
&lt;td&gt;1.46991&lt;/td&gt;
&lt;td&gt;check-mate&lt;/td&gt;
&lt;td&gt;1.74982&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;war.In&lt;/td&gt;
&lt;td&gt;1.49632&lt;/td&gt;
&lt;td&gt;war.In&lt;/td&gt;
&lt;td&gt;1.49632&lt;/td&gt;
&lt;td&gt;ill-begotten&lt;/td&gt;
&lt;td&gt;1.75498&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;unwinable&lt;/td&gt;
&lt;td&gt;1.51296&lt;/td&gt;
&lt;td&gt;unwinable&lt;/td&gt;
&lt;td&gt;1.51296&lt;/td&gt;
&lt;td&gt;subequent&lt;/td&gt;
&lt;td&gt;1.76617&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;war.And&lt;/td&gt;
&lt;td&gt;1.51830&lt;/td&gt;
&lt;td&gt;war.And&lt;/td&gt;
&lt;td&gt;1.51830&lt;/td&gt;
&lt;td&gt;humanitary&lt;/td&gt;
&lt;td&gt;1.77464&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hostilities&lt;/td&gt;
&lt;td&gt;1.54783&lt;/td&gt;
&lt;td&gt;Iraw&lt;/td&gt;
&lt;td&gt;1.54906&lt;/td&gt;
&lt;td&gt;execution&lt;/td&gt;
&lt;td&gt;1.77992&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;对于整个 999,994 个单词的语料库，我们还可视化了 HNSW 和自定义 Rust 索引下每个单词到其顶部 K=20 个近似邻居的平均欧几里得距离的分布：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/vector-search-in-200-lines-of-rust/3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;最先进的 HNSW 指数确实提供了比我们的示例索引相对更近的邻居，平均距离和中位距离分别为 1.31576 和 1.20230（与我们的示例索引的 1.47138 和 1.35620 相比）。在随机的 10,000 大小的语料库子集上，HNSW 对前 K=20 的召回率为 58.2%，而我们的示例索引针对不同的配置（如前所述，树的数量较多）产生了不同的召回率（从 11.465% 到 23.115%）提供更高的召回率）：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;num_trees&lt;/th&gt;
&lt;th&gt;max_node_size&lt;/th&gt;
&lt;th&gt;Average runtime&lt;/th&gt;
&lt;th&gt;QPS&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;129.48μs&lt;/td&gt;
&lt;td&gt;7723&lt;/td&gt;
&lt;td&gt;0.11465&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;112.02μs&lt;/td&gt;
&lt;td&gt;8297&lt;/td&gt;
&lt;td&gt;0.11175&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;114.48μs&lt;/td&gt;
&lt;td&gt;8735&lt;/td&gt;
&lt;td&gt;0.09265&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;16.77ms&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;0.22095&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;1.54ms&lt;/td&gt;
&lt;td&gt;649&lt;/td&gt;
&lt;td&gt;0.20985&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;370.80μs&lt;/td&gt;
&lt;td&gt;2697&lt;/td&gt;
&lt;td&gt;0.16835&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;35.45ms&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;0.29825&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;7.34ms&lt;/td&gt;
&lt;td&gt;136&lt;/td&gt;
&lt;td&gt;0.28520&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;2.19ms&lt;/td&gt;
&lt;td&gt;457&lt;/td&gt;
&lt;td&gt;0.23115&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;为什么fann这么快&#34;&gt;为什么FANN这么快？&lt;/h2&gt;
&lt;p&gt;正如您在上面的数字中看到的，虽然 FANN 算法在质量上无法与最先进的算法竞争，但它至少相当快。为什么会这样？&lt;/p&gt;
&lt;p&gt;老实说，当我们构建这个时，我们得意忘形并开始进行性能优化只是为了好玩。以下是一些最显着的优化：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将文档重复数据删除卸载到索引冷路径。通过索引而不是浮点数组引用向量可以显着加快搜索速度，因为跨树查找唯一候选者需要散列 8 字节索引（而不是 300 维 f32 数据）。&lt;/li&gt;
&lt;li&gt;在将项目添加到全局候选列表之前，急切地散列并查找唯一向量，并通过递归搜索调用之间的可变引用传递数据，以避免跨堆栈帧和堆栈帧内进行复制。&lt;/li&gt;
&lt;li&gt;将 N 作为通用类型参数传递，这允许将 300 维数据作为 300 长度的 f32 数组（而不是可变长度向量类型）进行类型检查，以提高缓存局部性并减少内存占用（向量具有堆上数据的附加重定向级别）。&lt;/li&gt;
&lt;li&gt;我们还怀疑 Rust 编译器正在对内部操作（例如点积）进行向量化，但我们没有检查。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;更多现实世界的考虑&#34;&gt;更多现实世界的考虑&lt;/h2&gt;
&lt;p&gt;此示例跳过了几个对于生产向量搜索至关重要的注意事项：(&lt;strong&gt;注&lt;/strong&gt;：单实例 cpu指令集优化 向量矩阵如axv2，SIMD + OpenMP； 分布式数据存储扩展 RPC + 分布式一致性协议)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当搜索涉及多棵树时进行并行化。我们可以并行化，而不是按顺序收集候选者，因为每棵树访问不同的内存 - 每棵树可以在单独的线程上运行，其中候选者通过消息沿着通道连续发送到主进程。线程可以在索引时生成，并通过虚拟搜索（使树的部分位于缓存中）进行预热，以减少搜索开销。搜索将不再随树的数量线性缩放。&lt;/li&gt;
&lt;li&gt;大型树可能不适合 RAM，需要有效的方法从磁盘读取 - 某些子图可能需要位于磁盘上，并且算法旨在允许搜索，同时最大限度地减少文件 I/O。&lt;/li&gt;
&lt;li&gt;更进一步，如果树不适合实例的磁盘，我们需要跨实例分布子树，并且如果数据在本地不可用，则递归搜索调用会触发一些 RPC 请求。&lt;/li&gt;
&lt;li&gt;该树涉及许多内存重定向（基于指针的树不适合 L1 缓存）。平衡树可以用数组很好地编写，但我们的树只能用随机超平面接近平衡——我们可以为树使用新的数据结构吗？&lt;/li&gt;
&lt;li&gt;当新数据被动态索引时（可能需要对大树进行重新分片），上述问题的解决方案也应该适用。如果特定的索引顺序导致树高度不平衡，是否应该重新创建树？&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;如果你到达这里，恭喜你！您刚刚看到了大约 200 行 Rust 中的简单向量搜索，以及我们对行星规模应用程序的向量搜索的漫谈。我们希望您喜欢阅读本文，并随时访问&lt;a href=&#34;https://github.com/fennel-ai/fann&#34;&gt;https://github.com/fennel-ai/fann&lt;/a&gt;的源代码。&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;注&lt;/strong&gt;：实验性质，运行下benchmark.sh 对比faiss hnsw了解下原理, faiss hnsw可以参数调优， LSH 可用于生产环境的库可参考&lt;a href=&#34;https://github.com/FALCONN-LIB/FALCONN&#34;&gt;FALCONN-LIB&lt;/a&gt;实现, 对 K，L,  T 调优，参考&lt;a href=&#34;https://github.com/FALCONN-LIB/FALCONN/wiki/LSH-Primer&#34;&gt;Locality-Sensitive Hashing: a Primer&lt;/a&gt;, 另外一个&lt;a href=&#34;https://github.com/ritchie46/lsh-rs&#34;&gt;lsh-rs&lt;/a&gt; 库也可以参考，&lt;a href=&#34;https://github.com/bwindsor22/thistle&#34;&gt;thistle&lt;/a&gt; 则参考了&lt;a href=&#34;https://github.com/ritchie46/lsh-rs&#34;&gt;lsh-rs&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/jean-pierreBoth/hnswlib-rs&#34;&gt;hnswlib-rs&lt;/a&gt;的实现，不过都不支持动态更新索引）&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Locality-sensitive_hashing&#34;&gt;https://en.wikipedia.org/wiki/Locality-sensitive_hashing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fennel.ai/blog/vector-search-in-200-lines-of-rust/&#34;&gt;https://fennel.ai/blog/vector-search-in-200-lines-of-rust/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://erikbern.com/2015/09/24/nearest-neighbor-methods-vector-models-part-1&#34;&gt;https://erikbern.com/2015/09/24/nearest-neighbor-methods-vector-models-part-1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html&#34;&gt;https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://erikbern.com/2016/06/02/approximate-nearest-news&#34;&gt;https://erikbern.com/2016/06/02/approximate-nearest-news&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ritchievink.com/blog/2020/04/07/sparse-neural-networks-and-hash-tables-with-locality-sensitive-hashing/&#34;&gt;https://www.ritchievink.com/blog/2020/04/07/sparse-neural-networks-and-hash-tables-with-locality-sensitive-hashing/&lt;/a&gt; &lt;a href=&#34;https://github.com/ritchie46/lsh-rs&#34;&gt;lsh-rs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.16780.pdf&#34;&gt;arxiv paper - Thistle: A Vector Database in Rust&lt;/a&gt; &lt;a href=&#34;https://github.com/bwindsor22/thistle&#34;&gt;thistle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/FALCONN-LIB/FALCONN/wiki/LSH-Primer&#34;&gt;https://github.com/FALCONN-LIB/FALCONN/wiki/LSH-Primer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2206.01382.pdf&#34;&gt;Falconn++: A Locality-sensitive Filtering Approach for Approximate Nearest Neighbor Search&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：Manas：高性能定制搜索系统</title>
      <link>https://weedge.github.io/post/oneday/manas-a-high-performing-customized-search-system/</link>
      <pubDate>Thu, 14 Sep 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/manas-a-high-performing-customized-search-system/</guid>
      
        <description>&lt;h1 id=&#34;章节一-manas高性能定制搜索系统&#34;&gt;章节一 Manas：高性能定制搜索系统&lt;/h1&gt;
&lt;p&gt;Pinterest 搜索每月处理数十亿次查询，每天返回近 40 亿个 Pin 图。去年，每月移动文本搜索量增长了 40%，视觉搜索量增长了近 60%。最近，通过在主页上推出 &lt;a href=&#34;https://blog.pinterest.com/en/search-and-lens-move-front-and-center&#34;&gt;Search 和 Lens&lt;/a&gt;，使它们在的应用程序中更加突出和集中，因为现在近 85% 的搜索都发生在移动设备上。&lt;/p&gt;
&lt;p&gt;为了继续扩展搜索，系统需要为每个 Pinner 在超过 1000 亿个 Pin 图中找到最相关的结果。此前，搜索系统是建立在 Lucene 之上并用 Java 编写的。但随着业务发展和引入新的发现功能，遗留系统面临着挑战，无法再支持。这就是构建 Manas 的原因，这是一个用 C++ 编写的定制全栈搜索系统，可以在提高容量的同时显着减少延迟。在这篇文章中，将概述 Manas 的架构，并了解 Pinterest 搜索的下一步发展。&lt;/p&gt;
&lt;h2 id=&#34;挑战&#34;&gt;&lt;strong&gt;挑战&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;随着 Pinterest 上的搜索使用量快速增长，基于 &lt;a href=&#34;https://github.com/apache/lucene&#34;&gt;Lucene&lt;/a&gt; 的解决方案日益面临挑战，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;查询量和索引大小增长如此之快，以至于需要减少服务延迟并提高容量。&lt;/li&gt;
&lt;li&gt;除了搜索之外，该系统还为 Pinterest 内的多个用例提供支持，包括 Pinner 搜索、图板搜索、相关 Pin 图、主页推送推荐等。需要灵活地定制搜索过程，这在以前是不可能的。&lt;/li&gt;
&lt;li&gt;希望将该系统应用于复杂而强大的排名模型，但 Lucene 索引格式和评分器界面不适合这些模型。&lt;/li&gt;
&lt;li&gt;还希望个性化搜索结果，这是标准 Lucene 系统无法支持的。&lt;/li&gt;
&lt;li&gt;构建 Manas 来解决这些挑战。Manas被设计为一个具有高性能、高可用性和高可扩展性的通用搜索框架。与旧系统相比，搜索后端延迟减少了一半，容量增加了30%。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;p&gt;Manas 是一个全栈搜索索引和服务系统。服务系统由几个阶段组成：查询理解、候选检索、轻量级评分、全面评分和混合。&lt;/p&gt;
&lt;h2 id=&#34;索引&#34;&gt;索引&lt;/h2&gt;
&lt;h3 id=&#34;索引格式&#34;&gt;&lt;strong&gt;索引格式&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Manas索引包括倒排索引和正向索引。&lt;/p&gt;
&lt;p&gt;与普通倒排索引相同，Manas倒排索引存储term到帖子列表的映射。每个发布都会记录内部文档 ID 和有效负载。为了优化索引大小和服务延迟，实现了密集倒排列表和分割倒排列表，这是根据所有文档中关键term的分布对倒排列表进行编码的两种方法。倒排索引用于候选生成和轻量级评分。&lt;/p&gt;
&lt;p&gt;另一方面，Manas的正向索引存储了从内部文档ID到实际文档的映射。为了优化数据局部性，前向索引支持列族，类似于HFile。前向指数用于全面评分。&lt;/p&gt;
&lt;h3 id=&#34;manas-doc&#34;&gt;&lt;strong&gt;Manas doc&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;将Manas doc定义为不同应用程序的统一模式，用于描述他们想要为每个文档索引哪些数据。在Manas文档中，可以指定匹配的term进行检索，并且可以添加文档的属性以进行过滤和轻量级评分。例如，系统在按语言属性过滤结果后只能返回英文文档。&lt;/p&gt;
&lt;h3 id=&#34;索引构建器&#34;&gt;&lt;strong&gt;索引构建器&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;索引构建器采用一批 Manas 文档并构建索引。定义了统一的 Manas 文档架构，以便可以为不同的用例共享索引构建器。&lt;/p&gt;
&lt;h3 id=&#34;索引管道&#34;&gt;&lt;strong&gt;索引管道&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图说明了索引管道。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;不同的应用程序为其语料库生成 Manas 文档。&lt;/li&gt;
&lt;li&gt;Manas 文档被划分为多个组。&lt;/li&gt;
&lt;li&gt;索引构建器将分区中的所有 Manas 文档转换为索引段。每个索引段都是完整索引的一小部分。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;服务&#34;&gt;服务&lt;/h2&gt;
&lt;p&gt;下图展示了Manas的搜索周期。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;当查询进入系统时会发生以下情况：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;查询理解服务处理原始查询并生成执行计划。&lt;/li&gt;
&lt;li&gt;语料库由服务树提供。Blender 将请求扇出到不同语料库的根，收集这些不同的结果并将它们混合。将这些混合结果存储在缓存中以进行分页。&lt;/li&gt;
&lt;li&gt;Root 是一种分散-聚集服务。它聚合叶子的结果并对它们重新排序。&lt;/li&gt;
&lt;li&gt;Leaf 首先加载由索引管道构建的索引段。它检索候选人并进行轻量级和全面评分。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;叶子服务&#34;&gt;叶子服务&lt;/h3&gt;
&lt;p&gt;Manas Leaf 是可扩展的，允许定制多个不同的应用程序。这是通过在索引中封装特定于应用程序的信息来实现的。可以embedding特定于应用程序的评分逻辑，以便 Manas 在对文档评分时仅执行应用程序执行的任务。&lt;/p&gt;
&lt;p&gt;服务架构设计为多层，层与层之间定义良好的接口，使得每一层都是可扩展的。Leaf节点的架构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上所述，存储层负责加载索引并提供抽象，允许在给定标识符的情况下获取连续的大量二进制数据。这一层允许轻松地改变索引的底层存储。在存储层之上，索引层将二进制数据解码为索引，并提供读取索引的接口。倒排列表层使能够灵活地实现倒排索引。算子层定义了用于实现查询算子的接口，模型运行器定义了用于全面评分的模型接口。最后，API 层指定叶节点评估的查询格式。&lt;/p&gt;
&lt;h2 id=&#34;候选检索和轻量级评分&#34;&gt;候选检索和轻量级评分&lt;/h2&gt;
&lt;h3 id=&#34;wand&#34;&gt;WAND&lt;/h3&gt;
&lt;p&gt;除了支持普通的“AND”、“OR”和“NOT”操作之外，还在 Leaf 中构建了“Weak And”支持（&lt;a href=&#34;https://www.semanticscholar.org/paper/Efficient-query-evaluation-using-a-two-level-Broder-Carmel/89d27fc4c5bf15762d001a39f0a74f84c89d3681&#34;&gt;paper&lt;/a&gt;)。这使能够快速跳过posting list。&lt;/p&gt;
&lt;h3 id=&#34;squery&#34;&gt;Squery&lt;/h3&gt;
&lt;p&gt;使用Squery 以树的形式表示结构化查询。它描述了 Leaf 如何从索引中检索候选者并对其进行轻量级评分。Leaf 理解 Squery 并在索引上执行它。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是 Squery 要求 Leaf 检索纯英文文档并匹配term“cute”和“cat”或“kitten”的示例。如果文档的点击率较高，则得分较高。&lt;/p&gt;
&lt;h3 id=&#34;满分&#34;&gt;满分&lt;/h3&gt;
&lt;p&gt;不同的应用程序使用不同的算法来计算最终分数。为了使 Manas 具有通用性，引入了前向索引，它是一个二进制 blob，可以是任何东西。实际上，前向索引是一个序列化的 Thrift 对象。Manas 不会解释前向索引，而是将其注入到 DSL 模型中并执行 DSL 模型来计算分数。DSL 是 Pinterest 使用的一种领域特定语言，用于定制从前向索引中提取特征，并选择机器学习模型来根据提取的特征计算分数。不同的应用程序可以创建不同的 DSL 模型并指定应注入哪个前向索引。&lt;/p&gt;
&lt;h3 id=&#34;ssd&#34;&gt;SSD&lt;/h3&gt;
&lt;p&gt;具有相当大的前向索引以支持复杂的评分算法，总索引大小显着增加。为了支持未来更复杂的评分，将向索引添加更多信号。将所有索引加载到内存中是不可扩展的，因此 Manas 仅加载用于候选检索和轻量级评分的倒排索引，并从 SSD 和本地缓存提供正向索引。&lt;/p&gt;
&lt;h3 id=&#34;索引交换&#34;&gt;索引交换&lt;/h3&gt;
&lt;p&gt;定期执行索引管道来构建索引。一旦新索引准备就绪，就会从 AWS 分配新实例来创建集群。将新索引部署到新创建的集群。然后 Blender 会将流量切换到新集群，旧集群将被弃用。&lt;/p&gt;
&lt;h1 id=&#34;章节二-manas-realtime--使更改能够在眨眼间被搜索到&#34;&gt;章节二 Manas Realtime — 使更改能够在眨眼间被搜索到&lt;/h1&gt;
&lt;p&gt;Manas 是 Pinterest 的内部搜索引擎，是一个通用信息检索平台。正如在&lt;a href=&#34;https://medium.com/pinterest-engineering/manas-a-high-performing-customized-search-system-cf189f6ca40f&#34;&gt;上一篇文章&lt;/a&gt;中讨论的那样，Manas 被设计为具有高性能、可用性和可扩展性的搜索框架。如今，Manas 为大多数 Pinterest 产品提供搜索功能，包括广告、搜索、Homefeed、相关 Pin 图、视觉效果和购物。&lt;/p&gt;
&lt;p&gt;搜索系统的关键指标之一是索引延迟，即更新搜索索引以反映更改所需的时间。随着不断增强系统功能并引入新的用例，即时索引新文档的能力变得更加重要。Manas已经支持增量索引，能够提供数十分钟左右的索引延迟。不幸的是，这无法满足不断增长的广告和关注源的业务需求。决定在 Manas 中构建一个新模块，以进一步将索引延迟减少到几分之一秒。&lt;/p&gt;
&lt;p&gt;在这篇博文中，描述了系统的架构及其主要挑战，并提供了有关所做的权衡的详细信息。&lt;/p&gt;
&lt;h2 id=&#34;挑战-1&#34;&gt;挑战&lt;/h2&gt;
&lt;p&gt;新要求伴随着新挑战。以下是面临的几个主要挑战。&lt;/p&gt;
&lt;h3 id=&#34;索引延迟&#34;&gt;索引延迟&lt;/h3&gt;
&lt;p&gt;小批量方法，又称近实时方法，是&lt;a href=&#34;https://lucene.apache.org/&#34;&gt;Lucene&lt;/a&gt;、&lt;a href=&#34;https://vespa.ai/&#34;&gt;Vespa&lt;/a&gt;等开源项目最流行的选择。通过这种方法，新编写的文档在调用索引提交之前不可搜索。因此，您需要在索引延迟和吞吐量之间进行权衡。不幸的是，无法利用这种方法将索引延迟减少到秒级。&lt;/p&gt;
&lt;h3 id=&#34;索引刷新能力&#34;&gt;索引刷新能力&lt;/h3&gt;
&lt;p&gt;实时服务的缺点之一是缺乏索引刷新敏捷性。对于批处理管道，重新运行索引作业以立即获取所有架构更改非常简单。然而，当涉及到实时服务管道时，高效的索引刷新支持变得复杂。&lt;/p&gt;
&lt;h3 id=&#34;针对不断变化的数据进行扩展&#34;&gt;针对不断变化的数据进行扩展&lt;/h3&gt;
&lt;p&gt;为了避免过度配置，采用自动缩放来根据实际查询负载调整副本。如果索引是不可变的，则创建新副本相对容易：您只需将索引复制到新节点即可。所有的困难都在于处理不断变化的索引：如何确保所有副本最终都有相同的索引？&lt;/p&gt;
&lt;h3 id=&#34;错误恢复&#34;&gt;错误恢复&lt;/h3&gt;
&lt;p&gt;Manas 是一项数据密集型服务，其中每个主机可以提供高达数百 GB 的索引。Manas也是一个有状态的系统。错误的二进制文件可能会引入回滚无法修复的数据问题。需要构建一个支持容错和错误恢复的系统，以便可以从二进制错误和数据损坏中恢复。&lt;/p&gt;
&lt;h3 id=&#34;从静态转向实时&#34;&gt;从静态转向实时&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;简单看一下传统静态服务和实时服务的区别。如上图所示，实时服务的主要工作是将索引管道从离线转移到在线。&lt;/p&gt;
&lt;p&gt;对于静态服务，索引是通过批处理工作流程离线生成的，然后将它们复制到叶子以进行在线服务。对于批处理工作流程，由于框架开销较高，几乎不可能在几分之一秒内构建可服务的索引。对于实时服务，所有写入都在服务内动态处理，而不是使用离线工作流程。此外，实时索引管道以生成与静态索引管道相同的索引格式的方式处理写入，从而允许重用整个索引读取逻辑。考虑到这一点，让继续了解实时服务的工作原理。&lt;/p&gt;
&lt;h3 id=&#34;索引接口&#34;&gt;索引接口&lt;/h3&gt;
&lt;p&gt;没有直接使用&lt;a href=&#34;https://en.wikipedia.org/wiki/Remote_procedure_call&#34;&gt;RPC&lt;/a&gt;，而是使用&lt;a href=&#34;https://kafka.apache.org/&#34;&gt;&lt;strong&gt;Kafka作为的高写入吞吐量流。&lt;/strong&gt;&lt;/a&gt;叶子服务器不断拉动突变来构建增量索引。事实证明，这个决定在多个方面极大地简化了的系统：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据复制和写入失败由 Kafka 负责。&lt;/li&gt;
&lt;li&gt;有了回溯能力，Kafka队列也充当了&lt;a href=&#34;https://en.wikipedia.org/wiki/Write-ahead_logging&#34;&gt;WAL&lt;/a&gt;角色。&lt;/li&gt;
&lt;li&gt;每个分区都有严格的顺序保证，系统可以盲目应用删除，而无需担心正确性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;架构概述&#34;&gt;架构概述&lt;/h2&gt;
&lt;p&gt;由于服务逻辑可以通过共享索引格式重用，因此将重点关注索引数据流。&lt;/p&gt;
&lt;p&gt;本质上，实时Manas leaf是一个&lt;a href=&#34;https://en.wikipedia.org/wiki/Log-structured_merge-tree&#34;&gt;LSM&lt;/a&gt;引擎，它将随机IO写入转换为顺序IO，并为读放大和写放大应用程序提供高效服务。如下所示，整个索引过程由三个关键步骤组成。来一一讨论。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/6.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;实时分段构建realtime-segment-build&#34;&gt;实时分段构建(Realtime Segment Build)&lt;/h3&gt;
&lt;p&gt;除了现有的静态段之外，还引入了实时段。如上图所示，系统中的实时段有两种类型：活动实时段和密封实时段。（&lt;strong&gt;注&lt;/strong&gt;: 这个类似leveldb/rocksdb LSMtree, 同样是append-only write顺序IO , 不同的是kafka充当了WAL, 内部数据结构变成了正排和倒排索引结构，分段存储）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;活动实时段是唯一的可变组件，用于累积从 Kafka 拉取的突变（添加/删除）。值得指出的是，将文档添加到实时段后，在文档级提交后立即可以搜索它。&lt;/li&gt;
&lt;li&gt;一旦活动实时段达到可配置的阈值，它就会被密封(&lt;strong&gt;sealed&lt;/strong&gt;)，变得不可变，并被放入刷新队列中。同时，创建一个新的活动实时片段以继续累积突变。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在服务重启的情况下，可以通过重放来自 Kafka 的消息来重建实时片段。&lt;/p&gt;
&lt;h3 id=&#34;索引落盘index-flush&#34;&gt;索引落盘(index flush)&lt;/h3&gt;
&lt;p&gt;索引落盘是将内存中数据从实时段持久保存到紧凑索引文件中的过程。当实时段被密封时，flush落盘会自动触发，也可以使用调试命令手动触发flush落盘。&lt;/p&gt;
&lt;p&gt;索引落盘是一个有益的操作，它可以保证数据持久性，这样就不需要在重启期间从头开始重建内存中的段。此外，flush落盘还可以减少段的内存占用，并通过紧凑的不可变索引提高服务效率。&lt;/p&gt;
&lt;h3 id=&#34;索引压缩index-compaction&#34;&gt;索引压缩(Index Compaction)&lt;/h3&gt;
&lt;p&gt;随着时间的推移，多个生成的小段会损害服务性能。为了克服这个问题，引入了后台压缩线程来将小段合并为更大的段。由于删除操作只是将文档标记为已删除，而不是物理删除它们，因此压缩线程还会保留这些已删除/过期的文档。&lt;/p&gt;
&lt;p&gt;在每个刷新和压缩操作之后，将生成一个由所有静态段组成的新索引清单。用作检查点的 Kafka 偏移量也会添加到每个清单中。根据检查点，服务知道重启后在哪里消费消息。&lt;/p&gt;
&lt;h2 id=&#34;详细设计&#34;&gt;详细设计&lt;/h2&gt;
&lt;p&gt;在本节中，将更详细地介绍几个关键领域。让从最有趣的部分开始，即并发模型。&lt;/p&gt;
&lt;h3 id=&#34;并发模型&#34;&gt;并发模型&lt;/h3&gt;
&lt;p&gt;如上所述，实时段是需要同时处理读取和写入的唯一可变组件。不幸的是，开源项目采用的近实时方法无法满足的业务需求。相反，选择了一种不同的方法，使能够在添加到索引后立即提交文档，而无需等待索引刷新。出于性能考虑，针对适合用途的数据结构采用了&lt;a href=&#34;https://en.wikipedia.org/wiki/Non-blocking_algorithm&#34;&gt;无锁技术。&lt;/a&gt;现在来开箱吧！&lt;/p&gt;
&lt;h4 id=&#34;实时片段realtime-segment&#34;&gt;&lt;strong&gt;实时片段(Realtime Segment)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;每个实时段由一个倒排索引和一个正向索引组成。倒排索引在逻辑上是term到posting list（用于检索的文档 ID 列表）的映射。同时，前向索引存储用于完整评分和数据获取的任意二进制 blob。只关注实时倒排索引部分，与正向索引相比，实时倒排索引更有趣且更具挑战性。&lt;/p&gt;
&lt;p&gt;在较高的层面上，实时段和静态段之间的主要区别是可变性。对于实时倒排索引，从term到倒排列表的映射需要是并发的。这得到了像 &lt;a href=&#34;https://github.com/facebook/folly&#34;&gt;&lt;strong&gt;folly&lt;/strong&gt;&lt;/a&gt; 的并发hashmap这样的开源的良好支持。更关心的是posting list的内部表示，它可以有效地支持的并发模型。&lt;/p&gt;
&lt;h4 id=&#34;仅附加向量append-only-vector&#34;&gt;&lt;strong&gt;仅附加向量(Append-only Vector)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;通常single-writer、multiple-readers模型更高效、更容易推理。选择了与&lt;a href=&#34;https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html&#34;&gt;HDFS&lt;/a&gt;类似的数据模型(注:CF)，具有仅附加无锁数据结构。了解reader和writer如何交互如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/7.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Writer将文档 ID 附加到向量中，然后提交大小以使其可供读者访问&lt;/li&gt;
&lt;li&gt;Reader在访问数据之前,获取快照(snapshot)直至提交大小&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/8.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;为了避免随着posting list的增长而产生内存复制开销，在内部将数据作为存储桶列表进行管理。当容量用完时，只需要添加一个新的存储桶，而无需触及旧的存储桶。另外，通常搜索引擎使用跳跃列表来加速跳跃操作。由于这种格式，可以很方便地支持单级跳表，这对于实时倒排索引来说已经足够了，因为它的大小通常很小。&lt;/p&gt;
&lt;h4 id=&#34;文档原子性document-atomicity&#34;&gt;&lt;strong&gt;文档原子性(Document Atomicity)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;现在，通过仅附加向量，能够实现单个posting list的原子性。但是，文档可以包含term列表，最终可能会返回带有部分更新索引的意外文档。为了解决这个潜在问题，引入了文档级提交来保证文档原子性。在服务管道中，使用附加过滤器来确保仅返回已提交的文档。&lt;/p&gt;
&lt;p&gt;说到文档原子性，文档更新是这里值得一提的另一个场景。对于每个文档更新，特意将其转换为两个操作：添加新文档，然后从索引中删除旧文档。虽然每个操作符都是原子性的，但在一起不能保证原子性。考虑到在很短的时间窗口内返回旧版本或新版本都可以，但尽管如此，还是在服务管道中添加了重复数据删除逻辑，以便在两者都返回时过滤掉旧版本。&lt;/p&gt;
&lt;h4 id=&#34;写入缩放writes-scaling&#34;&gt;&lt;strong&gt;写入缩放(Writes Scaling)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;自然而然出现的一个问题是，如果你的数据结构只支持单写多读并发模型，那么如果单个线程无法及时处理所有写操作怎么办？仅仅为了扩展写入吞吐量而盲目添加更多分片似乎不是一个好主意。这是一个合理的担忧，在设计中已经考虑到了这一点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/9.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;用于数据结构的单写多读并发模型并不意味着不能使用多线程进行写操作。使用了term分片策略来支持多线程写入。如上图所示，对于给定的包含term列表的文档，每个term将始终映射到固定线程，以便所有为单写和多读定制的数据结构都可以直接重用，没有任何限制。(&lt;strong&gt;注&lt;/strong&gt;：加快索引构建，map/reduce分而治之思考方式(分而治之算法的工作原理是将问题递归地分解为两个或多个相同或相关类型的子问题，直到这些问题变得简单到可以直接解决。然后将子问题的解决方案组合起来以给出原始问题的解决方案)，no block彼此独立可并发执行,可以结合硬件与操作系统异步io操作加速写入)&lt;/p&gt;
&lt;h3 id=&#34;索引刷新index-refresh&#34;&gt;索引刷新(Index Refresh)&lt;/h3&gt;
&lt;p&gt;索引刷新能力是产品的一项关键功能，可以实现快速周转并提高开发速度。一般来说，可以使用两种方法来有效地刷新索引，分别是动态回填和从离线构建的索引恢复。&lt;/p&gt;
&lt;h4 id=&#34;回填索引backfilling-index&#34;&gt;&lt;strong&gt;回填索引(Backfilling Index)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;能够以合理的吞吐量回填文档。为了避免影响生产新鲜度(freshness)，需要一个单独的流来处理优先级较低的回填流量。因此，两个流中可能存在文档的两个版本，并且旧版本会覆盖新版本。为了克服这个问题，需要在实时索引管道中引入版本控制机制和冲突解决程序来决定哪个更新鲜。&lt;/p&gt;
&lt;h4 id=&#34;从离线构建索引恢复reinstating-from-offline-built-index&#34;&gt;&lt;strong&gt;从离线构建索引恢复(Reinstating from Offline Built Index)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;有时，以给定速度回填完整数据集会非常耗时。支持的另一种更快的索引刷新方法是离线构建索引，然后通过离线构建的索引和 Kafka 流之间的同步机制从中恢复。()&lt;/p&gt;
&lt;h3 id=&#34;故障转移和自动缩放failover-and-auto-scaling&#34;&gt;故障转移和自动缩放(Failover and Auto-scaling)&lt;/h3&gt;
&lt;p&gt;有时，需要出于各种原因启动新实例，例如故障转移和自动扩展等。对于静态服务，可以很容易地使用从索引存储下载的不可变索引来启动新实例(&lt;strong&gt;注&lt;/strong&gt;：这个类似rocksdb &lt;a href=&#34;https://github.com/facebook/rocksdb/wiki/Creating-and-Ingesting-SST-files#ingesting-sst-files&#34;&gt;bulkloading&lt;/a&gt; SSTable文件)。然而，对于索引不断变化的实时服务来说，它变得很复杂。如何确保新实例最终具有与其他实例相同的索引副本？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/10.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;决定使用基于领导者的复制，如上图所示。的流程如下所示：(&lt;strong&gt;注&lt;/strong&gt;：这个和分布式存储系统中多副本replica同步操作一样，大多是通过一致性协议来保证，比如&lt;a href=&#34;https://en.wikipedia.org/wiki/Raft_(algorithm)&#34;&gt;raft&lt;/a&gt;/&lt;a href=&#34;https://en.wikipedia.org/wiki/Paxos_(computer_science)&#34;&gt;Paxos&lt;/a&gt; 协议，如果follower落后，从快照中恢复数据，不同的是这里快照是放在支持S3协议云存储服务，使用kafka充当WAL从最新checkpoint开始恢复日志消息，同步完成，提供流量访问；这个看着流程容易，实现起来细节还是挺多的)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;领导者定期dump新快照(snapshots)并将其上传到持久索引存储&lt;/li&gt;
&lt;li&gt;新实例默认从索引存储下载最新快照&lt;/li&gt;
&lt;li&gt;新实例根据快照索引中的检查点恢复消费来自 Kafka 的消息&lt;/li&gt;
&lt;li&gt;新实例赶上后就开始提供流量&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;设计中有一些要点值得指出：&lt;/p&gt;
&lt;h4 id=&#34;领导人选举&#34;&gt;&lt;strong&gt;领导人选举&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Leader 的唯一职责是定期dump快照并上传索引。这意味着可以承受在短时间内（最多几个小时）没有领导者或有多个领导者的情况。因此，在选择领导者选举算法时具有一定的灵活性。为简单起见，选择使用集群维护作业来静态选择领导者，并定期检查是否有好的领导者。(&lt;strong&gt;注&lt;/strong&gt;：因为这里场景是单写多读模式，只写leader,当leader failover时，业务可以接受一段时间检索不到最新的数据，这个不影响业务使用，但是没有了最新的召回数据)&lt;/p&gt;
&lt;h4 id=&#34;快照上传&#34;&gt;&lt;strong&gt;快照上传&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;通常，新实例只是连接到领导者以下载最新的快照。在这种方法中，从新实例下载快照可能会使领导者过载，从而导致级联故障。相反，选择定期将快照上传到索引存储、交易空间和新鲜度以确保稳定性。此外，上传的快照对于错误恢复很有用，稍后将对此进行介绍。(&lt;strong&gt;注&lt;/strong&gt;： 上传快照，主要是kafka中的数据有错误数据时，用于快速恢复(从历史快照中恢复正确的历史数据，然后从正确数据的checkpoint点开始从kafka中消费日志数据进行恢复，&lt;!-- raw HTML omitted --&gt;跳过损坏的消息，使用修复好的新消息，fix操作&lt;!-- raw HTML omitted --&gt;)，aws s3成本是很低的，常存放大量一段时间的冷日志数据)&lt;/p&gt;
&lt;h3 id=&#34;错误恢复-1&#34;&gt;错误恢复&lt;/h3&gt;
&lt;p&gt;如上所述，错误恢复是实时服务系统的另一个挑战。需要处理一些涉及数据损坏的特定场景。&lt;/p&gt;
&lt;h4 id=&#34;输入数据损坏&#34;&gt;&lt;strong&gt;输入数据损坏&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;使用 Kafka 作为输入写入流；不幸的是，这些消息是不可变的，因为生产者只能将消息附加到其中，但不能更改现有消息的内容。这意味着一旦数据损坏被引入 Kafka 消息中，它就是永久性的。借助上传的快照，能够将索引倒回到没有损坏的位置，跳过损坏的消息，然后使用修复后的新消息。&lt;/p&gt;
&lt;h4 id=&#34;二进制错误导致数据损坏&#34;&gt;&lt;strong&gt;二进制错误导致数据损坏&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;尽管有一个成熟的静态集群索引验证管道，可以保证在换入新版本之前新索引和新二进制文件不会出现问题，但仍然可能会出现一些错误潜入生产环境。幸运的是，可以通过回滚二进制文件或索引来解决该问题。对于实时服务来说，回滚二进制文件无法回滚索引中的错误变得更加困难。使用的快照上传机制，能够回滚二进制文件以及回滚索引，然后重播来自 Kafka 的消息以修复索引中的错误。&lt;/p&gt;
&lt;h2 id=&#34;下一步是什么&#34;&gt;下一步是什么&lt;/h2&gt;
&lt;p&gt;随着Manas接入的场景越来越多，需要不断提升系统的效率、扩展性和能力。的路线图中一些有趣的项目如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;共同托管静态和实时集群以简化的服务堆栈&lt;/li&gt;
&lt;li&gt;优化系统以支持大数据集&lt;/li&gt;
&lt;li&gt;构建基于通用embedding的检索来支持高级场景&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;章节三-manas-hnsw-realtime为基于embedding的实时检索提供支持&#34;&gt;章节三 Manas HNSW Realtime：为基于embedding的实时检索提供支持&lt;/h1&gt;
&lt;p&gt;在之前的&lt;a href=&#34;https://medium.com/pinterest-engineering/manas-a-high-performing-customized-search-system-cf189f6ca40f&#34;&gt;博文&lt;/a&gt;中，介绍了的内部搜索引擎 - Manas - 并分享了如何大规模提供基于term的搜索。自推出以来，Manas 已发展成为 Pinterest 的关键候选生成器之一，服务于许多超出其最初目的的用例。&lt;/p&gt;
&lt;p&gt;特别是，基于embedding的检索是 Pinterest 发现和推荐引擎的关键组成部分。Manas 传统上通过倒排索引上的局部敏感哈希 (LSH) 支持近似最近邻 (ANN) 搜索，倒排索引是基于term的搜索引擎的自然扩展。在发布新的最先进技术（例如分层可导航小世界图 (HNSW)&lt;a href=&#34;https://arxiv.org/abs/1603.09320&#34;&gt;论文&lt;/a&gt;，&lt;a href=&#34;https://github.com/nmslib/hnswlib&#34;&gt;开源库&lt;/a&gt;实现了metric space：L2(Euclidean Squared L2),IP(Inner product),Cosine(Cosine similarity)，一般用cosine，归一化处理；其中facebook &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss&lt;/a&gt;加入&lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki/Additive-quantizers&#34;&gt;量化处理(Additive-quantizers)&lt;/a&gt;)后，在 Manas 中构建了一个灵活的基于embedding的检索框架，这使能够轻松采用新的 ANN 技术。使用新框架将 HNSW 启动到批量索引集群（索引延迟从几分钟到几天不等），与 LSH 相比，可以节省大量服务成本并减少延迟。&lt;/p&gt;
&lt;p&gt;计划中的下一个里程碑是将 HNSW 启动到的实时流集群（秒级索引延迟）。实时、大规模地为 HNSW 提供服务并不是一项简单的任务，部分原因是正在开辟新的领域，而无法依赖任何开源实现。&lt;/p&gt;
&lt;p&gt;在这篇博客中，将分享为 HNSW 提供实时服务的历程——解决这个问题的方法、面临的挑战以及为生产系统所做的一些优化。&lt;/p&gt;
&lt;h2 id=&#34;manas-realtime&#34;&gt;Manas Realtime&lt;/h2&gt;
&lt;p&gt;该项目的本质是为 HNSW 构建实时组件并将其集成到&lt;a href=&#34;https://medium.com/pinterest-engineering/manas-realtime-enabling-changes-to-be-searchable-in-a-blink-of-an-eye-36acc3506843&#34;&gt;Manas Realtime&lt;/a&gt;中。为了更好地了解这些组件如何适应更大的情况，让简要了解一下 Manas Realtime 的高级架构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/11.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;Manas Realtime本质上是一个&lt;a href=&#34;https://en.wikipedia.org/wiki/Log-structured_merge-tree&#34;&gt;LSM&lt;/a&gt;引擎，它将随机IO写入转换为顺序IO写入。写入不是公开写入端点，而是从 Kafka 摄取，这使能够简化系统并依赖 Kafka 作为 WAL。写入分为三种类型，以下是它们的处理方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;新文档被写入内存中的实时段，最终被密封并刷新到磁盘上的静态段&lt;/li&gt;
&lt;li&gt;使用内存中标记应用删除，并在服务期间过滤掉&lt;/li&gt;
&lt;li&gt;更新是通过删除旧文档并添加新文档来完成的&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;后台压缩过程有时会组合各种静态段，以减少因段过多而产生的服务开销。还依靠压缩过程通过从索引中删除文档来执行实际删除。&lt;/p&gt;
&lt;p&gt;从服务的角度来看，Manas Realtime 与 Manas Static 没有太大区别。对索引进行了抽象，以便存储层对整个检索过程是透明的。因此，随着 HNSW 已经为 Manas Static 启动，大多数服务组件已经存在。工作主要是与Manas Realtime 的LSM 索引组件集成。需要构建和优化两个核心组件，将在下面的部分中详细介绍：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;实时 HNSW 图表&lt;/li&gt;
&lt;li&gt;HNSW 图压缩&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;实时-hnsw-图表realtime-hnsw-graph&#34;&gt;实时 HNSW 图表(Realtime HNSW Graph)&lt;/h3&gt;
&lt;p&gt;实时段是系统中唯一可变的组件，因此该区域的优化对于确保良好的并发读写性能至关重要。&lt;/p&gt;
&lt;p&gt;HNSW 索引本质上是一个多层稀疏图。选择一个邻接列表来表示图，其中键是节点 id，值是邻居 id 列表。从基于锁的版本开始，每个节点都拥有一个锁，在更新邻居列表之前，该锁将由reader和writer持有。它很容易实现和推理。然而，由于锁争用，系统 CPU 使用率较高，别无选择，只能使用&lt;a href=&#34;https://en.wikipedia.org/wiki/Non-blocking_algorithm&#34;&gt;无锁&lt;/a&gt;技术。&lt;/p&gt;
&lt;h3 id=&#34;无锁实现lock-free-implementation&#34;&gt;无锁实现(Lock-free Implementation)&lt;/h3&gt;
&lt;p&gt;让来剖析一下如何以直观的方式处理写入。HNSW的思想源于著名的&lt;a href=&#34;https://en.wikipedia.org/wiki/Skip_list&#34;&gt;跳表&lt;/a&gt;结构。因此，HNSW 的无锁实现也类似于无锁跳表。一般来说，为了向图中添加新节点，每一层都涉及两个步骤，如下图所示。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在层内查找新节点的邻居并将新节点连接到选定的邻居&lt;/li&gt;
&lt;li&gt;更新选定的邻居以连接到新节点。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/12.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;同样，在 HNSW 图中从基础层开始向上层添加新节点，以避免出现新节点被选为上层的进入点但下层实际上没有为其建立连接的情况，从而导致没有结果问题。&lt;/p&gt;
&lt;p&gt;对于删除，避免了将它们应用到图表中的成本和复杂性。相反，使用内存中的删除标记在图外处理它们，依靠过滤器在服务期间过滤掉已删除的节点。&lt;/p&gt;
&lt;p&gt;一些细节优化值得简单提及：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;单写多读&lt;/strong&gt;：为了简单起见，延续了使用单写多读并发模式的传统，从而使代码整洁且易于推理。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预分配图&lt;/strong&gt;：由于实时图通常较小且大小固定，因此为图预分配内存以避免调整大小带来的复杂性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定制邻居选择算法&lt;/strong&gt;：使用标准邻居选择算法，更新邻居列表有三种可能：添加一个新邻居、减少邻居和替换一个邻居。当涉及到无锁实现时，通过回填最近邻居来消除“减少邻居”场景实际上大大简化了逻辑，能够使用原子操作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“原子”变量&lt;/strong&gt;：即使使用释放-获取顺序，c++ std::atomic 变量实际上也是昂贵的。相反，使用对齐内存来保证原子性，并使用全局原子变量作为内存屏障，使能够仅一次显式提交一个节点的所有更改。&lt;!-- raw HTML omitted --&gt;一些部分更新仍然有可能泄漏到读取线程可见，从而在短时间内损害全局连接。由于观察没有明显的召回率下降，将其视为性能和质量之间的合理权衡&lt;!-- raw HTML omitted --&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hnsw-图压缩hnsw-graph-compaction&#34;&gt;HNSW 图压缩(HNSW Graph Compaction)&lt;/h2&gt;
&lt;p&gt;压缩需要解决的主要问题是压缩速度。如前所述，压缩是减少同时服务的段总数的方法。最好的情况是，较长的压缩时间会导致较高的 CPU 使用率；最坏的情况是，系统停止摄取，导致新的更新无法反映和提供。&lt;/p&gt;
&lt;h3 id=&#34;清白合并clean-slate-merger&#34;&gt;清白合并(Clean Slate Merger)&lt;/h3&gt;
&lt;p&gt;对 hnsw 压缩算法的第一次尝试就是所说的 clean slate；本质上，该算法根据所有输入段的未删除embedding构建一个全新的图。这种方法对于的一些用例来说太慢了，所以需要优化算法。&lt;/p&gt;
&lt;h3 id=&#34;添加合并add-on-merger&#34;&gt;添加合并(Add on Merger)&lt;/h3&gt;
&lt;p&gt;下一个策略是尽可能多地重用索引；从所有要压缩的段中选择最大的段，并将索引转换为可以重用的内存结构。然后将其他段的剩余embedding添加到重用图中。&lt;/p&gt;
&lt;p&gt;剩下的问题是如何处理从重用段中删除的embedding。尝试了两种不同的方法：1）持久删除并重新选择邻居，2）将已删除的embedding与附近的活动embedding分组。尽管这两个选项都适合客户，但事实证明第一个选项在某些情况下速度太慢。&lt;/p&gt;
&lt;h3 id=&#34;持久删除persisting-deletions&#34;&gt;持久删除(Persisting Deletions)&lt;/h3&gt;
&lt;p&gt;需要维护图的小世界属性，并且简单地删除已删除的节点及其输入/输出边缘可能会破坏图中的连接性。为了解决这个问题，使用称为邻居重选的过程，其中节点可能连接到已删除节点的邻居以保持连接。&lt;/p&gt;
&lt;p&gt;发现，如果存在大量删除节点，压缩时间实际上会比 clean slate 算法慢，这并不理想。&lt;/p&gt;
&lt;h3 id=&#34;将已删除的节点与其最近的活动节点分组grouping-deleted-nodes-with-their-closest-alive-nodes&#34;&gt;将已删除的节点与其最近的活动节点分组(Grouping Deleted Nodes with their Closest Alive Nodes)&lt;/h3&gt;
&lt;p&gt;持久删除可能比使用全新算法慢的原因有两个。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;正在重用段中回填节点与其邻居之间的距离，从而导致大量昂贵的距离计算。&lt;/li&gt;
&lt;li&gt;邻居重选过程可能非常昂贵，尤其是在删除许多节点的情况下。这是因为如果删除节点的邻居也被删除，则需要更多的重选迭代。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/13.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;第二个优化是将已删除的节点与附近的活动节点分组，从而避免昂贵的重选过程。原始图与以前相同，但现在多个节点映射到相同的embedding。由于图形未更改，因此保持连接性。此外，延迟计算节点与其邻居之间的距离，而不是主动回填它们，从而避免了不必要的距离计算。还需要在算法中添加重复数据删除步骤，因为多个节点可以对应相同的embedding。&lt;/p&gt;
&lt;h2 id=&#34;在线召回监控online-recall-monitoring&#34;&gt;在线召回监控(Online Recall Monitoring)&lt;/h2&gt;
&lt;p&gt;到目前为止，一直专注于如何构建和优化系统中的组件。但生产系统还有另一个非常重要的方面——质量验证。对于 HNSW，召回率是用来验证索引质量的指标。它是通过将近似最近邻 (ANN) 搜索的结果与精确最近邻 (KNN) 搜索返回的理想结果进行比较来计算的。&lt;/p&gt;
&lt;p&gt;监控召回也特别重要，因为某些优化可能涉及为了更好的系统性能而进行的质量权衡。需要跟踪这些质量下降情况，以确保仍然为客户提供良好的结果。&lt;/p&gt;
&lt;p&gt;通过一组不可变的embedding，计算给定查询的召回率相对容易。可以使用离线批处理作业预先计算 KNN，并通过生成索引并向其发出查询来计算 ANN。由于embedding集是恒定的，KNN 结果永远不会改变，可以调整索引构建参数来优化召回率。&lt;/p&gt;
&lt;p&gt;然而，在实时场景中，embedding不断被添加和删除，使得预先计算的 KNN 集无法使用。为了解决这个问题，开发了一个在线召回工具；在服务集群中添加了计算 ANN 和 KNN 结果的功能，这使能够计算给定时间点的召回率。&lt;/p&gt;
&lt;h2 id=&#34;下一步是什么-1&#34;&gt;下一步是什么&lt;/h2&gt;
&lt;p&gt;对于来说，在批量索引集群上启动 HNSW 并通过为 HNSW 提供实时服务来突破的能力界限是一次激动人心的旅程。但 HNSW 只是基于embedding的检索系统愿景的第一步。&lt;/p&gt;
&lt;h3 id=&#34;效率和实验efficiency-and-experimentation&#34;&gt;效率和实验(Efficiency and Experimentation)&lt;/h3&gt;
&lt;p&gt;构建了一个系统，可以为基于embedding的检索进行生产化，从而使机器学习工程师能够尝试新的embedding或新算法，而无需从头开始构建新的生产系统。将继续迭代该系统，改进服务性能、渠道效率和促进轻松实验等方面。&lt;/p&gt;
&lt;h3 id=&#34;流式过滤streaming-filtering&#34;&gt;流式过滤(Streaming Filtering)&lt;/h3&gt;
&lt;p&gt;当前的过滤方法是从 HNSW 图中预取 K 个 ANN，然后应用过滤器来获得最终的候选集。这不是非常有效的漏斗，并且很难弄清楚 K 的值将给带来需要的最终候选者的数量。计划以流式方式实现 HNSW 算法，其中可以在获取期间应用过滤器，并且流式获取仅在检索到所需数量的候选者时才终止。&lt;/p&gt;
&lt;p&gt;敬请关注！&lt;/p&gt;
&lt;h1 id=&#34;章节四-manas-hnsw-流式过滤器&#34;&gt;章节四 Manas HNSW 流式过滤器&lt;/h1&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;基于embedding的检索是 Pinterest 推荐引擎的核心部分。支持无数的用例，从基于内容相似性的检索到学习检索。它由内部搜索引擎&lt;a href=&#34;https://medium.com/pinterest-engineering/manas-a-high-performing-customized-search-system-cf189f6ca40f&#34;&gt;Manas&lt;/a&gt;提供支持，该引擎提供近似最近邻 (ANN) 搜索服务，主要使用&lt;a href=&#34;https://arxiv.org/abs/1603.09320&#34;&gt;分层可导航小世界图 (HNSW)&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;传统的基于token的搜索根据具有 AND 和 OR 等逻辑连接，匹配term在对应term树结构中来检索文档，而 ANN 搜索则基于embedding相似性进行检索。通常希望进行将两者结合起来的混合搜索查询。例如，“找到与这双鞋相似、价格低于 100 美元、评级为 4 星或以上的产品，然后运送到英国。” 这是一个常见问题，并非完全没有解决，但每种解决方案都有各自的注意事项和权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：这个类似查询paser流程的优化，引入filter, 相当于算子，尽量利用索引。&lt;/p&gt;
&lt;h2 id=&#34;现有解决方案&#34;&gt;现有解决方案&lt;/h2&gt;
&lt;h3 id=&#34;后置过滤&#34;&gt;后置过滤&lt;/h3&gt;
&lt;p&gt;之前的方法是后过滤，本质上是首先执行 ANN 搜索，然后执行仅限于结果集基于token的搜索。后过滤会受到漏斗效率的影响，使用超取来解决这个问题。然而，这是不可扩展的，因为客户端需要不断调整其超取，并且每个请求都可以具有不同的过滤率。&lt;/p&gt;
&lt;h3 id=&#34;预过滤&#34;&gt;预过滤&lt;/h3&gt;
&lt;p&gt;另一种方法是预过滤。首先，在索引期间或首先评估token搜索查询来找出与基于token过滤器匹配的文档集。然后执行 ANN 搜索，同时过滤掉该集合中不存在的文档。然而，索引时间方法很难推广到任意树过滤器；预评估token搜索查询对于一组简单的过滤器或一小部分文档可以很好地工作，但有不属于任一类别的用例。即使没有人工神经网络的传统搜索，导致提前终止，通常也只能搜索大型语料库的一小部分。&lt;/p&gt;
&lt;h3 id=&#34;解决方案&#34;&gt;解决方案&lt;/h3&gt;
&lt;p&gt;每种方法都有其优点，根据具体情况，它们甚至可能是解决问题的最理想方法。做为 Pinterest 的无数用例提供服务的通用平台，每个用例都有不同的语料库大小、查询复杂性和过滤条件。因此，选择了一种在 HNSW 图遍历过程中以流方式应用过滤器的通用方法。不对用例做出任何假设，同时仍然提供一种在此框架上构建并根据需要应用优化的方法（例如，可以将预评估作为构建过滤器添加预处理步骤）。&lt;/p&gt;
&lt;h2 id=&#34;概述-1&#34;&gt;概述&lt;/h2&gt;
&lt;p&gt;之前：查询被表示为一棵树，在叶子处执行 HNSW 预取，将混合查询减少为传统的搜索查询。 之后：HNSW 从叶子中提取到迭代器中，该迭代器可以流式传输近似按距离排序的结果。 树的其余部分用作这些结果的过滤器。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/14.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图总结了系统在流变化之前和之后如何处理 ANN 查询。有几个值得注意的点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HNSW从查询解析阶段的批量预取变为查询执行阶段的流式取。&lt;/li&gt;
&lt;li&gt;查询执行从按 doc_id 顺序检索文档更改为按近似距离顺序检索文档。这是一个需要解决的问题，因为作为搜索引擎，索引格式针对 doc_id 顺序进行了优化。&lt;/li&gt;
&lt;li&gt;查询结构保持不变，提供向后兼容性和无缝迁移。&lt;/li&gt;
&lt;li&gt;轻量级评分已与迭代器树中的执行解耦。这对于 HNSW 流并不重要，但它符合将评分从基于树的线性组合方法中推广出来。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;还有一些影响设计的原则，指出这些原则可能会有所帮助：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;模块化&lt;/strong&gt;：ANN 检索、过滤和评分都应该相互解耦。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最小的更改&lt;/strong&gt;：通过尽可能地重用现有组件来快速构建和启动，并在以后根据需要进行优化。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;向后兼容性&lt;/strong&gt;：客户应该能够在对其请求进行最小程度的更改的情况下加入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;前向兼容性&lt;/strong&gt;：接口应该是通用的，并且每个组件（例如过滤器索引格式）应该易于升级。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;希望本节能够对系统组件以及为何以这种方式构建事物提供良好的高级概述。为了更深入地了解一切如何工作，需要打开两个黑匣子：1）流算法，以及 2）过滤器如何工作。&lt;/p&gt;
&lt;h2 id=&#34;流式算法&#34;&gt;流式算法&lt;/h2&gt;
&lt;p&gt;流式算法实际上在高层次上非常简单：获取一些候选者，应用过滤器，评分，将候选者添加到结果堆中，然后重复。下图从高层次上展示了这一点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/manas-a-high-performing-customized-search-system/15.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;获取候选 -&amp;gt; 应用过滤器 -&amp;gt; 评分 -&amp;gt; 添加到结果堆 重复此操作，直到达到停止条件。&lt;/p&gt;
&lt;p&gt;以下是在实施过程中考虑的一些事项：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最初，设计的流处理过程是一次检索一个候选者，但很快意识到往返获取/过滤/评分效率不高，因此转而使用小批量。然后需要决定使用什么小批量大小。HNSW 实际上存储了每个节点的邻居列表，因此使用邻居列表作为小批量。&lt;/li&gt;
&lt;li&gt;为了继续流，需要存储内部 HNSW 算法的一些状态。由于使用邻居列表作为小批量，因此只存储已经处理的候选者（访问列表）和仍需要处理的候选者（候选集）。&lt;/li&gt;
&lt;li&gt;最后，必须弄清楚何时停止流式搜索。这需要单独的一节，将在接下来讨论。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;停止条件&#34;&gt;停止条件&lt;/h3&gt;
&lt;h4 id=&#34;hnsw-停止条件hnsw-stopping-condition&#34;&gt;HNSW 停止条件(HNSW Stopping Condition)&lt;/h4&gt;
&lt;p&gt;退后一步，如果看一下最初的 HNSW 论文，当检索到足够的候选者时，算法不会终止；相反，当积累的候选者都比候选集中最接近的候选者更接近时，它就会终止。这背后的主要直觉是确保算法以高概率检索最佳（最接近）的候选者。在流式搜索中应用了相同的概念，主要区别在于仅对过滤后的候选者进行操作。&lt;/p&gt;
&lt;h4 id=&#34;时间预算time-budget&#34;&gt;时间预算(Time Budget)&lt;/h4&gt;
&lt;p&gt;在高过滤率场景中，最终可能会遍历整个图，但仍然找不到足够的候选者，从而导致极高的延迟。由于大多数客户都有延迟要求，因此使用时间预算来限制流式搜索所花费的时间。一旦达到预算，就会退回已经积累的候选人。&lt;/p&gt;
&lt;h3 id=&#34;过滤器filters&#34;&gt;过滤器(Filters)&lt;/h3&gt;
&lt;p&gt;设计过滤的方式很大程度上受到上面列出的一些原则的影响：模块化和前向兼容性。实现过滤最简单的方法就是直接在HNSW代码中添加代码。事实上，开源 HNSW 代码中的删除标记已经这样做了。然而，这破坏了模块化性，并且对于过滤器代码的可维护性和前向兼容性来说并不理想。这对应用场景来说尤其重要，因为为许多具有不同过滤器要求的客户提供服务。&lt;/p&gt;
&lt;p&gt;设计接口时不采用任何底层过滤器结构或存储格式。实现了对主要用例的支持，其中客户端可以在请求中指定任意过滤树，用合取和析取连接词表示。&lt;/p&gt;
&lt;p&gt;本着最小改变的精神，重新使用倒排索引作为过滤器存储。因此，本质上有一个由叶子处的postinglists支持的过滤树，其结构与在基于标记的搜索中使用的迭代器树非常相似。这方便重用，但效率低下，因为倒排索引针对 doc_id 有序迭代进行了优化，但 HNSW 流需要非有序逐点查找。通过使用位图和数组支持的倒排列表而不是跳表支持的倒排列表来解决这个问题，以内存效率换取计算效率。这确实带来了明显的可扩展性挑战：使用大量过滤器，根本无法承受内存成本，但这并不是短期内需要解决的主要问题。计划未来的工作是升级过滤器存储。&lt;/p&gt;
&lt;h2 id=&#34;优化&#34;&gt;优化&lt;/h2&gt;
&lt;h3 id=&#34;如果已经有足够的候选者则放弃远处的候选者&#34;&gt;如果已经有足够的候选者，则放弃远处的候选者&lt;/h3&gt;
&lt;p&gt;在一些客户端用例中，过滤器树非常复杂，导致过滤器阶段占用最多的延迟。一种优化是当结果堆已满时跳过距离比结果堆中的候选者更差的候选者，以避免过滤无论如何都不会选择的候选者。&lt;/p&gt;
&lt;h3 id=&#34;批处理初始化&#34;&gt;批处理初始化&lt;/h3&gt;
&lt;p&gt;不是从头开始流式传输，而是首先检索等于客户端想要的候选者数量的批量大小，因为最初需要至少检索那么多。&lt;/p&gt;
&lt;h3 id=&#34;重新排序过滤器树节点&#34;&gt;重新排序过滤器树节点&lt;/h3&gt;
&lt;p&gt;由于流式处理进行非排序的逐点查找，因此过滤器树节点的排序变得很重要，因为首先评估最严格的过滤器会更有效。&lt;/p&gt;
&lt;h2 id=&#34;未来的工作&#34;&gt;未来的工作&lt;/h2&gt;
&lt;h3 id=&#34;带子图的流式传输streaming-with-subgraphs&#34;&gt;带子图的流式传输(Streaming with Subgraphs)&lt;/h3&gt;
&lt;p&gt;上面需要注意的关键是，当前的流方法实际上并没有减少检索所需的候选数量，它只是自动为每个请求计算出适当的超取。每个过滤的候选者仍然是浪费的距离计算。&lt;/p&gt;
&lt;p&gt;目前正在尝试通过更大的过滤器（例如美国或非美国）将空间划分为单独的子图。这对于使用一些大型过滤器的用例来说效果很好。更具可扩展性的扩展可能是使用过滤器来标记图形，并允许遍历标签的析取或合取。&lt;/p&gt;
&lt;h3 id=&#34;高效过滤器存储efficient-filter-store&#34;&gt;高效过滤器存储(Efficient Filter Store)&lt;/h3&gt;
&lt;p&gt;使用倒排索引作为过滤器存储在某些场景下效果很好，但它确实针对传统搜索进行了优化，而不是针对图遍历的过滤进行了优化。可以从头开始设计一个针对基于图的过滤进行优化的过滤器存储，并将其与其他基于图的检索系统（如&lt;a href=&#34;https://medium.com/pinterest-engineering/an-update-on-pixie-pinterests-recommendation-system-6f273f737e1b&#34;&gt;Pixie&lt;/a&gt;)共享。&lt;/p&gt;
&lt;h3 id=&#34;量化quantization&#34;&gt;量化(Quantization)&lt;/h3&gt;
&lt;p&gt;极高的过滤率场景可以通过暴力破解来解决，但仍然存在一系列具有非常高的过滤率但使用暴力破解成本高昂的情况。这些情况的瓶颈是大量浪费的距离计算。通过量化可以大大降低这一成本。可以转向不同的算法，例如 PQ IVF，或者HNSW 引入 PQ（注：这部分参考faiss，需要提供训练接口）&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;实现了流式过滤，它抽象了如何执行过滤的实现细节，并减轻了客户端过度获取调整的负担。从系统的角度来看，有一个通用的过滤器解决方案，它足够灵活，可以支持所有的用例，并且可以支持未来的优化，例如预过滤和过滤器存储升级。通过消除不精确的超取调整，已经看到了巨大的成本节省和质量改进，并且了解到了未来优化的许多机会。&lt;/p&gt;
&lt;p&gt;敬请关注！&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;题外话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不能脱离应用场景去理解算法所要解决的实际问题；&lt;/p&gt;
&lt;p&gt;没有上下文引发的问题，何来解决方案；只谈结果，拿果子，忽略了上下文case, YY~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;reference&#34;&gt;reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Non-blocking_algorithm&#34;&gt;https://en.wikipedia.org/wiki/Non-blocking_algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fulmanski.pl/tutorials/computer-science/nosql/column-family-bigtable-stores/&#34;&gt;https://fulmanski.pl/tutorials/computer-science/nosql/column-family-bigtable-stores/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/pinterest-engineering/manas-a-high-performing-customized-search-system-cf189f6ca40f&#34;&gt;https://medium.com/pinterest-engineering/manas-a-high-performing-customized-search-system-cf189f6ca40f&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/pinterest-engineering/manas-realtime-enabling-changes-to-be-searchable-in-a-blink-of-an-eye-36acc3506843&#34;&gt;https://medium.com/pinterest-engineering/manas-realtime-enabling-changes-to-be-searchable-in-a-blink-of-an-eye-36acc3506843&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/pinterest-engineering/manas-hnsw-realtime-powering-realtime-embedding-based-retrieval-dc71dfd6afdd&#34;&gt;https://medium.com/pinterest-engineering/manas-hnsw-realtime-powering-realtime-embedding-based-retrieval-dc71dfd6afdd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/pinterest-engineering/manas-hnsw-streaming-filters-351adf9ac1c4&#34;&gt;https://medium.com/pinterest-engineering/manas-hnsw-streaming-filters-351adf9ac1c4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/rXXm6c8LrTqqP4iWf9mtxA&#34;&gt;为什么微信推荐这么快？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/1a2bl983bGKM713xI_3v_A&#34;&gt;小红书高时效推荐系统背后的技术升级&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/1ed9BDZKzjQIgDScyxpbHw&#34;&gt;快手搜索在向量检索方向的探索和实践&lt;/a&gt; &lt;a href=&#34;https://yongyuan.name/blog/vector-ann-search.html&#34;&gt;向量索引&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://damo.alibaba.com/events/112?lang=zh&#34;&gt;达摩院自研向量检索引擎 Proxima&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/baidu/puck/blob/main/docs/README.md&#34;&gt;baidu-puck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.research.google/2020/07/announcing-scann-efficient-vector.html&#34;&gt;Announcing ScaNN: Efficient Vector Similarity Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/series/faiss/&#34;&gt;Faiss: The Missing Manual&lt;/a&gt; &lt;a href=&#34;https://www.pinecone.io/learn/&#34;&gt;https://www.pinecone.io/learn/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vector Search and Databases at Scale&lt;/strong&gt;. Highload++ Conference, Serbia. &lt;a href=&#34;https://highload.rs/2023/abstracts/9770&#34;&gt;Event&lt;/a&gt;. &lt;a href=&#34;https://drive.google.com/file/d/11M51Jw9UdEmzHDTGZmn4n3bxgTcQt3sw/view?usp=sharing&#34;&gt;Slides&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=UMrhB3icP9w&amp;amp;t=65s&#34;&gt;YouTube&lt;/a&gt;.   &lt;a href=&#34;https://ashvardanian.com/talks/&#34;&gt;ash.vardanian&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;附相关向量数据库hnsw使用一般都会支持&#34;&gt;附：相关向量数据库HNSW使用(一般都会支持)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vearch/vearch/wiki/Hnsw%E5%AE%9E%E6%97%B6%E7%B4%A2%E5%BC%95%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1&#34;&gt;vearch-Hnsw Real time Index Detailed Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://weaviate.io/developers/weaviate/concepts/vector-index&#34;&gt;weaviate-Vector Indexing&lt;/a&gt;。文档不错，知其然知其所以然~
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://weaviate.io/blog/why-is-vector-search-so-fast&#34;&gt;Why is Vector Search so fast?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://weaviate.io/blog/ann-algorithms-vamana-vs-hnsw&#34;&gt;Vamana vs. HNSW - Exploring ANN algorithms Part 1&lt;/a&gt; (In-memory Index 和 DiskANN)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://weaviate.io/blog/ann-algorithms-hnsw-pq&#34;&gt;HNSW+PQ - Exploring ANN algorithms Part 2.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://weaviate.io/blog/ann-algorithms-tiles-enocoder&#34;&gt;The Tile Encoder - Exploring ANN algorithms Part 2.2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://milvus.io/docs/index.md&#34;&gt;milvus-Vector Index&lt;/a&gt; (In-memory Index 和 DiskANN)  包括新ANN算法支持跟进活跃，比如ScANN ；好的开源生态~
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://milvus.io/blog/deep-dive-1-milvus-architecture-overview.md&#34;&gt;&lt;strong&gt;milvus-deep-dive&lt;/strong&gt;&lt;/a&gt; (整体思路和manas有一定相似，比如基于日志)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qdrant.tech/documentation/concepts/indexing/#vector-index&#34;&gt;qdrant-vector index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/8.0/knn-search.html&#34;&gt;ES8.0/knn-search&lt;/a&gt; &lt;a href=&#34;https://opensearch.org/docs/latest/search-plugins/knn/index/&#34;&gt;OpenSearch-plugin-knn&lt;/a&gt; &lt;a href=&#34;https://github.com/opensearch-project/k-NN&#34;&gt;https://github.com/opensearch-project/k-NN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://redis.io/docs/interact/search-and-query/search/vectors/&#34;&gt;Redis Stack7.2/vss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alibabacloud.com/help/zh/tair/developer-reference/vector&#34;&gt;alibabacloud-tair-vector&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：如何避免事务期间读取不一致</title>
      <link>https://weedge.github.io/post/oneday/distributed-system-question-how-to-avoid-read-inconsistency-during-a-transaction/</link>
      <pubDate>Sat, 26 Aug 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/distributed-system-question-how-to-avoid-read-inconsistency-during-a-transaction/</guid>
      
        <description>&lt;p&gt;想象一下，当您尝试将 100 美元从账户 A 转账到账户 B，并且两个账户都在同一家银行时。启动传输后，您刷新屏幕。然而，当您刷新屏幕时，您的总余额就会下降——那 100 美元似乎凭空消失了。您看到帐户 A 少了 100 美元。然而，B账户并没有多出100美元。然后，您刷新屏幕几次，可以看到帐户 B 获得了 100 美元。&lt;/p&gt;
&lt;p&gt;您在事务期间遇到的这个问题称为读取偏差。当您在不幸运的时间（写入交易期间和之后）读取交易时，就会发生异常。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://edward-huang.com/images/distributed-system-question-how-to-avoid-read-inconsistency-during-a-transaction/Distributed%20System%20Question_%20How%20to%20Avoid%20Read%20Inconsistency%20during%20a%20Transaction-%20bank%20transfer.png&#34; alt=&#34;银行转账时序图&#34;&gt;&lt;/p&gt;
&lt;p&gt;这可能会带来不好的用户体验，但如果转账交易成功后刷新页面，这不会造成任何问题。&lt;/p&gt;
&lt;p&gt;然而，在进行数据库备份或分析查询时，读取偏差会成为一个问题。&lt;/p&gt;
&lt;p&gt;在数据库备份中，我们需要制作数据库的副本。备份过程中可能会有写请求进来，如果出现读倾斜不一致的情况，可能会导致备份结果不一致。部分数据为旧版本，部分数据为新版本。通过这样的操作，这种不一致的问题可能会永久存在。&lt;/p&gt;
&lt;p&gt;我们需要在分析查询中扫描大型数据库并定期检查数据损坏。读取偏差可能会导致搜索和检查不一致 - 通常可能会产生不一致的结果并引发有关数据损坏的错误警报。&lt;/p&gt;
&lt;h2 id=&#34;解决读取偏差&#34;&gt;解决读取偏差&lt;/h2&gt;
&lt;p&gt;读取倾斜的问题是读事务在旧数据库版本中读取一次，在新数据库版本中读取另一次。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://edward-huang.com/images/distributed-system-question-how-to-avoid-read-inconsistency-during-a-transaction/Distributed%20System%20Question_%20How%20to%20Avoid%20Read%20Inconsistency%20during%20a%20Transaction-reading%20skew.png&#34; alt=&#34;读取倾斜的图像&#34;&gt;&lt;/p&gt;
&lt;p&gt;这里重要的一点是读取事务需要一致 - 它不需要是最新版本。从事务开始到结束需要保持一致，所以我们需要保持数据版本相同。&lt;/p&gt;
&lt;p&gt;例如，如果 Bob 正在以数据版本 1 运行读事务，则在整个事务中，Bob 应该只能读取数据库数据版本 1。如果在事务处理过程中，发生新的写事务，这将导致更新数据库中的数据。Bob 将不会在他的交易中看到该新版本。&lt;/p&gt;
&lt;p&gt;因此，我们可以使事务从数据库的一致快照中读取——事务将从事务开始时其他事务在数据库中提交的所有数据中看到。&lt;/p&gt;
&lt;p&gt;此功能称为快照隔离，许多关系数据库都提供此功能，例如 PostgreSQL 和 MySQL。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://edward-huang.com/images/distributed-system-question-how-to-avoid-read-inconsistency-during-a-transaction/Distributed%20System%20Question_%20How%20to%20Avoid%20Read%20Inconsistency%20during%20a%20Transaction-snapshot%20isolation%20sequence%20diagram.png&#34; alt=&#34;快照隔离序列图的图像&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;实施快照隔离&#34;&gt;实施快照隔离&lt;/h2&gt;
&lt;p&gt;我们需要在数据库中保留各种快照版本来实现快照隔离。每次事务开始时，数据库都会将最新提交的快照版本赋予该事务。然后，数据库将跟踪每个事务及其相应的快照版本，以保持读取的一致性。&lt;/p&gt;
&lt;p&gt;每个事务都有一个&lt;code&gt;transactionId&lt;/code&gt;，并且&lt;code&gt;transactionId&lt;/code&gt;是从数据库中检索的。因此，&lt;code&gt;transactionId&lt;/code&gt;总是在增加。数据库跟踪每个&lt;code&gt;transactionId&lt;/code&gt;写入数据库的使用&lt;code&gt;createdAt&lt;/code&gt;和&lt;code&gt;deletedAt&lt;/code&gt;值。&lt;code&gt;transactionId&lt;/code&gt;提交事务后，数据库使用事务中的 对该操作创建了一个标记。数据库进一步制作新交易的快照，并用最新的 transactionId 标记该快照。当有新的事务从数据库读取时，数据库会检索该事务之前最新提交的事务，有以下几个规则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;即使提交了后续事务，也不会显示当前尚未提交到数据库的任何 transactionId。&lt;/li&gt;
&lt;li&gt;任何中止的交易也不会显示。&lt;/li&gt;
&lt;li&gt;数据库不会显示任何晚于&lt;code&gt;transactionId&lt;/code&gt;（大于）当前的事务&lt;code&gt;transactionId&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;数据库将向读取数据库的其他传入事务显示任何其他事务。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;让我们看看 Bob 的场景中会发生什么：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://edward-huang.com/images/distributed-system-question-how-to-avoid-read-inconsistency-during-a-transaction/Distributed%20System%20Question_%20How%20to%20Avoid%20Read%20Inconsistency%20during%20a%20Transaction-snapshot%20isolation%20on%20Bob%20banking%20scenario%20with%20the%20algorithm%20implementation.png&#34; alt=&#34;Bob 银行业务场景的快照隔离图像以及算法实现&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;当 Bob 发起转账交易时，它会启动一个后台进程，将 100 美元从账户 A 转账到账户 B。该交易将首先调用数据库或辅助服务来获取增量，然后发起交易 - 假设交易是&lt;code&gt;transactionId&lt;/code&gt;1234 。&lt;/li&gt;
&lt;li&gt;后续的读取事务将需要通过获取增量&lt;code&gt;transactionId&lt;/code&gt;并向数据库调用读取请求来执行相同的操作 - 假设是&lt;code&gt;transactionId&lt;/code&gt;1345。&lt;/li&gt;
&lt;li&gt;当传输尚未完成时，数据库不会向 Bob 显示&lt;code&gt;transactionId&lt;/code&gt;1234（规则 1）应用的数据。&lt;/li&gt;
&lt;li&gt;如果在 1345 之后启动另一个写入事务&lt;code&gt;transactionId&lt;/code&gt;，因为该事务具有更大的&lt;code&gt;transactionId&lt;/code&gt;，数据库将不会向&lt;code&gt;transactionId&lt;/code&gt;1345 显示该事务（规则号 3）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在删除过程中，数据库不会立即删除该字段中的值，而是会在该字段上标记一个&lt;a href=&#34;https://en.wikipedia.org/wiki/Tombstone_(data_store)#:~:text=A%20tombstone%20is%20a%20deleted,is%20considered%20to%20be%20successful.&#34;&gt;墓碑&lt;/a&gt; 逻辑删除。不立即删除该值的原因之一是那些早期的交易可能仍然使用该值。因此，一旦所有事务都使用提交给其事务的值，我们就可以利用垃圾收集来异步检查和删除该值。&lt;/p&gt;
&lt;h2 id=&#34;对分布式环境进行快照隔离&#34;&gt;对分布式环境进行快照隔离&lt;/h2&gt;
&lt;p&gt;到目前为止，我们已经探索了如何解决单节点环境中的读取倾斜——我们假设数据库不分布在多个集群上。&lt;/p&gt;
&lt;p&gt;如何在分布式环境中扩展快照隔离？&lt;/p&gt;
&lt;p&gt;&lt;code&gt;transactionId&lt;/code&gt;在分布式环境中很难得到一个全局的、不断增长的。出于一个原因，可能驻留在不同数据库中的每台计算机可能都有其 UUID 计数器，我们需要进行一些协调以确保因果关系。如果事务B从事务A读取值，我们要确保事务B的值大于&lt;code&gt;transactionId&lt;/code&gt;事务A。我们如何处理复制数据库中的一致快照？&lt;/p&gt;
&lt;p&gt;我们可以使用时钟或一天中的时间作为&lt;code&gt;transactionId&lt;/code&gt;写入数据库吗？当天时钟不可靠，因为 NTP 同步基于不可靠的网络。因此，有些机器可能会出现时钟偏差，在时间上任意向后移动。一个节点的时间也可能与另一节点的时间不同。然而，如果我们能让时钟足够准确，它就可以作为&lt;code&gt;transactionId&lt;/code&gt;——时钟的时间晚意味着事件产生的晚。我们如何确保时钟对于 transactionId 来说足够准确？&lt;/p&gt;
&lt;p&gt;当检索每台机器中的时间值时，我们希望它返回一个置信区间，&lt;code&gt;[Tbegin, Tlast]&lt;/code&gt;而不是获取单个值。置信区间表示时钟的标准偏差为正负范围&lt;code&gt;Begin&lt;/code&gt;和&lt;code&gt;Tlast&lt;/code&gt;。如果有两笔交易，&lt;code&gt;transactionX&lt;/code&gt;，&lt;code&gt;transactionY&lt;/code&gt;进来，&lt;code&gt;[TbeginX, TlastX]&lt;/code&gt;，&lt;code&gt;[TbeginY, TlastY]&lt;/code&gt;， 和&lt;code&gt;TlastX &amp;lt; TbeginY&lt;/code&gt;。我们可以确保&lt;code&gt;transactionX&lt;/code&gt;早于&lt;code&gt;tranasctionY&lt;/code&gt;。但是，如果值重叠，我们就无法确定顺序。&lt;a href=&#34;https://cloud.google.com/spanner/docs/true-time-external-consistency&#34;&gt;Google Spanner&lt;/a&gt;使用的是这种方法实现其快照隔离。Spanner 会故意等到超过前一个事务的置信区间而不重叠时才提交当前事务。因此，他们需要保持机器上每个时钟的置信时间间隔尽可能小，以避免延迟。Google 在每个数据中心部署原子钟或 GPS 服务器，以实现时钟同步。&lt;/p&gt;
&lt;p&gt;为了确保每个数据库副本上的快照都是最新的，我们可以使用&lt;a href=&#34;https://en.wikipedia.org/wiki/Quorum_(distributed_computing)#:~:text=A%20quorum%20is%20the%20minimum,operation%20in%20a%20distributed%20system.&#34;&gt;Quorum&lt;/a&gt;仲裁策略从其所有数据库集群中获取所有最新的事务快照。我们可以使用的另一个策略是确保事务始终路由到同一数据库实例以获得一致的快照结果。&lt;/p&gt;
&lt;h2 id=&#34;概括&#34;&gt;概括&lt;/h2&gt;
&lt;p&gt;当由于后台发生另一个写入事务而无法从读取数据库数据中看到一致的结果时，就会发生读取偏差。一致性快照是单节点数据库读取倾斜的解决方案。&lt;/p&gt;
&lt;p&gt;一致性快照是一种隔离级别，可保证每个事务都从数据库的一致性快照中读取数据，通常是当前启动的事务之前的最新快照。&lt;/p&gt;
&lt;p&gt;实现快照隔离需要一个单调递增的计数器transactionId来确定返回哪个版本给事务调用。然而，在处理分布式环境时，这可能很困难，因为需要协调才能产生因果关系。解决此问题的一种解决方案是使用时钟返回置信区间来创建不断增加的&lt;code&gt;transactionId&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;最后，为了确保每个事务获得一致的快照，我们可以使用仲裁策略始终返回大多数节点返回的当前事务的最新快照，或者在事务调用和数据库实例上具有会话关联性。&lt;/p&gt;
&lt;p&gt;如何确保分布式系统中的读取一致性？将如何解决创建全局的问题&lt;code&gt;transactionId&lt;/code&gt;？&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://edward-huang.com/distributed-system/2022/04/03/distributed-system-question-how-to-avoid-read-inconsistency-during-a-transaction/&#34;&gt;https://edward-huang.com/distributed-system/2022/04/03/distributed-system-question-how-to-avoid-read-inconsistency-during-a-transaction/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>译：Uber是如何解决数据一致性问题的？</title>
      <link>https://weedge.github.io/post/oneday/how-did-uber-solve-data-consistency-problem/</link>
      <pubDate>Wed, 16 Aug 2023 10:26:23 +0800</pubDate>
      
      <guid>https://weedge.github.io/post/oneday/how-did-uber-solve-data-consistency-problem/</guid>
      
        <description>&lt;h1 id=&#34;介绍&#34;&gt;介绍&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/how-did-uber-solv-data-consistency-problem/1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Uber 的请求流程相当复杂，从上图可以看出，他们使用 Spanner 来存储大量数据。Spanner 是一种完全托管的关键任务关系数据库服务，可提供全球范围内的事务一致性、自动同步复制以实现高可用性。&lt;/p&gt;
&lt;p&gt;但在此之前，最初的架构有本地数据库。更具体地说，他们使用 Cassandra 来存储实时数据。另外，在 Cassandra 之上，他们还使用了 Ringpop。有关更多详细信息，您可以查看&lt;a href=&#34;https://www.uber.com/blog/ringpop-open-source-nodejs-library/&#34;&gt;此博客文章&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;但当扩展到数百万个并发请求时，Cassandra 很难保证低延迟写入。另一个问题是工程团队开始注意到需要多行和多表写入的复杂存储交互（我不知道这意味着什么）。但无论如何，本地 Cassandra DB 开始变得非常具有挑战性。&lt;/p&gt;
&lt;p&gt;对于 Uber 来说，数据不一致可能会导致两名司机接送同一位顾客。&lt;/p&gt;
&lt;p&gt;解决方案是构建一个应用层框架，引入中间层(间), 使用&lt;strong&gt;Saga 模式&lt;/strong&gt;来编排数据库操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Saga设计模式&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/how-did-uber-solv-data-consistency-problem/2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Saga设计模式是一种在分布式事务场景中管理跨微服务的数据一致性的方法。saga是更新每个服务并发布消息或事件以触发下一个事务步骤的事务序列。如果某个步骤失败，saga 会执行补偿事务来抵消前面的事务。&lt;/p&gt;
&lt;p&gt;简而言之，如果事务期间出现问题（&lt;em&gt;事务&lt;/em&gt;是单个逻辑或工作单元，有时由多个操作组成），则应恢复之前的更改。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;什么是事务？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;事务是单个逻辑或工作单元，有时由多个操作组成。在事务中，&lt;em&gt;事件&lt;/em&gt;是实体发生的状态更改，&lt;em&gt;命令&lt;/em&gt;封装了执行操作或触发后续事件所需的所有信息。&lt;/p&gt;
&lt;p&gt;事务必须是&lt;em&gt;原子的、一致的、隔离的和持久的（ACID）&lt;/em&gt;。单个服务内的事务是ACID的，但是跨服务的数据一致性需要跨服务的事务管理策略。&lt;/p&gt;
&lt;p&gt;在多服务架构中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;原子性&lt;/em&gt; 是一组不可分割且不可简化的操作，要么全部发生，要么不发生。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;一致性&lt;/em&gt; 是指事务仅将数据从一种有效状态带到另一种有效状态。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;隔离性&lt;/em&gt; 保证并发事务产生与顺序执行的事务产生的数据状态相同的数据状态。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;持久性&lt;/em&gt; 确保即使在系统故障或断电的情况下，已提交的事务仍保持提交状态。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Saga 模式使用本地事务&lt;/em&gt;序列提供事务管理。本地事务是 saga 参与者执行的原子工作。每个本地事务都会更新数据库并发布消息或事件以触发Sagas中的下一个本地事务。如果本地事务失败，saga 会执行一系列&lt;em&gt;补偿事务&lt;/em&gt;，以撤消先前本地事务所做的更改。&lt;/p&gt;
&lt;p&gt;有两种常见的 saga 实现方法：choreography 和 orchestration。每种方法都有自己的一套挑战和技术来协调工作流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;choreography&lt;/strong&gt;是一种协调sagas的方法，参与者可以在没有集中控制点的情况下交换事件。通过编排，每个本地事务都会发布触发其他服务中的本地事务的域事件。rocketMQ中的消息事务采用的这种编排模式，满足RC隔离(&lt;em&gt;Read Committed&lt;/em&gt;)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/how-did-uber-solv-data-consistency-problem/3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;orchestration&lt;/strong&gt;是一种协调sagas的方法，其中集中控制器告诉saga参与者要执行哪些本地事务。saga 协调器处理所有事务并告诉参与者根据事件执行哪个操作。编排器执行 saga 请求，存储和解释每个任务的状态，并通过补偿事务处理故障恢复，本质是2PC。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/how-did-uber-solv-data-consistency-problem/4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;观看视频&lt;a href=&#34;https://www.youtube.com/watch?v=DY2AR8Wzg3Y&#34;&gt;How does Uber scale to millions of concurrent requests?&lt;/a&gt;了解有关 Uber 迁移及其挑战的更多信息。如果您想了解有关 Saga 模式的更多详细信息参考&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/saga/saga&#34;&gt;此链接&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;spanner：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/weedge/mypic/raw/master/oneday/how-did-uber-solv-data-consistency-problem/5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;
&lt;p&gt;主要是为了保证数据一致性， 不出现司机乘客对同一个事务操作异常，从原来的nosql 转向 newsql， 引入了google cloud spanner (new SQL) 提供全球范围内的事务一致性、自动同步复制以实现高可用性。结合uber这个场景，然后看下spanner的论文去寻求解决方案，了解下细节结合场景进行设计。视频中提到了大规模数据迁移的挑战，特别是全球跨数据中心的迁移同步，有相应的网络优化技术（这些技术解决方案刚出来的时候国内有跟进，当然技术文章介绍肯定少不了）&lt;/p&gt;
&lt;p&gt;题外话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;需求场景驱动技术方案去落地，技术方案不一定最优解， 能满足当前需求场景，过度设计优化产生无意义的消耗，不过可以留下接口可以去扩展，满足不同场景的进一步优化，软件中没有通用的解决方案银弹。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;reference&#34;&gt;reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@dmosyan/how-did-uber-solve-data-consistency-problem-dcdd39bd3ed6&#34;&gt;https://medium.com/@dmosyan/how-did-uber-solve-data-consistency-problem-dcdd39bd3ed6&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.uber.com/blog/ringpop-open-source-nodejs-library/&#34;&gt;https://www.uber.com/blog/ringpop-open-source-nodejs-library/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.uber.com/blog/fulfillment-platform-rearchitecture/&#34;&gt;https://www.uber.com/blog/fulfillment-platform-rearchitecture/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.uber.com/blog/building-ubers-fulfillment-platform/&#34;&gt;https://www.uber.com/blog/building-ubers-fulfillment-platform/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=DY2AR8Wzg3Y&#34;&gt;https://www.youtube.com/watch?v=DY2AR8Wzg3Y&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.google/pubs/pub39966/&#34;&gt;https://research.google/pubs/pub39966/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
  </channel>
</rss>
